# Enterprise Inference Engine Platform - Application Configuration
# Profile: Default (Development)

quarkus:
  application:
    name: inference-engine-platform
    version: 1.0.0

  # ===== HTTP Configuration =====
  http:
    port: 8080
    host: 0.0.0.0
    cors:
      ~: true
      origins: "*"
      methods: GET,POST,PUT,DELETE,OPTIONS
      headers: "*"
    limits:
      max-body-size: 100M
      max-form-attribute-size: 10M
    body:
      handle-file-uploads: true
      uploads-directory: ${java.io.tmpdir}/uploads
      delete-uploaded-files-on-end: true

  # ===== Database Configuration =====
  datasource:
    db-kind: postgresql
    username: ${DB_USERNAME:inference}
    password: ${DB_PASSWORD:inference}
    jdbc:
      url: ${DB_URL:jdbc:postgresql://localhost:5432/inference_db}
      max-size: 20
      min-size: 5
      acquisition-timeout: 30s
      background-validation-interval: 2M
      leak-detection-interval: 5M

  hibernate-orm:
    database:
      generation: validate  # Use Flyway for schema management
    log:
      sql: ${DB_LOG_SQL:false}
      bind-parameters: ${DB_LOG_BIND:false}
    jdbc:
      statement-batch-size: 50
    query:
      query-plan-cache-max-size: 2048

  # ===== Flyway Migration =====
  flyway:
    migrate-at-start: true
    baseline-on-migrate: true
    baseline-version: 0
    locations: classpath:db/migration
    clean-at-start: ${FLYWAY_CLEAN:false}  # Never true in production!

  # ===== Redis Configuration =====
  redis:
    hosts: ${REDIS_URL:redis://localhost:6379}
    password: ${REDIS_PASSWORD:}
    database: 0
    client-type: standalone
    timeout: 10s
    max-pool-size: 20

  # ===== Security - JWT =====
  smallrye-jwt:
    enabled: true
    auth:
      mechanism: MP-JWT
    sign:
      key-location: ${JWT_PRIVATE_KEY:/keys/jwt-private.pem}
    verify:
      key-location: ${JWT_PUBLIC_KEY:/keys/jwt-public.pem}
      issuer: ${JWT_ISSUER:https://inference-engine.ai}
      audience: ${JWT_AUDIENCE:inference-api}
    token:
      age: 3600  # 1 hour

  # ===== OpenAPI Documentation =====
  smallrye-openapi:
    path: /openapi
    info:
      title: Enterprise Inference Engine API
      version: 1.0.0
      description: Production-grade ML inference platform with multi-framework support
      contact:
        name: bhangun
        email: support@inference-engine.ai
        url: https://inference-engine.ai
      license:
        name: Apache 2.0
        url: https://www.apache.org/licenses/LICENSE-2.0.html
    servers:
      - url: http://localhost:8080
        description: Development server
      - url: https://api.inference-engine.ai
        description: Production server

  # ===== Metrics & Observability =====
  micrometer:
    enabled: true
    registry-enabled-default: true
    binder-enabled-default: true
    export:
      prometheus:
        enabled: true
        path: /q/metrics
        default-registry: true
    binder:
      jvm: true
      system: true
      http-server: true

  # ===== OpenTelemetry Tracing =====
  otel:
    enabled: ${OTEL_ENABLED:true}
    service:
      name: ${quarkus.application.name}
    exporter:
      otlp:
        endpoint: ${OTEL_ENDPOINT:http://jaeger:4317}
    traces:
      exporter: otlp
    metrics:
      exporter: none  # Use Prometheus for metrics
    logs:
      exporter: none  # Use standard logging

  # ===== Health Checks =====
  smallrye-health:
    root-path: /q/health
    liveness-path: /live
    readiness-path: /ready
    ui:
      enable: true
      root-path: /q/health-ui

  # ===== Logging =====
  log:
    level: INFO
    min-level: DEBUG
    console:
      enable: true
      format: "%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %s%e%n"
      json: ${LOG_JSON:false}
      color: true
    category:
      "ai.enterprise.inference":
        level: ${APP_LOG_LEVEL:INFO}
      "org.hibernate":
        level: WARN
      "io.quarkus":
        level: INFO
    handler:
      file:
        "APP_LOG":
          enable: ${LOG_FILE_ENABLED:false}
          path: ${LOG_FILE_PATH:/var/log/inference-engine/app.log}
          format: "%d{yyyy-MM-dd HH:mm:ss,SSS} %-5p [%c{3.}] (%t) %s%e%n"
          rotation:
            max-file-size: 10M
            max-backup-index: 10
            file-suffix: .yyyy-MM-dd

  # ===== Native Image =====
  native:
    resources:
      includes: "native-libs/**,models/**,db/migration/**"
    additional-build-args:
      - --initialize-at-build-time=org.postgresql.Driver
      - --initialize-at-run-time=io.netty.handler.codec.http.HttpObjectEncoder
      - -H:+AddAllCharsets
      - -H:ReflectionConfigurationFiles=reflection-config.json
      - -H:JNIConfigurationFiles=jni-config.json

# ===== Custom Application Configuration =====

inference:
  # ===== Adapter Configuration =====
  adapter:
    litert-cpu:
      enabled: ${LITERT_CPU_ENABLED:true}
      num-threads: ${LITERT_THREADS:4}
      use-xnnpack: true
      delegate:
        gpu: false
    
    litert-gpu:
      enabled: ${LITERT_GPU_ENABLED:false}
      gpu-backend: opencl  # opencl | vulkan | metal
      gpu-device-id: 0
    
    litert-npu:
      enabled: ${LITERT_NPU_ENABLED:false}
      npu-type: auto  # auto | hexagon | neuron | ethos
    
    onnx-cpu:
      enabled: ${ONNX_CPU_ENABLED:false}
      num-threads: ${ONNX_THREADS:4}
      execution-provider: cpu
    
    onnx-gpu:
      enabled: ${ONNX_GPU_ENABLED:false}
      execution-provider: cuda  # cuda | tensorrt | directml | rocm
      cuda-device-id: 0
      tensorrt:
        enable-fp16: true
        enable-int8: false
    
    tensorflow-cpu:
      enabled: ${TF_CPU_ENABLED:false}
      num-threads: ${TF_THREADS:4}
    
    tensorflow-gpu:
      enabled: ${TF_GPU_ENABLED:false}
      gpu-device-id: 0
      allow-growth: true
    
    tensorflow-tpu:
      enabled: ${TF_TPU_ENABLED:false}
      tpu-name: ${TPU_NAME:}
      tpu-zone: ${TPU_ZONE:}
    
    triton:
      enabled: ${TRITON_ENABLED:false}
      endpoint: ${TRITON_ENDPOINT:triton.svc:8001}
      protocol: grpc  # grpc | http
      timeout-ms: 30000
      ssl:
        enabled: false
        verify: true

  # ===== Routing & Selection =====
  routing:
    default-runners: ${DEFAULT_RUNNERS:litert-cpu,onnx-cpu}
    selection-policy: ${SELECTION_POLICY:balanced}  # balanced | latency | cost | memory
    enable-fallback: true
    max-fallback-attempts: 2
    
    policies:
      balanced:
        latency-weight: 0.4
        cost-weight: 0.3
        availability-weight: 0.3
      
      latency:
        latency-weight: 0.8
        cost-weight: 0.1
        availability-weight: 0.1
      
      cost:
        latency-weight: 0.1
        cost-weight: 0.8
        availability-weight: 0.1

  # ===== Warm Pool Management =====
  warm-pool:
    enabled: ${WARM_POOL_ENABLED:true}
    min-size: ${WARM_POOL_MIN:2}
    max-size: ${WARM_POOL_MAX:10}
    prewarm-on-startup: true
    prewarm-models: ${PREWARM_MODELS:}  # Comma-separated model IDs
    eviction-policy: lru  # lru | lfu | ttl
    ttl-minutes: 60
    size-based-eviction: true
    max-memory-gb: ${WARM_POOL_MAX_MEMORY:10}

  # ===== Batching =====
  batching:
    enabled: ${BATCHING_ENABLED:true}
    max-batch-size: ${MAX_BATCH_SIZE:32}
    max-wait-ms: ${BATCH_MAX_WAIT:10}
    queue-capacity: 1000
    adaptive: true  # Dynamically adjust batch size

  # ===== Circuit Breaker =====
  circuit-breaker:
    enabled: true
    failure-threshold: 5
    failure-rate-threshold: 50  # percent
    slow-call-threshold: 30  # seconds
    wait-duration-open: 60  # seconds
    permitted-calls-half-open: 3
    sliding-window-size: 100
    minimum-calls: 20

  # ===== Rate Limiting =====
  rate-limiting:
    enabled: ${RATE_LIMIT_ENABLED:true}
    default-requests-per-second: 100
    default-burst: 20
    per-tenant: true
    storage: redis  # redis | memory

  # ===== Model Storage =====
  model-storage:
    provider: ${STORAGE_PROVIDER:s3}  # s3 | gcs | azure | minio | local
    
    s3:
      bucket: ${S3_BUCKET:ml-models}
      region: ${S3_REGION:us-east-1}
      endpoint: ${S3_ENDPOINT:}  # For MinIO
      access-key: ${S3_ACCESS_KEY:}
      secret-key: ${S3_SECRET_KEY:}
      path-prefix: models/
    
    gcs:
      bucket: ${GCS_BUCKET:ml-models}
      project-id: ${GCS_PROJECT:}
      credentials-file: ${GCS_CREDENTIALS:/keys/gcs-key.json}
    
    azure:
      container: ${AZURE_CONTAINER:ml-models}
      connection-string: ${AZURE_CONNECTION:}
    
    local:
      base-path: ${LOCAL_STORAGE_PATH:/var/lib/inference/models}
    
    caching:
      enabled: true
      max-size-gb: 50
      ttl-hours: 24
      cache-path: ${CACHE_PATH:/var/cache/inference/models}

  # ===== Model Conversion =====
  conversion:
    enabled: ${CONVERSION_ENABLED:true}
    supported-formats: litert,onnx,tensorflow,pytorch
    workers: 2
    timeout-minutes: 30
    
    quantization:
      enabled: true
      supported-types: int8,fp16,bf16
      calibration-samples: 100
    
    storage:
      temp-dir: ${CONVERSION_TEMP:/tmp/conversions}
      cleanup-on-complete: true

  # ===== Multi-Tenancy =====
  multitenancy:
    enabled: ${MULTITENANCY_ENABLED:true}
    isolation-level: strong  # strong | moderate | weak
    
    quota-enforcement:
      enabled: true
      strict-mode: ${QUOTA_STRICT:true}
      
    default-quotas:
      requests-per-hour: 1000
      requests-per-day: 10000
      storage-gb: 10
      models: 10
      concurrent-requests: 10

  # ===== Monitoring & Alerts =====
  monitoring:
    health-check-interval: 30  # seconds
    metrics-retention-hours: 168  # 7 days
    
    alerts:
      enabled: ${ALERTS_ENABLED:false}
      webhook-url: ${ALERT_WEBHOOK:}
      
      thresholds:
        error-rate-percent: 5
        p95-latency-ms: 500
        circuit-breaker-open: true

# ===== Profile-Specific Overrides =====

"%dev":
  quarkus:
    log:
      console:
        json: false
        color: true
      level: DEBUG
      category:
        "ai.enterprise.inference":
          level: DEBUG
    datasource:
      jdbc:
        url: jdbc:postgresql://localhost:5432/inference_dev
    flyway:
      clean-at-start: true
  
  inference:
    warm-pool:
      enabled: false
    batching:
      enabled: false
    multitenancy:
      enabled: false

"%test":
  quarkus:
    datasource:
      jdbc:
        url: jdbc:h2:mem:test;DB_CLOSE_DELAY=-1
    flyway:
      clean-at-start: true
    hibernate-orm:
      database:
        generation: drop-and-create

"%prod":
  quarkus:
    log:
      console:
        json: true
      level: INFO
      category:
        "ai.enterprise.inference":
          level: INFO
    datasource:
      jdbc:
        url: ${DB_URL}
        max-size: 50
    flyway:
      clean-at-start: false
    http:
      cors:
        ~: true
        origins: ${CORS_ORIGINS:https://app.inference-engine.ai}
  
  inference:
    warm-pool:
      enabled: true
      min-size: 5
      max-size: 20
    rate-limiting:
      enabled: true
    monitoring:
      alerts:
        enabled: true
