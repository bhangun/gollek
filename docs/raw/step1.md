
Good. Iâ€™ll **go on systematically** and stay **strictly inside the inference kernel** we just froze.

No workflow.
No agent runtime.
No orchestration creep.

---

# STEP 1 â€” Kernel Module & Package Layout (FOUNDATION)

This step answers **â€œwhere does everything live?â€**
Before any implementation, this must be rock-solid.

---

## 1ï¸âƒ£ Kernel Module Boundary (Hard Line)

Create **one** module only:

```
golek-inference-kernel
```

Everything inside this module must obey:

* request-scoped execution
* no persistence
* no async background jobs
* no business semantics

---

## 2ï¸âƒ£ Canonical Package Structure

```
golek-inference-kernel/
â””â”€â”€ src/main/java/com/kayys/golek/inference/kernel/
    â”œâ”€â”€ engine/            # Entry point
    â”œâ”€â”€ context/           # Request + execution context
    â”œâ”€â”€ pipeline/          # Phase execution
    â”œâ”€â”€ phase/             # Phase definitions
    â”œâ”€â”€ plugin/            # Plugin SPI
    â”œâ”€â”€ provider/          # LLM provider SPI
    â”œâ”€â”€ streaming/         # Streaming abstractions
    â”œâ”€â”€ observer/          # Metrics / tracing hooks
    â”œâ”€â”€ exception/         # Kernel-level exceptions
    â””â”€â”€ model/             # Pure inference DTOs
```

Nothing else.

If something doesnâ€™t fit â†’ it does NOT belong to kernel.

---

## 3ï¸âƒ£ engine/ â€” Entry Point Only

```
engine/
â”œâ”€â”€ InferenceEngine.java
â”œâ”€â”€ DefaultInferenceEngine.java
â””â”€â”€ EngineContext.java
```

### Responsibility

* Accept request
* Create context
* Invoke pipeline
* Return response

ğŸš« No logic
ğŸš« No policies
ğŸš« No provider code

---

## 4ï¸âƒ£ context/ â€” Per-Request State

```
context/
â”œâ”€â”€ InferenceContext.java
â”œâ”€â”€ DefaultInferenceContext.java
â”œâ”€â”€ RequestAttributes.java
```

### Rules

* Created per request
* Destroyed after response
* Thread-confined
* Never cached

---

## 5ï¸âƒ£ phase/ â€” Deterministic Execution Order

```
phase/
â””â”€â”€ InferencePhase.java
```

Only the enum.
No logic here.

---

## 6ï¸âƒ£ pipeline/ â€” Phase Executor

```
pipeline/
â”œâ”€â”€ InferencePipeline.java
â””â”€â”€ DefaultInferencePipeline.java
```

### Responsibility

* Iterate phases
* Execute plugins
* Stop on failure

ğŸš« No provider knowledge
ğŸš« No policies

---

## 7ï¸âƒ£ plugin/ â€” EXTENSION POINT (Most Important)

```
plugin/
â”œâ”€â”€ Plugin.java
â”œâ”€â”€ ConfigurablePlugin.java
â”œâ”€â”€ InferencePhasePlugin.java
â”œâ”€â”€ ModelValidationPlugin.java
â””â”€â”€ PluginRegistry.java
```

### This is the **only** extension mechanism.

No â€œcustom hooksâ€, no side channels.

---

## 8ï¸âƒ£ provider/ â€” LLM Abstraction (Hard Boundary)

```
provider/
â”œâ”€â”€ LLMProvider.java
â”œâ”€â”€ StreamingLLMProvider.java
â”œâ”€â”€ ProviderCapabilities.java
â”œâ”€â”€ ProviderRequest.java
â””â”€â”€ ProviderRegistry.java
```

### Rules

* Providers do inference ONLY
* Providers do NOT mutate context
* Providers do NOT apply policy

---

## 9ï¸âƒ£ streaming/ â€” Optional but Clean

```
streaming/
â”œâ”€â”€ StreamingResponse.java
â”œâ”€â”€ StreamChunk.java
â””â”€â”€ StreamSubscriber.java
```

Streaming is **data flow only**, not control flow.

---

## ğŸ”Ÿ observer/ â€” Observability Hooks

```
observer/
â””â”€â”€ InferenceObserver.java
```

Observers are passive.

ğŸš« No mutation
ğŸš« No retries
ğŸš« No blocking

---

## 1ï¸âƒ£1ï¸âƒ£ model/ â€” Pure DTOs

```
model/
â”œâ”€â”€ InferenceRequest.java
â”œâ”€â”€ InferenceResponse.java
â”œâ”€â”€ Message.java
â””â”€â”€ ToolCall.java
```

No behavior.
No validation logic.

---

## 1ï¸âƒ£2ï¸âƒ£ exception/ â€” Kernel Errors Only

```
exception/
â”œâ”€â”€ InferenceException.java
â”œâ”€â”€ ValidationException.java
â”œâ”€â”€ ProviderException.java
â””â”€â”€ KernelPanicException.java
```

If an exception needs **business meaning**, itâ€™s not kernel.

---

## 13ï¸âƒ£ Dependency Direction (Non-Negotiable)

```
engine
 â””â”€â”€ pipeline
      â””â”€â”€ phase
           â””â”€â”€ plugin
                â””â”€â”€ provider
```

**Never the reverse.**

---

## 14ï¸âƒ£ What Comes Next (Roadmap)

Now that the **structure is frozen**, next steps are:

### STEP 2

â¡ï¸ Implement **InferenceEngine + InferenceContext**

### STEP 3

â¡ï¸ Implement **InferencePipeline + phase execution**

### STEP 4

â¡ï¸ Implement **PluginRegistry + ordering + lifecycle**

### STEP 5

â¡ï¸ Implement **LLMProvider + cloud providers (OpenAI, Anthropic, etc.)**

### STEP 6

â¡ï¸ Streaming execution path

---

If you say **â€œgo step 2â€**, I will:

* implement `InferenceEngine`
* implement `DefaultInferenceContext`
* show exact call flow
* no abstractions skipped

I will not jump ahead again.
