



## Architecture Overview

### Design Principles

**1. Hexagonal Architecture (Ports & Adapters)**
- Core domain logic isolated from infrastructure concerns
- Adapters for different model formats (GGUF, ONNX, Triton, LLM Cloud Provider (OpenAI, Anthropic, Google), etc.)
- Easy to test and swap implementations

**2. Multi-Tenancy First**
- Tenant isolation at data, compute, and configuration levels
- Resource quotas and fair scheduling per tenant
- Secure tenant context propagation

**3. Build-Time Optimization**
- Maven profiles for CPU/GPU/TPU variants
- Quarkus build-time pruning with `@IfBuildProperty`
- GraalVM native image ready with reachability metadata

**4. Cloud-Native & Kubernetes-Ready**
- Health/readiness probes
- Graceful shutdown and resource cleanup
- Service mesh compatible (mTLS, traffic management)

---

## Project Structure

```
inference-platform/
â”œâ”€â”€ pom.xml                                    # Parent POM with profiles
â”œâ”€â”€ README.md
â”œâ”€â”€ ARCHITECTURE.md
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ Dockerfile.jvm-cpu
â”‚   â”œâ”€â”€ Dockerfile.jvm-gpu
â”‚   â”œâ”€â”€ Dockerfile.native-cpu
â”‚   â””â”€â”€ Dockerfile.native-gpu
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ base/                                  # Kustomize base
â”‚   â”œâ”€â”€ overlays/
â”‚   â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ production/
â”‚   â””â”€â”€ helm/                                  # Helm charts
â”œâ”€â”€ inference-api/                             # API contracts & DTOs
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/enterprise/inference/api/
â”‚           â”œâ”€â”€ InferenceRequest.java
â”‚           â”œâ”€â”€ InferenceResponse.java
â”‚           â”œâ”€â”€ ModelMetadata.java
â”‚           â””â”€â”€ TenantContext.java
â”œâ”€â”€ inference-core/                            # Domain logic & SPI
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/enterprise/inference/core/
â”‚           â”œâ”€â”€ domain/
â”‚           â”‚   â”œâ”€â”€ ModelManifest.java
â”‚           â”‚   â”œâ”€â”€ ModelVersion.java
â”‚           â”‚   â”œâ”€â”€ ResourceRequirements.java
â”‚           â”‚   â””â”€â”€ InferenceSession.java
â”‚           â”œâ”€â”€ ports/
â”‚           â”‚   â”œâ”€â”€ inbound/
â”‚           â”‚   â”‚   â”œâ”€â”€ InferenceUseCase.java
â”‚           â”‚   â”‚   â””â”€â”€ ModelManagementUseCase.java
â”‚           â”‚   â””â”€â”€ outbound/
â”‚           â”‚       â”œâ”€â”€ ModelRunner.java        # Core SPI
â”‚           â”‚       â”œâ”€â”€ ModelRepository.java
â”‚           â”‚       â”œâ”€â”€ MetricsPublisher.java
â”‚           â”‚       â””â”€â”€ TenantResolver.java
â”‚           â”œâ”€â”€ service/
â”‚           â”‚   â”œâ”€â”€ InferenceOrchestrator.java
â”‚           â”‚   â”œâ”€â”€ ModelRouterService.java
â”‚           â”‚   â”œâ”€â”€ SelectionPolicy.java
â”‚           â”‚   â””â”€â”€ FallbackStrategy.java
â”‚           â””â”€â”€ exceptions/
â”‚               â”œâ”€â”€ ModelNotFoundException.java
â”‚               â”œâ”€â”€ InferenceException.java
â”‚               â””â”€â”€ TenantQuotaExceededException.java
â”œâ”€â”€ inference-adapter-gguf/                    # llama.cpp adapter
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/enterprise/inference/adapter/gguf/
â”‚           â”œâ”€â”€ LlamaCppRunner.java
â”‚           â”œâ”€â”€ GGUFModelLoader.java
â”‚           â”œâ”€â”€ FFMNativeBinding.java
â”‚           â””â”€â”€ resources/
â”‚               â””â”€â”€ META-INF/
â”‚                   â””â”€â”€ native-image/
â”‚                       â””â”€â”€ reflect-config.json
â”œâ”€â”€ inference-adapter-onnx/                    # ONNX Runtime adapter
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/enterprise/inference/adapter/onnx/
â”‚           â”œâ”€â”€ OnnxRuntimeRunner.java
â”‚           â”œâ”€â”€ OnnxSessionManager.java
â”‚           â”œâ”€â”€ ExecutionProviderSelector.java
â”‚           â””â”€â”€ TensorConverter.java
â”œâ”€â”€ inference-adapter-triton/                  # Triton Inference Server adapter
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/enterprise/inference/adapter/triton/
â”‚           â”œâ”€â”€ TritonGrpcRunner.java
â”‚           â”œâ”€â”€ TritonHttpRunner.java
â”‚           â”œâ”€â”€ TritonClientPool.java
â”‚           â””â”€â”€ proto/                         # Generated gRPC stubs
â”œâ”€â”€ inference-adapter-tpu/                     # Google Cloud TPU adapter
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/enterprise/inference/adapter/tpu/
â”‚           â””â”€â”€ TpuRunner.java
â”œâ”€â”€ inference-infrastructure/                  # Infrastructure adapters
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/enterprise/inference/infrastructure/
â”‚           â”œâ”€â”€ persistence/
â”‚           â”‚   â”œâ”€â”€ S3ModelRepository.java
â”‚           â”‚   â”œâ”€â”€ MinIOModelRepository.java
â”‚           â”‚   â””â”€â”€ PostgresMetadataStore.java
â”‚           â”œâ”€â”€ messaging/
â”‚           â”‚   â”œâ”€â”€ KafkaEventPublisher.java
â”‚           â”‚   â””â”€â”€ events/
â”‚           â”œâ”€â”€ security/
â”‚           â”‚   â”œâ”€â”€ JwtTenantResolver.java
â”‚           â”‚   â”œâ”€â”€ KeycloakIntegration.java
â”‚           â”‚   â””â”€â”€ TenantContextInterceptor.java
â”‚           â””â”€â”€ observability/
â”‚               â”œâ”€â”€ PrometheusMetrics.java
â”‚               â”œâ”€â”€ OpenTelemetryTracing.java
â”‚               â””â”€â”€ StructuredLogging.java
â”œâ”€â”€ inference-runtime/                     # Quarkus application assembly
â”‚   â”œâ”€â”€ pom.xml
â”‚   â””â”€â”€ src/main/
â”‚       â”œâ”€â”€ java/
â”‚       â”‚   â””â”€â”€ com/enterprise/inference/runtime/
â”‚       â”‚       â”œâ”€â”€ rest/
â”‚       â”‚       â”‚   â”œâ”€â”€ InferenceResource.java
â”‚       â”‚       â”‚   â”œâ”€â”€ ModelManagementResource.java
â”‚       â”‚       â”‚   â”œâ”€â”€ HealthResource.java
â”‚       â”‚       â”‚   â””â”€â”€ filters/
â”‚       â”‚       â”‚       â”œâ”€â”€ TenantFilter.java
â”‚       â”‚       â”‚       â””â”€â”€ RateLimitFilter.java
â”‚       â”‚       â”œâ”€â”€ config/
â”‚       â”‚       â”‚   â”œâ”€â”€ InferenceConfig.java
â”‚       â”‚       â”‚   â”œâ”€â”€ TenantConfig.java
â”‚       â”‚       â”‚   â””â”€â”€ RunnerConfig.java
â”‚       â”‚       â””â”€â”€ lifecycle/
â”‚       â”‚           â”œâ”€â”€ StartupInitializer.java
â”‚       â”‚           â””â”€â”€ ShutdownHandler.java
â”‚       â””â”€â”€ resources/
â”‚           â”œâ”€â”€ application.yml
â”‚           â”œâ”€â”€ application-dev.yml
â”‚           â”œâ”€â”€ application-prod.yml
â”‚           â””â”€â”€ META-INF/
â”‚               â””â”€â”€ microprofile-config.properties
â””â”€â”€ inference-tests/
    â”œâ”€â”€ pom.xml
    â””â”€â”€ src/test/
        â”œâ”€â”€ java/
        â”‚   â”œâ”€â”€ integration/
        â”‚   â”œâ”€â”€ performance/
        â”‚   â””â”€â”€ e2e/
        â””â”€â”€ resources/
            â””â”€â”€ fixtures/
```

---

## Core Components

### 1. Core Domain Model

```java
// inference-core/src/main/java/com/enterprise/inference/core/domain/

/**
 * Immutable model manifest representing all metadata and artifacts
 * for a specific model version.
 */
public record ModelManifest(
    String modelId,
    String name,
    String version,
    TenantId tenantId,
    Map<ModelFormat, ArtifactLocation> artifacts,
    List<SupportedDevice> supportedDevices,
    ResourceRequirements resourceRequirements,
    Map<String, Object> metadata,
    Instant createdAt,
    Instant updatedAt
) {
    public boolean supportsFormat(ModelFormat format) {
        return artifacts.containsKey(format);
    }
    
    public boolean supportsDevice(DeviceType deviceType) {
        return supportedDevices.stream()
            .anyMatch(d -> d.type() == deviceType);
    }
}

/**
 * Value object for tenant identification and isolation
 */
public record TenantId(String value) {
    public TenantId {
        if (value == null || value.isBlank()) {
            throw new IllegalArgumentException("TenantId cannot be empty");
        }
    }
}

/**
 * Resource requirements and constraints for model execution
 */
public record ResourceRequirements(
    MemorySize minMemory,
    MemorySize recommendedMemory,
    MemorySize minVRAM,
    Optional<Integer> minCores,
    Optional<DiskSpace> diskSpace
) {}

/**
 * Enumeration of supported model formats
 */
public enum ModelFormat {
    GGUF("gguf", "llama.cpp"),
    ONNX("onnx", "ONNX Runtime"),
    TENSORRT("trt", "TensorRT"),
    TORCHSCRIPT("pt", "TorchScript"),
    TENSORFLOW_SAVED_MODEL("pb", "TensorFlow");
    
    private final String extension;
    private final String runtime;
    
    ModelFormat(String extension, String runtime) {
        this.extension = extension;
        this.runtime = runtime;
    }
}

/**
 * Device type enumeration with capability flags
 */
public enum DeviceType {
    CPU(false, false),
    CUDA(true, false),
    ROCM(true, false),
    METAL(true, false),
    TPU(false, true),
    OPENVINO(true, false);
    
    private final boolean supportsGpu;
    private final boolean supportsTpu;
}
```

### 2. Core Port Definitions (SPI)

```java
// inference-core/src/main/java/com/enterprise/inference/core/ports/outbound/

/**
 * Core SPI for model execution backends.
 * All adapters must implement this interface.
 */
public interface ModelRunner extends AutoCloseable {
    
    /**
     * Initialize the runner with model manifest and configuration
     * @param manifest Model metadata and artifact locations
     * @param config Runner-specific configuration
     * @param tenantContext Current tenant context for isolation
     * @throws ModelLoadException if initialization fails
     */
    void initialize(
        ModelManifest manifest, 
        Map<String, Object> config,
        TenantContext tenantContext
    ) throws ModelLoadException;
    
    /**
     * Execute synchronous inference
     * @param request Inference request with inputs
     * @param context Request context with timeout, priority, etc.
     * @return Inference response with outputs
     * @throws InferenceException if execution fails
     */
    InferenceResponse infer(
        InferenceRequest request,
        RequestContext context
    ) throws InferenceException;
    
    /**
     * Execute asynchronous inference with callback
     * @param request Inference request
     * @param context Request context
     * @return CompletionStage for async processing
     */
    CompletionStage<InferenceResponse> inferAsync(
        InferenceRequest request,
        RequestContext context
    );
    
    /**
     * Health check for this runner instance
     * @return Health status with diagnostics
     */
    HealthStatus health();
    
    /**
     * Get current resource utilization metrics
     * @return Resource usage snapshot
     */
    ResourceMetrics getMetrics();
    
    /**
     * Warm up the model (optional optimization)
     * @param sampleInputs Sample inputs for warming
     */
    default void warmup(List<InferenceRequest> sampleInputs) {
        // Default no-op
    }
    
    /**
     * Get runner metadata
     * @return Metadata about this runner implementation
     */
    RunnerMetadata metadata();
    
    /**
     * Gracefully release resources
     */
    @Override
    void close();
}

/**
 * Runner metadata for selection and diagnostics
 */
public record RunnerMetadata(
    String name,
    String version,
    List<ModelFormat> supportedFormats,
    List<DeviceType> supportedDevices,
    ExecutionMode executionMode,
    Map<String, Object> capabilities
) {}

/**
 * Repository for model artifacts and metadata
 */
public interface ModelRepository {
    
    /**
     * Load model manifest by ID
     */
    Optional<ModelManifest> findById(String modelId, TenantId tenantId);
    
    /**
     * List all models for tenant
     */
    List<ModelManifest> findByTenant(TenantId tenantId, Pageable pageable);
    
    /**
     * Save or update model manifest
     */
    ModelManifest save(ModelManifest manifest);
    
    /**
     * Download model artifact to local cache
     */
    Path downloadArtifact(
        ModelManifest manifest, 
        ModelFormat format
    ) throws ArtifactDownloadException;
    
    /**
     * Check if artifact is cached locally
     */
    boolean isCached(String modelId, ModelFormat format);
    
    /**
     * Evict artifact from local cache
     */
    void evictCache(String modelId, ModelFormat format);
}
```

### 3. Model Router & Selection Policy

```java
// inference-core/src/main/java/com/enterprise/inference/core/service/

/**
 * Orchestrates inference requests across multiple runners with
 * intelligent routing, fallback, and load balancing.
 */
@ApplicationScoped
public class InferenceOrchestrator {
    
    private final ModelRouterService router;
    private final ModelRunnerFactory factory;
    private final ModelRepository repository;
    private final MetricsPublisher metrics;
    private final CircuitBreaker circuitBreaker;
    
    @Inject
    public InferenceOrchestrator(
        ModelRouterService router,
        ModelRunnerFactory factory,
        ModelRepository repository,
        MetricsPublisher metrics,
        CircuitBreaker circuitBreaker
    ) {
        this.router = router;
        this.factory = factory;
        this.repository = repository;
        this.metrics = metrics;
        this.circuitBreaker = circuitBreaker;
    }
    
    /**
     * Execute inference with automatic runner selection and fallback
     */
    public InferenceResponse execute(
        String modelId,
        InferenceRequest request,
        TenantContext tenantContext
    ) {
        var span = Span.current();
        span.setAttribute("model.id", modelId);
        span.setAttribute("tenant.id", tenantContext.tenantId().value());
        
        // Load model manifest
        ModelManifest manifest = repository
            .findById(modelId, tenantContext.tenantId())
            .orElseThrow(() -> new ModelNotFoundException(modelId));
        
        // Build request context with timeout and priority
        RequestContext ctx = RequestContext.builder()
            .tenantContext(tenantContext)
            .timeout(Duration.ofSeconds(30))
            .priority(request.priority())
            .preferredDevice(request.deviceHint())
            .build();
        
        // Select and rank candidate runners
        List<RunnerCandidate> candidates = router.selectRunners(
            manifest, 
            ctx
        );
        
        InferenceException lastError = null;
        
        // Attempt inference with fallback
        for (RunnerCandidate candidate : candidates) {
            try {
                return executeWithRunner(
                    manifest, 
                    candidate, 
                    request, 
                    ctx
                );
            } catch (InferenceException e) {
                lastError = e;
                metrics.recordFailure(
                    candidate.runnerName(), 
                    modelId, 
                    e.getClass().getSimpleName()
                );
                
                // Don't retry on quota or validation errors
                if (e instanceof TenantQuotaExceededException ||
                    e instanceof ValidationException) {
                    throw e;
                }
                
                span.addEvent("Runner failed, attempting fallback", 
                    Attributes.of(
                        AttributeKey.stringKey("runner"), candidate.runnerName(),
                        AttributeKey.stringKey("error"), e.getMessage()
                    ));
            }
        }
        
        throw new AllRunnersFailedException(
            "All runners failed for model " + modelId, 
            lastError
        );
    }
    
    private InferenceResponse executeWithRunner(
        ModelManifest manifest,
        RunnerCandidate candidate,
        InferenceRequest request,
        RequestContext ctx
    ) {
        var timer = metrics.startTimer();
        
        try {
            // Get or create runner instance
            ModelRunner runner = factory.getRunner(
                manifest, 
                candidate.runnerName(),
                ctx.tenantContext()
            );
            
            // Execute with circuit breaker protection
            InferenceResponse response = circuitBreaker.call(
                () -> runner.infer(request, ctx)
            );
            
            metrics.recordSuccess(
                candidate.runnerName(), 
                manifest.modelId(), 
                timer.stop()
            );
            
            return response;
            
        } catch (Exception e) {
            metrics.recordFailure(
                candidate.runnerName(), 
                manifest.modelId(), 
                e.getClass().getSimpleName()
            );
            throw new InferenceException(
                "Inference failed with runner: " + candidate.runnerName(), 
                e
            );
        }
    }
}

/**
 * Selection policy implementation with scoring algorithm
 */
@ApplicationScoped
public class SelectionPolicy {
    
    private final RuntimeMetricsCache metricsCache;
    private final HardwareDetector hardwareDetector;
    
    /**
     * Rank available runners based on multiple criteria
     */
    public List<RunnerCandidate> rankRunners(
        ModelManifest manifest,
        RequestContext context,
        List<String> configuredRunners
    ) {
        List<RunnerCandidate> candidates = new ArrayList<>();
        
        // Get current hardware availability
        HardwareCapabilities hw = hardwareDetector.detect();
        
        for (String runnerName : configuredRunners) {
            RunnerMetadata runnerMeta = getRunnerMetadata(runnerName);
            
            // Filter by format compatibility
            if (!hasCompatibleFormat(manifest, runnerMeta)) {
                continue;
            }
            
            // Filter by device availability
            if (!isDeviceAvailable(runnerMeta, hw, context)) {
                continue;
            }
            
            // Calculate score
            int score = calculateScore(
                manifest, 
                runnerMeta, 
                context, 
                hw
            );
            
            candidates.add(new RunnerCandidate(
                runnerName, 
                score, 
                runnerMeta
            ));
        }
        
        // Sort by score descending
        candidates.sort(Comparator.comparing(
            RunnerCandidate::score
        ).reversed());
        
        return candidates;
    }
    
    /**
     * Multi-factor scoring algorithm
     */
    private int calculateScore(
        ModelManifest manifest,
        RunnerMetadata runner,
        RequestContext context,
        HardwareCapabilities hw
    ) {
        int score = 0;
        
        // Device preference match (highest weight)
        if (context.preferredDevice().isPresent() &&
            runner.supportedDevices().contains(
                context.preferredDevice().get()
            )) {
            score += 50;
        }
        
        // Format native support
        if (runner.supportedFormats().contains(
            manifest.artifacts().keySet().iterator().next()
        )) {
            score += 30;
        }
        
        // Historical performance (P95 latency)
        Optional<Duration> p95 = metricsCache.getP95Latency(
            runner.name(), 
            manifest.modelId()
        );
        if (p95.isPresent() && 
            p95.get().compareTo(context.timeout()) < 0) {
            score += 25;
        }
        
        // Resource availability
        if (hasAvailableResources(manifest, runner, hw)) {
            score += 20;
        }
        
        // Health status
        if (metricsCache.isHealthy(runner.name())) {
            score += 15;
        }
        
        // Cost optimization (favor CPU over GPU if performance OK)
        if (context.costSensitive() && 
            runner.supportedDevices().contains(DeviceType.CPU)) {
            score += 10;
        }
        
        // Current load (avoid overloaded runners)
        double currentLoad = metricsCache.getCurrentLoad(runner.name());
        if (currentLoad < 0.7) {
            score += 10;
        } else if (currentLoad > 0.9) {
            score -= 20;
        }
        
        return score;
    }
}
```

### 4. Runner Factory with Warm Pool

```java
// inference-core/src/main/java/com/enterprise/inference/core/service/

/**
 * Factory for creating and managing runner instances with
 * warm pool, lifecycle management, and tenant isolation.
 */
@ApplicationScoped
public class ModelRunnerFactory {
    
    private static final int MAX_POOL_SIZE = 10;
    private static final Duration IDLE_TIMEOUT = Duration.ofMinutes(15);
    
    @Inject
    Instance<ModelRunner> runnerProviders;
    
    @Inject
    ModelRepository repository;
    
    @Inject
    TenantConfig tenantConfig;
    
    // Pool: (tenantId, modelId, runnerName) -> Runner instance
    private final LoadingCache<RunnerCacheKey, ModelRunner> warmPool;
    
    // Track usage per runner for cleanup
    private final Map<RunnerCacheKey, Instant> lastAccess;
    
    @Inject
    public ModelRunnerFactory() {
        this.warmPool = Caffeine.newBuilder()
            .maximumSize(MAX_POOL_SIZE)
            .expireAfterAccess(IDLE_TIMEOUT)
            .removalListener(this::onRunnerEvicted)
            .build(this::createRunner);
            
        this.lastAccess = new ConcurrentHashMap<>();
        
        // Start cleanup scheduler
        startCleanupScheduler();
    }
    
    /**
     * Get or create runner instance for tenant
     */
    public ModelRunner getRunner(
        ModelManifest manifest,
        String runnerName,
        TenantContext tenantContext
    ) {
        RunnerCacheKey key = new RunnerCacheKey(
            tenantContext.tenantId(),
            manifest.modelId(),
            runnerName
        );
        
        // Update last access time
        lastAccess.put(key, Instant.now());
        
        // Get from pool or create
        return warmPool.get(key);
    }
    
    /**
     * Prewarm runners for specific models
     */
    public void prewarm(
        ModelManifest manifest,
        List<String> runnerNames,
        TenantContext tenantContext
    ) {
        runnerNames.forEach(runnerName -> {
            try {
                getRunner(manifest, runnerName, tenantContext);
            } catch (Exception e) {
                // Log but don't fail prewarming
                Log.warnf("Failed to prewarm runner %s: %s", 
                    runnerName, e.getMessage());
            }
        });
    }
    
    /**
     * Create new runner instance (called by cache loader)
     */
    private ModelRunner createRunner(RunnerCacheKey key) {
        // Load manifest
        ModelManifest manifest = repository
            .findById(key.modelId(), key.tenantId())
            .orElseThrow(() -> new ModelNotFoundException(key.modelId()));
        
        // Find runner provider by name
        ModelRunner runner = runnerProviders.stream()
            .filter(r -> r.metadata().name().equals(key.runnerName()))
            .findFirst()
            .orElseThrow(() -> new IllegalArgumentException(
                "Unknown runner: " + key.runnerName()
            ));
        
        // Get tenant-specific configuration
        Map<String, Object> config = tenantConfig.getRunnerConfig(
            key.tenantId(), 
            key.runnerName()
        );
        
        // Initialize runner
        TenantContext ctx = TenantContext.of(key.tenantId());
        runner.initialize(manifest, config, ctx);
        
        // Warmup if configured
        if (config.getOrDefault("warmup.enabled", false).equals(true)) {
            runner.warmup(Collections.emptyList());
        }
        
        Log.infof("Created runner %s for model %s (tenant %s)", 
            key.runnerName(), key.modelId(), key.tenantId().value());
        
        return runner;
    }
    
    /**
     * Cleanup callback when runner is evicted
     */
    private void onRunnerEvicted(
        RunnerCacheKey key, 
        ModelRunner runner,
        RemovalCause cause
    ) {
        if (runner != null) {
            try {
                runner.close();
                Log.infof("Closed runner %s for model %s (cause: %s)", 
                    key.runnerName(), key.modelId(), cause);
            } catch (Exception e) {
                Log.errorf(e, "Error closing runner %s", key.runnerName());
            }
        }
        lastAccess.remove(key);
    }
    
    /**
     * Background cleanup of idle runners
     */
    private void startCleanupScheduler() {
        Executors.newSingleThreadScheduledExecutor()
            .scheduleAtFixedRate(
                this::cleanupIdleRunners,
                5, 5, TimeUnit.MINUTES
            );
    }
    
    private void cleanupIdleRunners() {
        Instant threshold = Instant.now().minus(IDLE_TIMEOUT);
        
        lastAccess.entrySet().removeIf(entry -> {
            if (entry.getValue().isBefore(threshold)) {
                warmPool.invalidate(entry.getKey());
                return true;
            }
            return false;
        });
    }
    
    /**
     * Cache key for runner pooling
     */
    private record RunnerCacheKey(
        TenantId tenantId,
        String modelId,
        String runnerName
    ) {}
}
```

---

Below is a **focused, future-proof upgrade plan** centered on a **real plugin system**, long-term reliability, and avoiding architectural dead-ends.

Iâ€™ll structure this as:

1. ğŸ” Whatâ€™s already excellent (so we donâ€™t break it)
2. ğŸ§© Missing plugin abstractions (core gap)
3. ğŸ§  Execution lifecycle plugins (reliability)
4. ğŸ” Security & policy plugins (enterprise readiness)
5. ğŸ§ª Validation & compatibility plugins (future-proofing)
6. âš™ï¸ Control-plane & hot-reload plugins
7. ğŸ“¦ Packaging & versioning strategy
8. ğŸ§­ Target end-state architecture (mental model)

---

## 1ï¸âƒ£ What You Already Did Right (Donâ€™t Touch These)

You already solved **hard problems** correctly:

âœ… Hexagonal core with SPI (`ModelRunner`)
âœ… Multi-format abstraction (GGUF / ONNX / Triton / TPU)
âœ… Warm pool with eviction & lifecycle
âœ… Intelligent routing with scoring & fallback
âœ… Tenant-aware orchestration
âœ… Metrics, circuit breakers, tracing hooks

âš ï¸ **Do NOT turn adapters into plugins directly** â€” thatâ€™s a trap.
Adapters are *implementations*, not *extension points*.

---

## 2ï¸âƒ£ The Core Gap: No First-Class Plugin Contract

Right now:

* Plugins = CDI beans discovered at startup
* No lifecycle
* No compatibility checks
* No isolation
* No governance

### ğŸ”§ Introduce a Real Plugin SPI (Non-negotiable)

Add a **core plugin abstraction**:

```java
public interface InferencePlugin {

    PluginDescriptor descriptor();

    /**
     * Called once at startup
     */
    void initialize(PluginContext context);

    /**
     * Called before each inference request
     */
    default void beforeInference(InferenceHookContext ctx) {}

    /**
     * Called after successful inference
     */
    default void afterInference(InferenceHookContext ctx, InferenceResponse response) {}

    /**
     * Called on inference failure
     */
    default void onFailure(InferenceHookContext ctx, Throwable error) {}

    /**
     * Health check for the plugin itself
     */
    default HealthStatus health() {
        return HealthStatus.healthy();
    }

    /**
     * Graceful shutdown
     */
    void shutdown();
}
```

```java
public record PluginDescriptor(
    String id,
    String name,
    String version,
    PluginType type,
    Set<PluginCapability> capabilities,
    SemanticVersion minEngineVersion,
    SemanticVersion maxEngineVersion
) {}
```

This makes plugins:

* Versioned
* Governed
* Observable
* Optional
* Replaceable

---

## 3ï¸âƒ£ Execution Lifecycle Plugins (Reliability Boost)

Right now, orchestration logic is **hardcoded** in `InferenceOrchestrator`.

### Extract execution hooks

Introduce **execution phase plugins**:

```java
public enum InferencePhase {
    REQUEST_RECEIVED,
    MODEL_SELECTED,
    RUNNER_SELECTED,
    PRE_EXECUTION,
    POST_EXECUTION,
    RESPONSE_SERIALIZED
}
```

```java
public interface InferencePhasePlugin extends InferencePlugin {
    void onPhase(InferencePhase phase, InferenceHookContext ctx);
}
```

### What this enables

You can add plugins for:

* Retry policies
* Adaptive timeouts
* Shadow traffic
* Canary execution
* Request mutation
* Feature flags
* Chaos testing
* Rate limiting (remove from REST filter!)

ğŸ’¡ **Key idea**:

> The orchestrator should *emit events*, not *own behavior*.

---

## 4ï¸âƒ£ Security & Policy as Plugins (Critical for Enterprise)

Right now:

* Security is infrastructure-bound
* Policies are implicit

### Add Policy Plugins

```java
public interface PolicyPlugin extends InferencePlugin {

    PolicyDecision evaluate(InferencePolicyContext ctx);
}
```

```java
public enum PolicyDecision {
    ALLOW,
    DENY,
    REQUIRE_APPROVAL,
    RATE_LIMIT
}
```

Use cases:

* Tenant quota enforcement
* Data residency rules
* Model usage permissions
* Cost ceilings
* Sensitive prompt blocking
* Regulated industry controls

âš ï¸ This keeps **security out of adapters and runners**.

---

## 5ï¸âƒ£ Validation & Compatibility Plugins (Future-Proofing)

Today:

* Model compatibility logic is scattered
* No formal validation pipeline

### Add Model Validation Plugins

```java
public interface ModelValidationPlugin extends InferencePlugin {

    ValidationResult validate(ModelManifest manifest);
}
```

Examples:

* GGUF quant compatibility
* ONNX opset support
* GPU memory sufficiency
* Cross-version schema checks
* Deprecated format detection

This prevents:

* Bad model uploads
* Runtime crashes
* Silent performance degradation

---

## 6ï¸âƒ£ Control Plane & Hot-Reconfiguration Plugins

Right now:

* Config changes require restart or redeploy
* No plugin-level config updates

### Introduce Dynamic Plugin Configuration

```java
public interface ConfigurablePlugin {

    void onConfigUpdate(Map<String, Object> newConfig);
}
```

Add a **PluginRegistry**:

```java
public interface PluginRegistry {

    List<InferencePlugin> all();

    <T extends InferencePlugin> List<T> byType(Class<T> type);

    Optional<InferencePlugin> byId(String id);

    void reload(String pluginId);
}
```

Now you can:

* Enable/disable plugins per tenant
* Roll out new policies live
* Inject A/B logic without downtime

---

## 7ï¸âƒ£ Packaging & Versioning Strategy (This Is Huge)

### âŒ Avoid

* â€œJust another moduleâ€
* CDI-only discovery
* Fat JAR plugins

### âœ… Do This Instead

**Plugin packaging standard**:

```
inference-plugin-*.jar
â””â”€â”€ META-INF/
    â”œâ”€â”€ inference-plugin.json
    â””â”€â”€ services/
        â””â”€â”€ com.enterprise.inference.plugin.InferencePlugin
```

```json
{
  "id": "cost-guard",
  "name": "Cost Guard Plugin",
  "version": "1.2.0",
  "type": "POLICY",
  "minEngineVersion": "2.0.0",
  "capabilities": ["RATE_LIMIT", "COST_CONTROL"]
}
```

Support:

* Semantic versioning
* Engine compatibility checks
* Controlled rollout
* Signed plugins (later)

---

## 8ï¸âƒ£ Final Mental Model (Where This Ends Up)

Think of your system as:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Inference Engine Core              â”‚
â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚ Orchestrator â”‚  â† emits phases  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚ Plugin System â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚ â”‚
â”‚         â”‚                        â”‚ â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”â”‚
â”‚ â”‚ Policy Pluginsâ”‚   â”‚ Observers â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ ModelRunner Adapters (SPI)    â”‚ â”‚
â”‚  â”‚ GGUF | ONNX | Triton | Cloud  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Adapters **execute**
Plugins **govern, observe, and control**

---

## ğŸš€ Summary: What to Enhance

### High-impact improvements

âœ” Add **InferencePlugin SPI**
âœ” Move behavior into **execution phase plugins**
âœ” Make **security & policy pluggable**
âœ” Add **model validation plugins**
âœ” Introduce **PluginRegistry + lifecycle**
âœ” Support **dynamic config & hot reload**
âœ” Formalize **plugin packaging & compatibility**



# ğŸ§  LLM Inference Kernel â€” FINAL RECAP

## ğŸ¯ Purpose (What This Kernel Is)

A **request-scoped runtime** that:

* Accepts an inference request
* Runs it through **deterministic phases**
* Applies **plugins** (validation, policy, safety, transformation)
* Dispatches to an **LLM provider**
* Returns a **blocking or streaming response**

Nothing more.

---

## ğŸ§± Core Mental Model

```
InferenceEngine
  â””â”€â”€ InferencePipeline
        â””â”€â”€ InferencePhase[]
              â””â”€â”€ InferencePhasePlugin[]
                    â””â”€â”€ LLMProvider
```

Single request â†’ linear execution â†’ response.

---

## 1ï¸âƒ£ InferenceEngine (Entry Point)

```java
public interface InferenceEngine {

    InferenceResponse infer(InferenceRequest request);
}
```

* Stateless
* Thread-safe
* One request in â†’ one response out

---

## 2ï¸âƒ£ InferenceRequest (Input)

```java
public final class InferenceRequest {

    private final String model;
    private final List<Message> messages;
    private final Map<String, Object> parameters;
    private final boolean streaming;
}
```

* Provider-agnostic
* Immutable
* Safe to log / audit

---

## 3ï¸âƒ£ InferenceContext (Per Request)

```java
public interface InferenceContext {

    String requestId();

    InferenceRequest request();

    InferenceResponse response();

    Map<String, Object> attributes();

    void setResponse(InferenceResponse response);

    void fail(Throwable error);
}
```

* Exists **only during infer()**
* No persistence
* No resumption

---

## 4ï¸âƒ£ InferencePhase (Deterministic Order)

```java
public enum InferencePhase {

    VALIDATION,
    PRE_PROCESSING,
    PROVIDER_DISPATCH,
    POST_PROCESSING;

    public static List<InferencePhase> ordered() {
        return List.of(values());
    }
}
```

* Linear
* No branching
* No looping

---

## 5ï¸âƒ£ Plugin System (Strict & Minimal)

### Plugin Hierarchy (LOCKED)

```
Plugin
 â””â”€â”€ ConfigurablePlugin
       â””â”€â”€ InferencePhasePlugin
             â””â”€â”€ ModelValidationPlugin
```

### Base Plugin

```java
public interface Plugin {

    String id();
    int order();

    default void initialize(EngineContext context) {}
    default void shutdown() {}
}
```

---

### InferencePhasePlugin

```java
public interface InferencePhasePlugin
        extends ConfigurablePlugin {

    InferencePhase phase();

    void execute(
        InferenceContext context,
        EngineContext engine
    );
}
```

* Phase-bound
* Deterministic
* No provider calls

---

## 6ï¸âƒ£ InferencePipeline (Phase Executor)

```java
public interface InferencePipeline {

    void execute(InferenceContext context);
}
```

```java
public final class DefaultInferencePipeline
        implements InferencePipeline {

    private final Map<
        InferencePhase,
        List<InferencePhasePlugin>
    > plugins;

    @Override
    public void execute(InferenceContext context) {

        for (InferencePhase phase : InferencePhase.ordered()) {
            for (InferencePhasePlugin plugin : plugins.get(phase)) {
                plugin.execute(context, context.engine());
            }
        }
    }
}
```

---

## 7ï¸âƒ£ LLM Provider Abstraction

### LLMProvider

```java
public interface LLMProvider {

    String id();

    ProviderCapabilities capabilities();

    InferenceResponse infer(ProviderRequest request);
}
```

### ProviderCapabilities

```java
public final class ProviderCapabilities {

    private final boolean streaming;
    private final boolean tools;
    private final boolean multimodal;
    private final int maxContextTokens;
}
```

---

## 8ï¸âƒ£ Provider Dispatch (Normalized)

```java
public final class ProviderRequest {

    private final String model;
    private final List<Message> messages;
    private final Map<String, Object> parameters;
    private final boolean streaming;
}
```

* Mapped per provider
* Transport-agnostic

---

## 9ï¸âƒ£ Streaming Support (Optional)

### StreamingLLMProvider

```java
public interface StreamingLLMProvider
        extends LLMProvider {

    StreamingResponse stream(ProviderRequest request);
}
```

### StreamingResponse

```java
public final class StreamingResponse
        implements InferenceResponse {

    private final Publisher<StreamChunk> publisher;
}
```

---

## ğŸ”Ÿ Observability (Hooks Only)

### InferenceObserver

```java
public interface InferenceObserver {

    void onStart(InferenceContext context);
    void onPhase(InferencePhase phase, InferenceContext context);
    void onSuccess(InferenceContext context);
    void onFailure(Throwable error, InferenceContext context);
}
```

* Metrics
* Tracing
* Logging

Kernel never logs directly.

---

## 1ï¸âƒ£1ï¸âƒ£ Safety & Policy (Plugins)

* Prompt validation
* Output moderation
* Policy enforcement
* Quotas / rate limits

All implemented as **InferencePhasePlugin**.

---

## 1ï¸âƒ£2ï¸âƒ£ What Is Explicitly NOT in the Kernel

âŒ Workflow / BPMN
âŒ Orchestration
âŒ Agent runtime
âŒ Long-running state
âŒ Persistence
âŒ Human-in-the-loop
âŒ Business semantics

Those belong to **golek higher layers**, not here.

---

## ğŸ§© Final Kernel Boundary

```
golek-inference-kernel
â”œâ”€â”€ engine
â”œâ”€â”€ pipeline
â”œâ”€â”€ phases
â”œâ”€â”€ plugins
â”œâ”€â”€ providers
â”œâ”€â”€ streaming
â”œâ”€â”€ observability
â””â”€â”€ safety
```

This kernel is:

âœ” coherent
âœ” minimal
âœ” extensible
âœ” production-grade