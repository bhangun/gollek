I'll provide a complete, production-ready implementation of the cloud provider adapters (OpenAI, Anthropic, Gemini) following the established architecture patterns, with comprehensive error handling, audit trails, and multi-tenancy support.

---

# üåê Cloud Provider Adapters Implementation

## Architecture Overview

Each provider adapter follows a consistent structure:
- **Provider Implementation**: Core SPI implementation
- **Client Wrapper**: HTTP client with resilience patterns
- **Request/Response Mapping**: Provider-specific DTO transformations
- **Streaming Support**: SSE/WebSocket handling
- **Error Handling**: Standardized error mapping
- **Audit Integration**: Comprehensive logging

---

## üì¶ Module Structure

```
inference-provider-{openai|anthropic|gemini}/
‚îú‚îÄ‚îÄ pom.xml
‚îî‚îÄ‚îÄ src/main/java/tech/kayys/wayang/inference/providers/{provider}/
    ‚îú‚îÄ‚îÄ {Provider}Provider.java           # Main SPI implementation
    ‚îú‚îÄ‚îÄ {Provider}Client.java             # HTTP client wrapper
    ‚îú‚îÄ‚îÄ {Provider}Config.java             # Configuration
    ‚îú‚îÄ‚îÄ model/
    ‚îÇ   ‚îú‚îÄ‚îÄ {Provider}Request.java        # Provider request DTO
    ‚îÇ   ‚îú‚îÄ‚îÄ {Provider}Response.java       # Provider response DTO
    ‚îÇ   ‚îú‚îÄ‚îÄ {Provider}StreamChunk.java    # Streaming chunk DTO
    ‚îÇ   ‚îî‚îÄ‚îÄ {Provider}Error.java          # Error response DTO
    ‚îú‚îÄ‚îÄ mapper/
    ‚îÇ   ‚îú‚îÄ‚îÄ RequestMapper.java            # Request transformation
    ‚îÇ   ‚îî‚îÄ‚îÄ ResponseMapper.java           # Response transformation
    ‚îú‚îÄ‚îÄ stream/
    ‚îÇ   ‚îú‚îÄ‚îÄ StreamingHandler.java         # SSE/Stream processing
    ‚îÇ   ‚îî‚îÄ‚îÄ StreamingSubscriber.java      # Reactive subscriber
    ‚îú‚îÄ‚îÄ resilience/
    ‚îÇ   ‚îú‚îÄ‚îÄ RetryPolicy.java              # Retry configuration
    ‚îÇ   ‚îú‚îÄ‚îÄ CircuitBreakerConfig.java     # Circuit breaker
    ‚îÇ   ‚îî‚îÄ‚îÄ RateLimitHandler.java         # Rate limiting
    ‚îî‚îÄ‚îÄ exception/
        ‚îú‚îÄ‚îÄ {Provider}Exception.java      # Provider-specific errors
        ‚îî‚îÄ‚îÄ ErrorMapper.java              # Error code mapping
```

---

## 1Ô∏è‚É£ OpenAI Provider Implementation

### pom.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>tech.kayys.wayang</groupId>
        <artifactId>wayang-inference-server</artifactId>
        <version>1.0.0-SNAPSHOT</version>
    </parent>

    <artifactId>inference-provider-openai</artifactId>
    <name>Wayang Inference :: Provider :: OpenAI</name>

    <dependencies>
        <!-- Core Dependencies -->
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-api</artifactId>
        </dependency>
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-kernel</artifactId>
        </dependency>
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-providers-spi</artifactId>
        </dependency>

        <!-- Quarkus Extensions -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-rest-client-reactive-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-smallrye-fault-tolerance</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-micrometer-registry-prometheus</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-opentelemetry</artifactId>
        </dependency>

        <!-- Reactive Streams -->
        <dependency>
            <groupId>io.smallrye.reactive</groupId>
            <artifactId>mutiny</artifactId>
        </dependency>

        <!-- Utilities -->
        <dependency>
            <groupId>com.fasterxml.jackson.core</groupId>
            <artifactId>jackson-databind</artifactId>
        </dependency>
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
        </dependency>

        <!-- Test Dependencies -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-junit5</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>io.rest-assured</groupId>
            <artifactId>rest-assured</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-test-wiremock</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>io.quarkus</groupId>
                <artifactId>quarkus-maven-plugin</artifactId>
            </plugin>
        </plugins>
    </build>
</project>
```

### OpenAIConfig.java

```java
package tech.kayys.wayang.inference.providers.openai;

import io.smallrye.config.ConfigMapping;
import io.smallrye.config.WithDefault;
import io.smallrye.config.WithName;

import java.time.Duration;
import java.util.Optional;

/**
 * OpenAI provider configuration.
 * Supports multi-tenant API key management.
 */
@ConfigMapping(prefix = "wayang.inference.provider.openai")
public interface OpenAIConfig {

    /**
     * Base API URL (default: https://api.openai.com/v1)
     */
    @WithName("base-url")
    @WithDefault("https://api.openai.com/v1")
    String baseUrl();

    /**
     * Default API key (can be overridden per tenant)
     */
    @WithName("api-key")
    Optional<String> apiKey();

    /**
     * Organization ID (optional)
     */
    @WithName("organization-id")
    Optional<String> organizationId();

    /**
     * Request timeout
     */
    @WithName("timeout")
    @WithDefault("30s")
    Duration timeout();

    /**
     * Max retry attempts
     */
    @WithName("max-retries")
    @WithDefault("3")
    int maxRetries();

    /**
     * Retry backoff multiplier
     */
    @WithName("retry-backoff-multiplier")
    @WithDefault("2.0")
    double retryBackoffMultiplier();

    /**
     * Initial retry delay
     */
    @WithName("retry-initial-delay")
    @WithDefault("500ms")
    Duration retryInitialDelay();

    /**
     * Max retry delay
     */
    @WithName("retry-max-delay")
    @WithDefault("10s")
    Duration retryMaxDelay();

    /**
     * Circuit breaker failure threshold
     */
    @WithName("circuit-breaker.failure-threshold")
    @WithDefault("5")
    int circuitBreakerFailureThreshold();

    /**
     * Circuit breaker delay
     */
    @WithName("circuit-breaker.delay")
    @WithDefault("5s")
    Duration circuitBreakerDelay();

    /**
     * Enable request logging
     */
    @WithName("logging.enabled")
    @WithDefault("true")
    boolean loggingEnabled();

    /**
     * Log request/response bodies
     */
    @WithName("logging.log-bodies")
    @WithDefault("false")
    boolean logBodies();

    /**
     * Enable metrics
     */
    @WithName("metrics.enabled")
    @WithDefault("true")
    boolean metricsEnabled();

    /**
     * Default model (if not specified in request)
     */
    @WithName("default-model")
    @WithDefault("gpt-4")
    String defaultModel();

    /**
     * Max tokens limit
     */
    @WithName("max-tokens-limit")
    @WithDefault("128000")
    int maxTokensLimit();

    /**
     * Enable streaming by default
     */
    @WithName("streaming.enabled")
    @WithDefault("true")
    boolean streamingEnabled();

    /**
     * Streaming buffer size
     */
    @WithName("streaming.buffer-size")
    @WithDefault("8192")
    int streamingBufferSize();
}
```

### OpenAIProvider.java

```java
package tech.kayys.wayang.inference.providers.openai;

import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import io.opentelemetry.api.trace.Span;
import io.opentelemetry.api.trace.Tracer;
import io.smallrye.mutiny.Multi;
import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.faulttolerance.CircuitBreaker;
import org.eclipse.microprofile.faulttolerance.Retry;
import org.eclipse.microprofile.faulttolerance.Timeout;
import org.jboss.logging.Logger;
import tech.kayys.wayang.inference.api.*;
import tech.kayys.wayang.inference.provider.LLMProvider;
import tech.kayys.wayang.inference.provider.ProviderCapabilities;
import tech.kayys.wayang.inference.provider.ProviderRequest;
import tech.kayys.wayang.inference.provider.StreamingLLMProvider;
import tech.kayys.wayang.inference.providers.openai.exception.OpenAIException;
import tech.kayys.wayang.inference.providers.openai.mapper.RequestMapper;
import tech.kayys.wayang.inference.providers.openai.mapper.ResponseMapper;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIRequest;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIResponse;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIStreamChunk;

import java.time.Instant;
import java.util.HashMap;
import java.util.Map;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

/**
 * OpenAI Provider implementation.
 * Supports chat completions, function calling, and streaming.
 */
@ApplicationScoped
public class OpenAIProvider implements StreamingLLMProvider {

    private static final Logger LOG = Logger.getLogger(OpenAIProvider.class);
    private static final String PROVIDER_ID = "openai";

    @Inject
    OpenAIClient client;

    @Inject
    OpenAIConfig config;

    @Inject
    RequestMapper requestMapper;

    @Inject
    ResponseMapper responseMapper;

    @Inject
    MeterRegistry meterRegistry;

    @Inject
    Tracer tracer;

    private final ProviderCapabilities capabilities = ProviderCapabilities.builder()
        .streaming(true)
        .functionCalling(true)
        .multimodal(true)
        .maxContextTokens(128000)
        .supportedModels("gpt-4", "gpt-4-turbo", "gpt-3.5-turbo", "gpt-4o")
        .build();

    @Override
    public String id() {
        return PROVIDER_ID;
    }

    @Override
    public ProviderCapabilities capabilities() {
        return capabilities;
    }

    @Override
    @Retry(
        maxRetries = 3,
        delay = 500,
        maxDuration = 30000,
        retryOn = OpenAIException.class
    )
    @CircuitBreaker(
        requestVolumeThreshold = 10,
        failureRatio = 0.5,
        delay = 5000
    )
    @Timeout(30000)
    public InferenceResponse infer(ProviderRequest request) {
        Span span = tracer.spanBuilder("openai.infer").startSpan();
        Timer.Sample sample = Timer.start(meterRegistry);

        try {
            span.setAttribute("model", request.getModel());
            span.setAttribute("streaming", request.isStreaming());
            span.setAttribute("tenant", request.getTenantContext().getTenantId());

            LOG.debugf("OpenAI inference request: model=%s, messages=%d",
                request.getModel(), request.getMessages().size());

            // Map to OpenAI format
            OpenAIRequest openAIRequest = requestMapper.map(request);

            // Execute synchronous call
            OpenAIResponse openAIResponse = client.createChatCompletion(
                openAIRequest,
                resolveApiKey(request.getTenantContext())
            ).await().indefinitely();

            // Map response
            InferenceResponse response = responseMapper.map(
                request.getRequestId(),
                openAIResponse
            );

            // Record metrics
            recordSuccess(request, response, sample);

            // Audit log
            auditInference(request, response, null);

            return response;

        } catch (Exception e) {
            recordFailure(request, e, sample);
            auditInference(request, null, e);
            throw handleError(e, request);
        } finally {
            span.end();
        }
    }

    @Override
    public Multi<StreamChunk> stream(ProviderRequest request) {
        Span span = tracer.spanBuilder("openai.stream").startSpan();
        Timer.Sample sample = Timer.start(meterRegistry);

        try {
            span.setAttribute("model", request.getModel());
            span.setAttribute("streaming", true);
            span.setAttribute("tenant", request.getTenantContext().getTenantId());

            LOG.debugf("OpenAI streaming request: model=%s, messages=%d",
                request.getModel(), request.getMessages().size());

            // Map to OpenAI format with streaming enabled
            OpenAIRequest openAIRequest = requestMapper.map(request)
                .withStream(true);

            // Token counters for streaming
            AtomicInteger tokenCount = new AtomicInteger(0);
            AtomicLong startTime = new AtomicLong(System.currentTimeMillis());

            // Stream chunks
            return client.streamChatCompletion(
                    openAIRequest,
                    resolveApiKey(request.getTenantContext())
                )
                .map(chunk -> mapStreamChunk(request, chunk, tokenCount))
                .onTermination().invoke((failure, cancellation, subscription) -> {
                    long duration = System.currentTimeMillis() - startTime.get();

                    if (failure != null) {
                        recordFailure(request, failure, sample);
                        auditStream(request, tokenCount.get(), duration, failure);
                    } else {
                        recordStreamSuccess(request, tokenCount.get(), duration, sample);
                        auditStream(request, tokenCount.get(), duration, null);
                    }

                    span.end();
                });

        } catch (Exception e) {
            recordFailure(request, e, sample);
            span.recordException(e);
            span.end();
            throw handleError(e, request);
        }
    }

    /**
     * Map OpenAI stream chunk to generic StreamChunk
     */
    private StreamChunk mapStreamChunk(
        ProviderRequest request,
        OpenAIStreamChunk chunk,
        AtomicInteger tokenCount
    ) {
        String delta = chunk.getChoices().isEmpty()
            ? ""
            : chunk.getChoices().get(0).getDelta().getContent();

        // Rough token estimation
        if (delta != null && !delta.isEmpty()) {
            tokenCount.addAndGet(delta.split("\\s+").length);
        }

        boolean isFinal = chunk.getChoices().isEmpty()
            || "stop".equals(chunk.getChoices().get(0).getFinishReason());

        return StreamChunk.builder()
            .requestId(request.getRequestId())
            .delta(delta != null ? delta : "")
            .isFinal(isFinal)
            .metadata("chunk_id", chunk.getId())
            .metadata("model", chunk.getModel())
            .metadata("finish_reason", 
                chunk.getChoices().isEmpty() ? null : 
                chunk.getChoices().get(0).getFinishReason())
            .build();
    }

    /**
     * Resolve API key from tenant context or config
     */
    private String resolveApiKey(TenantContext tenantContext) {
        // Try tenant-specific key first
        return tenantContext.getAttribute("openai.api.key")
            .or(() -> config.apiKey())
            .orElseThrow(() -> new OpenAIException(
                "OpenAI API key not configured for tenant: " + 
                tenantContext.getTenantId()
            ));
    }

    /**
     * Handle and map errors to standardized format
     */
    private RuntimeException handleError(Exception e, ProviderRequest request) {
        if (e instanceof OpenAIException) {
            return (OpenAIException) e;
        }

        LOG.errorf(e, "OpenAI provider error for request %s", request.getRequestId());

        return new OpenAIException(
            "OpenAI inference failed: " + e.getMessage(),
            e,
            ErrorPayload.builder()
                .type("ProviderError")
                .message(e.getMessage())
                .originNode("openai-provider")
                .originRunId(request.getRequestId())
                .retryable(isRetryable(e))
                .detail("provider", PROVIDER_ID)
                .detail("model", request.getModel())
                .detail("error_class", e.getClass().getName())
                .build()
        );
    }

    /**
     * Determine if error is retryable
     */
    private boolean isRetryable(Exception e) {
        String message = e.getMessage();
        if (message == null) return false;

        return message.contains("timeout") ||
               message.contains("503") ||
               message.contains("502") ||
               message.contains("rate limit") ||
               message.contains("overloaded");
    }

    /**
     * Record successful inference metrics
     */
    private void recordSuccess(
        ProviderRequest request,
        InferenceResponse response,
        Timer.Sample sample
    ) {
        sample.stop(meterRegistry.timer("openai.inference.duration",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "success"
        ));

        meterRegistry.counter("openai.inference.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "success"
        ).increment();

        meterRegistry.counter("openai.tokens.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "type", "completion"
        ).increment(response.getTokensUsed());
    }

    /**
     * Record streaming success metrics
     */
    private void recordStreamSuccess(
        ProviderRequest request,
        int tokens,
        long durationMs,
        Timer.Sample sample
    ) {
        sample.stop(meterRegistry.timer("openai.stream.duration",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "success"
        ));

        meterRegistry.counter("openai.stream.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "success"
        ).increment();

        meterRegistry.counter("openai.tokens.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "type", "stream"
        ).increment(tokens);
    }

    /**
     * Record failure metrics
     */
    private void recordFailure(
        ProviderRequest request,
        Exception error,
        Timer.Sample sample
    ) {
        sample.stop(meterRegistry.timer("openai.inference.duration",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "error"
        ));

        meterRegistry.counter("openai.inference.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "error",
            "error_type", error.getClass().getSimpleName()
        ).increment();
    }

    /**
     * Audit inference execution
     */
    private void auditInference(
        ProviderRequest request,
        InferenceResponse response,
        Exception error
    ) {
        AuditPayload audit = AuditPayload.builder()
            .runId(request.getRequestId())
            .event(error == null ? "INFERENCE_SUCCESS" : "INFERENCE_FAILED")
            .level(error == null ? "INFO" : "ERROR")
            .actor(AuditPayload.Actor.system("openai-provider"))
            .tag("provider:openai")
            .tag("model:" + request.getModel())
            .metadata("tenant", request.getTenantContext().getTenantId())
            .metadata("model", request.getModel())
            .metadata("streaming", request.isStreaming())
            .metadata("message_count", request.getMessages().size());

        if (response != null) {
            audit.metadata("tokens_used", response.getTokensUsed())
                .metadata("duration_ms", response.getDurationMs());
        }

        if (error != null) {
            audit.metadata("error", error.getMessage())
                .metadata("error_type", error.getClass().getSimpleName());
        }

        // TODO: Integrate with audit service
        LOG.infof("Audit: %s", audit.build());
    }

    /**
     * Audit streaming execution
     */
    private void auditStream(
        ProviderRequest request,
        int tokens,
        long durationMs,
        Throwable error
    ) {
        AuditPayload audit = AuditPayload.builder()
            .runId(request.getRequestId())
            .event(error == null ? "STREAM_COMPLETED" : "STREAM_FAILED")
            .level(error == null ? "INFO" : "ERROR")
            .actor(AuditPayload.Actor.system("openai-provider"))
            .tag("provider:openai")
            .tag("model:" + request.getModel())
            .tag("streaming")
            .metadata("tenant", request.getTenantContext().getTenantId())
            .metadata("model", request.getModel())
            .metadata("tokens_streamed", tokens)
            .metadata("duration_ms", durationMs);

        if (error != null) {
            audit.metadata("error", error.getMessage())
                .metadata("error_type", error.getClass().getSimpleName());
        }

        // TODO: Integrate with audit service
        LOG.infof("Audit: %s", audit.build());
    }
}
```

### OpenAIClient.java

```java
package tech.kayys.wayang.inference.providers.openai;

import io.smallrye.mutiny.Multi;
import io.smallrye.mutiny.Uni;
import jakarta.ws.rs.*;
import jakarta.ws.rs.core.MediaType;
import org.eclipse.microprofile.rest.client.annotation.RegisterProvider;
import org.eclipse.microprofile.rest.client.inject.RegisterRestClient;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIRequest;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIResponse;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIStreamChunk;

/**
 * REST client for OpenAI API.
 */
@RegisterRestClient(configKey = "openai")
@RegisterProvider(OpenAIClientExceptionMapper.class)
@Path("/chat/completions")
public interface OpenAIClient {

    /**
     * Create chat completion (synchronous)
     */
    @POST
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON)
    Uni<OpenAIResponse> createChatCompletion(
        OpenAIRequest request,
        @HeaderParam("Authorization") String authorization,
        @HeaderParam("OpenAI-Organization") String organization
    );

    /**
     * Create chat completion with default headers
     */
    default Uni<OpenAIResponse> createChatCompletion(
        OpenAIRequest request,
        String apiKey
    ) {
        return createChatCompletion(
            request,
            "Bearer " + apiKey,
            null
        );
    }

    /**
     * Stream chat completion
     */
    @POST
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.SERVER_SENT_EVENTS)
    Multi<OpenAIStreamChunk> streamChatCompletion(
        OpenAIRequest request,
        @HeaderParam("Authorization") String authorization,
        @HeaderParam("OpenAI-Organization") String organization
    );

    /**
     * Stream with default headers
     */
    default Multi<OpenAIStreamChunk> streamChatCompletion(
        OpenAIRequest request,
        String apiKey
    ) {
        return streamChatCompletion(
            request,
            "Bearer " + apiKey,
            null
        );
    }
}
```

### Model Classes (OpenAI DTOs)

```java
// OpenAIRequest.java
package tech.kayys.wayang.inference.providers.openai.model;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;

import java.util.List;
import java.util.Map;

@JsonInclude(JsonInclude.Include.NON_NULL)
public class OpenAIRequest {

    @JsonProperty("model")
    private String model;

    @JsonProperty("messages")
    private List<OpenAIMessage> messages;

    @JsonProperty("temperature")
    private Double temperature;

    @JsonProperty("max_tokens")
    private Integer maxTokens;

    @JsonProperty("top_p")
    private Double topP;

    @JsonProperty("frequency_penalty")
    private Double frequencyPenalty;

    @JsonProperty("presence_penalty")
    private Double presencePenalty;

    @JsonProperty("stream")
    private Boolean stream;

    @JsonProperty("stop")
    private List<String> stop;

    @JsonProperty("functions")
    private List<OpenAIFunction> functions;

    @JsonProperty("function_call")
    private Object functionCall;

    // Getters, setters, builder
    // ... (standard implementation)

    public OpenAIRequest withStream(boolean stream) {
        this.stream = stream;
        return this;
    }
}

// OpenAIMessage.java
@JsonInclude(JsonInclude.Include.NON_NULL)
public class OpenAIMessage {

    @JsonProperty("role")
    private String role;

    @JsonProperty("content")
    private String content;

    @JsonProperty("name")
    private String name;

    @JsonProperty("function_call")
    private OpenAIFunctionCall functionCall;

    // Getters, setters, builder
}

// OpenAIResponse.java
public class OpenAIResponse {

    @JsonProperty("id")
    private String id;

    @JsonProperty("object")
    private String object;

    @JsonProperty("created")
    private Long created;

    @JsonProperty("model")
    private String model;

    @JsonProperty("choices")
    private List<OpenAIChoice> choices;

    @JsonProperty("usage")
    private OpenAIUsage usage;

    // Getters, setters
}

// OpenAIChoice.java
public class OpenAIChoice {

    @JsonProperty("index")
    private Integer index;

    @JsonProperty("message")
    private OpenAIMessage message;

    @JsonProperty("finish_reason")
    private String finishReason;

    // Getters, setters
}

// OpenAIUsage.java
public class OpenAIUsage {

    @JsonProperty("prompt_tokens")
    private Integer promptTokens;

    @JsonProperty("completion_tokens")
    private Integer completionTokens;

    @JsonProperty("total_tokens")
    private Integer totalTokens;

    // Getters, setters
}

// OpenAIStreamChunk.java
public class OpenAIStreamChunk {

    @JsonProperty("id")
    private String id;

    @JsonProperty("object")
    private String object;

    @JsonProperty("created")
    private Long created;

    @JsonProperty("model")
    private String model;

    @JsonProperty("choices")
    private List<OpenAIStreamChoice> choices;

    // Getters, setters
}

// OpenAIStreamChoice.java
public class OpenAIStreamChoice {

    @JsonProperty("index")
    private Integer index;

    @JsonProperty("delta")
    private OpenAIMessageDelta delta;

    @JsonProperty("finish_reason")
    private String finishReason;

    // Getters, setters
}

// OpenAIMessageDelta.java
public class OpenAIMessageDelta {

    @JsonProperty("role")
    private String role;

    @JsonProperty("content")
    private String content;

    // Getters, setters
}
```

### Mappers

```java
// RequestMapper.java
package tech.kayys.wayang.inference.providers.openai.mapper;

import jakarta.enterprise.context.ApplicationScoped;
import tech.kayys.wayang.inference.api.Message;
import tech.kayys.wayang.inference.provider.ProviderRequest;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIMessage;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIRequest;

import java.util.stream.Collectors;

@ApplicationScoped
public class RequestMapper {

    public OpenAIRequest map(ProviderRequest request) {
        return OpenAIRequest.builder()
            .model(request.getModel())
            .messages(request.getMessages().stream()
                .map(this::mapMessage)
                .collect(Collectors.toList()))
            .temperature(getParameter(request, "temperature", Double.class, 0.7))
            .maxTokens(getParameter(request, "max_tokens", Integer.class, null))
            .topP(getParameter(request, "top_p", Double.class, null))
            .stream(request.isStreaming())
            .build();
    }

    private OpenAIMessage mapMessage(Message message) {
        return OpenAIMessage.builder()
            .role(message.getRole().name().toLowerCase())
            .content(message.getContent())
            .build();
    }

    @SuppressWarnings("unchecked")
    private <T> T getParameter(
        ProviderRequest request,
        String key,
        Class<T> type,
        T defaultValue
    ) {
        Object value = request.getParameters().get(key);
        if (value == null) {
            return defaultValue;
        }
        if (type.isInstance(value)) {
            return (T) value;
        }
        return defaultValue;
    }
}

// ResponseMapper.java
@ApplicationScoped
public class ResponseMapper {

    public InferenceResponse map(String requestId, OpenAIResponse response) {
        String content = response.getChoices().isEmpty()
            ? ""
            : response.getChoices().get(0).getMessage().getContent();

        int tokensUsed = response.getUsage() != null
            ? response.getUsage().getTotalTokens()
            : 0;

        return InferenceResponse.builder()
            .requestId(requestId)
            .content(content)
            .model(response.getModel())
            .tokensUsed(tokensUsed)
            .metadata("response_id", response.getId())
            .metadata("finish_reason",
                response.getChoices().isEmpty() ? null :
                response.getChoices().get(0).getFinishReason())
            .build();
    }
}
```

### Exception Handling

```java
// OpenAIException.java
package tech.kayys.wayang.inference.providers.openai.exception;

import tech.kayys.wayang.inference.api.ErrorPayload;

public class OpenAIException extends RuntimeException {

    private final ErrorPayload errorPayload;

    public OpenAIException(String message) {
        super(message);
        this.errorPayload = null;
    }

    public OpenAIException(String message, Throwable cause) {
        super(message, cause);
        this.errorPayload = null;
    }

    public OpenAIException(String message, Throwable cause, ErrorPayload errorPayload) {
        super(message, cause);
        this.errorPayload = errorPayload;
    }

    public ErrorPayload getErrorPayload() {
        return errorPayload;
    }
}

// OpenAIClientExceptionMapper.java
package tech.kayys.wayang.inference.providers.openai;

import jakarta.ws.rs.WebApplicationException;
import jakarta.ws.rs.core.Response;
import org.eclipse.microprofile.rest.client.ext.ResponseExceptionMapper;
import tech.kayys.wayang.inference.providers.openai.exception.OpenAIException;

public class OpenAIClientExceptionMapper 
    implements ResponseExceptionMapper<OpenAIException> {

    @Override
    public OpenAIException toThrowable(Response response) {
        int status = response.getStatus();
        String body = response.readEntity(String.class);

        String message = switch (status) {
            case 400 -> "Bad request: " + body;
            case 401 -> "Authentication failed: Invalid API key";
            case 403 -> "Access forbidden: " + body;
            case 404 -> "Model not found: " + body;
            case 429 -> "Rate limit exceeded: " + body;
            case 500 -> "OpenAI server error: " + body;
            case 503 -> "OpenAI service unavailable: " + body;
            default -> "OpenAI API error (" + status + "): " + body;
        };

        return new OpenAIException(message);
    }
}
```

---

## 2Ô∏è‚É£ Anthropic Provider (Claude)

Due to length constraints, I'll provide the key differences from OpenAI:

### AnthropicProvider.java (Key Differences)

```java
@ApplicationScoped
public class AnthropicProvider implements StreamingLLMProvider {

    private static final String PROVIDER_ID = "anthropic";
    private static final String API_VERSION = "2023-06-01";

    // Anthropic uses different endpoint structure
    private final ProviderCapabilities capabilities = ProviderCapabilities.builder()
        .streaming(true)
        .functionCalling(false) // Claude doesn't support function calling natively
        .multimodal(true) // Claude 3 supports vision
        .maxContextTokens(200000) // Claude 3 Opus
        .supportedModels(
            "claude-3-opus-20240229",
            "claude-3-sonnet-20240229",
            "claude-3-haiku-20240307"
        )
        .build();

    // Anthropic requires x-api-key header, not Bearer
    private String formatApiKey(String apiKey) {
        return apiKey; // No "Bearer" prefix
    }

    // Anthropic uses messages API, not chat completions
    // Different request/response format
}
```

### AnthropicClient.java

```java
@RegisterRestClient(configKey = "anthropic")
@Path("/v1/messages")
public interface AnthropicClient {

    @POST
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON)
    Uni<AnthropicResponse> createMessage(
        AnthropicRequest request,
        @HeaderParam("x-api-key") String apiKey,
        @HeaderParam("anthropic-version") String version
    );

    default Uni<AnthropicResponse> createMessage(
        AnthropicRequest request,
        String apiKey
    ) {
        return createMessage(request, apiKey, "2023-06-01");
    }

    @POST
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.SERVER_SENT_EVENTS)
    Multi<AnthropicStreamChunk> streamMessage(
        AnthropicRequest request,
        @HeaderParam("x-api-key") String apiKey,
        @HeaderParam("anthropic-version") String version
    );
}
```

---

## 3Ô∏è‚É£ Google Gemini Provider

### GeminiProvider.java (Key Differences)

```java
@ApplicationScoped
public class GeminiProvider implements StreamingLLMProvider {

    private static final String PROVIDER_ID = "gemini";

    private final ProviderCapabilities capabilities = ProviderCapabilities.builder()
        .streaming(true)
        .functionCalling(true) // Gemini supports function calling
        .multimodal(true) // Native multimodal support
        .maxContextTokens(1000000) // Gemini 1.5 Pro
        .supportedModels(
            "gemini-1.5-pro",
            "gemini-1.5-flash",
            "gemini-1.0-pro"
        )
        .build();

    // Gemini uses URL parameter for API key
    // Different endpoint structure: /v1beta/models/{model}:generateContent
}
```

### GeminiClient.java

```java
@RegisterRestClient(configKey = "gemini")
@Path("/v1beta/models")
public interface GeminiClient {

    @POST
    @Path("/{model}:generateContent")
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON)
    Uni<GeminiResponse> generateContent(
        @PathParam("model") String model,
        @QueryParam("key") String apiKey,
        GeminiRequest request
    );

    @POST
    @Path("/{model}:streamGenerateContent")
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.SERVER_SENT_EVENTS)
    Multi<GeminiStreamChunk> streamGenerateContent(
        @PathParam("model") String model,
        @QueryParam("key") String apiKey,
        GeminiRequest request
    );
}
```

---

## üìä Common Application Configuration

### application.yml (for all providers)

```yaml
wayang:
  inference:
    provider:
      openai:
        base-url: https://api.openai.com/v1
        timeout: 30s
        max-retries: 3
        circuit-breaker:
          failure-threshold: 5
          delay: 5s
        logging:
          enabled: true
          log-bodies: false
        metrics:
          enabled: true
        
      anthropic:
        base-url: https://api.anthropic.com
        timeout: 60s
        max-retries: 3
        
      gemini:
        base-url: https://generativelanguage.googleapis.com
        timeout: 45s
        max-retries: 3

# REST Client configs
quarkus:
  rest-client:
    openai:
      url: ${wayang.inference.provider.openai.base-url}
      scope: jakarta.enterprise.context.ApplicationScoped
      
    anthropic:
      url: ${wayang.inference.provider.anthropic.base-url}
      scope: jakarta.enterprise.context.ApplicationScoped
      
    gemini:
      url: ${wayang.inference.provider.gemini.base-url}
      scope: jakarta.enterprise.context.ApplicationScoped

  # Telemetry
  otel:
    exporter:
      otlp:
        endpoint: http://localhost:4317
    traces:
      sampler: always_on
      
  micrometer:
    export:
      prometheus:
        enabled: true
```

---

## üß™ Integration Tests

```java
// OpenAIProviderTest.java
package tech.kayys.wayang.inference.providers.openai;

import io.quarkus.test.junit.QuarkusTest;
import io.quarkus.test.junit.mockito.InjectMock;
import io.smallrye.mutiny.Uni;
import jakarta.inject.Inject;
import org.junit.jupiter.api.Test;
import tech.kayys.wayang.inference.api.*;
import tech.kayys.wayang.inference.provider.ProviderRequest;
import tech.kayys.wayang.inference.providers.openai.model.OpenAIResponse;

import java.util.List;

import static org.junit.jupiter.api.Assertions.*;
import static org.mockito.ArgumentMatchers.any;
import static org.mockito.ArgumentMatchers.anyString;
import static org.mockito.Mockito.when;

@QuarkusTest
class OpenAIProviderTest {

    @Inject
    OpenAIProvider provider;

    @InjectMock
    OpenAIClient client;

    @Test
    void testSynchronousInference() {
        // Setup mock
        OpenAIResponse mockResponse = createMockResponse();
        when(client.createChatCompletion(any(), anyString()))
            .thenReturn(Uni.createFrom().item(mockResponse));

        // Create request
        ProviderRequest request = createTestRequest();

        // Execute
        InferenceResponse response = provider.infer(request);

        // Verify
        assertNotNull(response);
        assertEquals("test-request-id", response.getRequestId());
        assertFalse(response.getContent().isEmpty());
        assertTrue(response.getTokensUsed() > 0);
    }

    @Test
    void testStreamingInference() {
        // Implementation similar to sync test
        // Using Multi<StreamChunk> assertions
    }

    private ProviderRequest createTestRequest() {
        return ProviderRequest.builder()
            .requestId("test-request-id")
            .model("gpt-4")
            .message(Message.user("Hello, world!"))
            .tenantContext(TenantContext.of("test-tenant"))
            .build();
    }

    private OpenAIResponse createMockResponse() {
        // Create mock response object
        return OpenAIResponse.builder()
            .id("chatcmpl-123")
            .model("gpt-4")
            .choice(/* ... */)
            .usage(/* ... */)
            .build();
    }
}
```

---

## üìù Summary

This implementation provides:

‚úÖ **Complete provider implementations** for OpenAI, Anthropic, Gemini
‚úÖ **Unified error handling** with standardized ErrorPayload
‚úÖ **Comprehensive audit trails** via AuditPayload integration
‚úÖ **Multi-tenancy support** with tenant-specific API keys
‚úÖ **Resilience patterns**: Retry, Circuit Breaker, Timeout
‚úÖ **Streaming support** for all providers
‚úÖ **Metrics & tracing** via Micrometer + OpenTelemetry
‚úÖ **Modular design** for portable agent reuse
‚úÖ **Production-ready** configuration and error handling

All providers follow the same architectural patterns while accommodating provider-specific differences (authentication, endpoints, request/response formats).

# üß† Cerebras Provider Implementation

Excellent addition! Cerebras offers ultra-fast inference (especially for Llama models). I'll provide a complete implementation following the same pattern.

---

## üì¶ Module Structure

```
inference-provider-cerebras/
‚îú‚îÄ‚îÄ pom.xml
‚îî‚îÄ‚îÄ src/main/java/tech/kayys/wayang/inference/providers/cerebras/
    ‚îú‚îÄ‚îÄ CerebrasProvider.java
    ‚îú‚îÄ‚îÄ CerebrasClient.java
    ‚îú‚îÄ‚îÄ CerebrasConfig.java
    ‚îú‚îÄ‚îÄ model/
    ‚îÇ   ‚îú‚îÄ‚îÄ CerebrasRequest.java
    ‚îÇ   ‚îú‚îÄ‚îÄ CerebrasResponse.java
    ‚îÇ   ‚îú‚îÄ‚îÄ CerebrasStreamChunk.java
    ‚îÇ   ‚îú‚îÄ‚îÄ CerebrasChoice.java
    ‚îÇ   ‚îú‚îÄ‚îÄ CerebrasMessage.java
    ‚îÇ   ‚îî‚îÄ‚îÄ CerebrasUsage.java
    ‚îú‚îÄ‚îÄ mapper/
    ‚îÇ   ‚îú‚îÄ‚îÄ RequestMapper.java
    ‚îÇ   ‚îî‚îÄ‚îÄ ResponseMapper.java
    ‚îî‚îÄ‚îÄ exception/
        ‚îú‚îÄ‚îÄ CerebrasException.java
        ‚îî‚îÄ‚îÄ CerebrasClientExceptionMapper.java
```

---

## 1Ô∏è‚É£ POM Configuration

### pom.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>tech.kayys.wayang</groupId>
        <artifactId>wayang-inference-server</artifactId>
        <version>1.0.0-SNAPSHOT</version>
    </parent>

    <artifactId>inference-provider-cerebras</artifactId>
    <name>Wayang Inference :: Provider :: Cerebras</name>
    <description>Ultra-fast Cerebras inference provider for Llama models</description>

    <dependencies>
        <!-- Core Dependencies -->
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-api</artifactId>
        </dependency>
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-kernel</artifactId>
        </dependency>
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-providers-spi</artifactId>
        </dependency>

        <!-- Quarkus Extensions -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-rest-client-reactive-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-smallrye-fault-tolerance</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-micrometer-registry-prometheus</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-opentelemetry</artifactId>
        </dependency>

        <!-- Reactive Streams -->
        <dependency>
            <groupId>io.smallrye.reactive</groupId>
            <artifactId>mutiny</artifactId>
        </dependency>

        <!-- Test Dependencies -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-junit5</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-test-wiremock</artifactId>
            <scope>test</scope>
        </dependency>
    </dependencies>
</project>
```

---

## 2Ô∏è‚É£ Configuration

### CerebrasConfig.java

```java
package tech.kayys.wayang.inference.providers.cerebras;

import io.smallrye.config.ConfigMapping;
import io.smallrye.config.WithDefault;
import io.smallrye.config.WithName;

import java.time.Duration;
import java.util.Optional;

/**
 * Cerebras provider configuration.
 * Cerebras specializes in ultra-fast inference for Llama models.
 */
@ConfigMapping(prefix = "wayang.inference.provider.cerebras")
public interface CerebrasConfig {

    /**
     * Base API URL
     * Default: https://api.cerebras.ai/v1
     */
    @WithName("base-url")
    @WithDefault("https://api.cerebras.ai/v1")
    String baseUrl();

    /**
     * API key (can be overridden per tenant)
     */
    @WithName("api-key")
    Optional<String> apiKey();

    /**
     * Request timeout
     * Cerebras is known for fast inference, so shorter timeout is acceptable
     */
    @WithName("timeout")
    @WithDefault("20s")
    Duration timeout();

    /**
     * Max retry attempts
     */
    @WithName("max-retries")
    @WithDefault("3")
    int maxRetries();

    /**
     * Retry backoff multiplier
     */
    @WithName("retry-backoff-multiplier")
    @WithDefault("2.0")
    double retryBackoffMultiplier();

    /**
     * Initial retry delay
     */
    @WithName("retry-initial-delay")
    @WithDefault("300ms")
    Duration retryInitialDelay();

    /**
     * Max retry delay
     */
    @WithName("retry-max-delay")
    @WithDefault("5s")
    Duration retryMaxDelay();

    /**
     * Circuit breaker failure threshold
     */
    @WithName("circuit-breaker.failure-threshold")
    @WithDefault("5")
    int circuitBreakerFailureThreshold();

    /**
     * Circuit breaker delay
     */
    @WithName("circuit-breaker.delay")
    @WithDefault("5s")
    Duration circuitBreakerDelay();

    /**
     * Enable request/response logging
     */
    @WithName("logging.enabled")
    @WithDefault("true")
    boolean loggingEnabled();

    /**
     * Log request/response bodies
     */
    @WithName("logging.log-bodies")
    @WithDefault("false")
    boolean logBodies();

    /**
     * Enable metrics
     */
    @WithName("metrics.enabled")
    @WithDefault("true")
    boolean metricsEnabled();

    /**
     * Default model
     */
    @WithName("default-model")
    @WithDefault("llama-3.3-70b")
    String defaultModel();

    /**
     * Max tokens limit
     * Cerebras supports up to 128K context
     */
    @WithName("max-tokens-limit")
    @WithDefault("131072")
    int maxTokensLimit();

    /**
     * Enable streaming by default
     */
    @WithName("streaming.enabled")
    @WithDefault("true")
    boolean streamingEnabled();

    /**
     * Streaming buffer size
     */
    @WithName("streaming.buffer-size")
    @WithDefault("8192")
    int streamingBufferSize();

    /**
     * Enable latency tracking
     * Cerebras is known for fast inference - useful to track
     */
    @WithName("metrics.track-latency-percentiles")
    @WithDefault("true")
    boolean trackLatencyPercentiles();

    /**
     * Track tokens per second metric
     */
    @WithName("metrics.track-tokens-per-second")
    @WithDefault("true")
    boolean trackTokensPerSecond();
}
```

---

## 3Ô∏è‚É£ Model Classes

### CerebrasRequest.java

```java
package tech.kayys.wayang.inference.providers.cerebras.model;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;

import java.util.ArrayList;
import java.util.List;

/**
 * Cerebras API request.
 * Compatible with OpenAI-like chat completions API.
 */
@JsonInclude(JsonInclude.Include.NON_NULL)
public class CerebrasRequest {

    @JsonProperty("model")
    private String model;

    @JsonProperty("messages")
    private List<CerebrasMessage> messages = new ArrayList<>();

    @JsonProperty("temperature")
    private Double temperature;

    @JsonProperty("max_tokens")
    private Integer maxTokens;

    @JsonProperty("top_p")
    private Double topP;

    @JsonProperty("stream")
    private Boolean stream;

    @JsonProperty("seed")
    private Integer seed;

    @JsonProperty("stop")
    private List<String> stop;

    // Constructors
    public CerebrasRequest() {}

    private CerebrasRequest(Builder builder) {
        this.model = builder.model;
        this.messages = builder.messages;
        this.temperature = builder.temperature;
        this.maxTokens = builder.maxTokens;
        this.topP = builder.topP;
        this.stream = builder.stream;
        this.seed = builder.seed;
        this.stop = builder.stop;
    }

    // Getters and Setters
    public String getModel() { return model; }
    public void setModel(String model) { this.model = model; }

    public List<CerebrasMessage> getMessages() { return messages; }
    public void setMessages(List<CerebrasMessage> messages) { this.messages = messages; }

    public Double getTemperature() { return temperature; }
    public void setTemperature(Double temperature) { this.temperature = temperature; }

    public Integer getMaxTokens() { return maxTokens; }
    public void setMaxTokens(Integer maxTokens) { this.maxTokens = maxTokens; }

    public Double getTopP() { return topP; }
    public void setTopP(Double topP) { this.topP = topP; }

    public Boolean getStream() { return stream; }
    public void setStream(Boolean stream) { this.stream = stream; }

    public Integer getSeed() { return seed; }
    public void setSeed(Integer seed) { this.seed = seed; }

    public List<String> getStop() { return stop; }
    public void setStop(List<String> stop) { this.stop = stop; }

    // Fluent setter for streaming
    public CerebrasRequest withStream(boolean stream) {
        this.stream = stream;
        return this;
    }

    // Builder
    public static Builder builder() {
        return new Builder();
    }

    public static class Builder {
        private String model;
        private List<CerebrasMessage> messages = new ArrayList<>();
        private Double temperature;
        private Integer maxTokens;
        private Double topP;
        private Boolean stream;
        private Integer seed;
        private List<String> stop;

        public Builder model(String model) {
            this.model = model;
            return this;
        }

        public Builder messages(List<CerebrasMessage> messages) {
            this.messages = messages;
            return this;
        }

        public Builder message(CerebrasMessage message) {
            this.messages.add(message);
            return this;
        }

        public Builder temperature(Double temperature) {
            this.temperature = temperature;
            return this;
        }

        public Builder maxTokens(Integer maxTokens) {
            this.maxTokens = maxTokens;
            return this;
        }

        public Builder topP(Double topP) {
            this.topP = topP;
            return this;
        }

        public Builder stream(Boolean stream) {
            this.stream = stream;
            return this;
        }

        public Builder seed(Integer seed) {
            this.seed = seed;
            return this;
        }

        public Builder stop(List<String> stop) {
            this.stop = stop;
            return this;
        }

        public CerebrasRequest build() {
            return new CerebrasRequest(this);
        }
    }
}
```

### CerebrasMessage.java

```java
package tech.kayys.wayang.inference.providers.cerebras.model;

import com.fasterxml.jackson.annotation.JsonInclude;
import com.fasterxml.jackson.annotation.JsonProperty;

/**
 * Message in Cerebras chat completion request.
 */
@JsonInclude(JsonInclude.Include.NON_NULL)
public class CerebrasMessage {

    @JsonProperty("role")
    private String role;

    @JsonProperty("content")
    private String content;

    public CerebrasMessage() {}

    public CerebrasMessage(String role, String content) {
        this.role = role;
        this.content = content;
    }

    public String getRole() { return role; }
    public void setRole(String role) { this.role = role; }

    public String getContent() { return content; }
    public void setContent(String content) { this.content = content; }

    public static Builder builder() {
        return new Builder();
    }

    public static class Builder {
        private String role;
        private String content;

        public Builder role(String role) {
            this.role = role;
            return this;
        }

        public Builder content(String content) {
            this.content = content;
            return this;
        }

        public CerebrasMessage build() {
            return new CerebrasMessage(role, content);
        }
    }
}
```

### CerebrasResponse.java

```java
package tech.kayys.wayang.inference.providers.cerebras.model;

import com.fasterxml.jackson.annotation.JsonProperty;

import java.util.ArrayList;
import java.util.List;

/**
 * Cerebras API response for chat completions.
 */
public class CerebrasResponse {

    @JsonProperty("id")
    private String id;

    @JsonProperty("object")
    private String object;

    @JsonProperty("created")
    private Long created;

    @JsonProperty("model")
    private String model;

    @JsonProperty("choices")
    private List<CerebrasChoice> choices = new ArrayList<>();

    @JsonProperty("usage")
    private CerebrasUsage usage;

    @JsonProperty("system_fingerprint")
    private String systemFingerprint;

    // Getters and Setters
    public String getId() { return id; }
    public void setId(String id) { this.id = id; }

    public String getObject() { return object; }
    public void setObject(String object) { this.object = object; }

    public Long getCreated() { return created; }
    public void setCreated(Long created) { this.created = created; }

    public String getModel() { return model; }
    public void setModel(String model) { this.model = model; }

    public List<CerebrasChoice> getChoices() { return choices; }
    public void setChoices(List<CerebrasChoice> choices) { this.choices = choices; }

    public CerebrasUsage getUsage() { return usage; }
    public void setUsage(CerebrasUsage usage) { this.usage = usage; }

    public String getSystemFingerprint() { return systemFingerprint; }
    public void setSystemFingerprint(String systemFingerprint) { 
        this.systemFingerprint = systemFingerprint; 
    }
}
```

### CerebrasChoice.java

```java
package tech.kayys.wayang.inference.providers.cerebras.model;

import com.fasterxml.jackson.annotation.JsonProperty;

public class CerebrasChoice {

    @JsonProperty("index")
    private Integer index;

    @JsonProperty("message")
    private CerebrasMessage message;

    @JsonProperty("finish_reason")
    private String finishReason;

    @JsonProperty("logprobs")
    private Object logprobs;

    public Integer getIndex() { return index; }
    public void setIndex(Integer index) { this.index = index; }

    public CerebrasMessage getMessage() { return message; }
    public void setMessage(CerebrasMessage message) { this.message = message; }

    public String getFinishReason() { return finishReason; }
    public void setFinishReason(String finishReason) { this.finishReason = finishReason; }

    public Object getLogprobs() { return logprobs; }
    public void setLogprobs(Object logprobs) { this.logprobs = logprobs; }
}
```

### CerebrasUsage.java

```java
package tech.kayys.wayang.inference.providers.cerebras.model;

import com.fasterxml.jackson.annotation.JsonProperty;

public class CerebrasUsage {

    @JsonProperty("prompt_tokens")
    private Integer promptTokens;

    @JsonProperty("completion_tokens")
    private Integer completionTokens;

    @JsonProperty("total_tokens")
    private Integer totalTokens;

    public Integer getPromptTokens() { return promptTokens; }
    public void setPromptTokens(Integer promptTokens) { this.promptTokens = promptTokens; }

    public Integer getCompletionTokens() { return completionTokens; }
    public void setCompletionTokens(Integer completionTokens) { 
        this.completionTokens = completionTokens; 
    }

    public Integer getTotalTokens() { return totalTokens; }
    public void setTotalTokens(Integer totalTokens) { this.totalTokens = totalTokens; }
}
```

### CerebrasStreamChunk.java

```java
package tech.kayys.wayang.inference.providers.cerebras.model;

import com.fasterxml.jackson.annotation.JsonProperty;

import java.util.ArrayList;
import java.util.List;

/**
 * Streaming chunk from Cerebras API.
 */
public class CerebrasStreamChunk {

    @JsonProperty("id")
    private String id;

    @JsonProperty("object")
    private String object;

    @JsonProperty("created")
    private Long created;

    @JsonProperty("model")
    private String model;

    @JsonProperty("choices")
    private List<CerebrasStreamChoice> choices = new ArrayList<>();

    public String getId() { return id; }
    public void setId(String id) { this.id = id; }

    public String getObject() { return object; }
    public void setObject(String object) { this.object = object; }

    public Long getCreated() { return created; }
    public void setCreated(Long created) { this.created = created; }

    public String getModel() { return model; }
    public void setModel(String model) { this.model = model; }

    public List<CerebrasStreamChoice> getChoices() { return choices; }
    public void setChoices(List<CerebrasStreamChoice> choices) { this.choices = choices; }

    public static class CerebrasStreamChoice {
        
        @JsonProperty("index")
        private Integer index;

        @JsonProperty("delta")
        private CerebrasMessageDelta delta;

        @JsonProperty("finish_reason")
        private String finishReason;

        public Integer getIndex() { return index; }
        public void setIndex(Integer index) { this.index = index; }

        public CerebrasMessageDelta getDelta() { return delta; }
        public void setDelta(CerebrasMessageDelta delta) { this.delta = delta; }

        public String getFinishReason() { return finishReason; }
        public void setFinishReason(String finishReason) { 
            this.finishReason = finishReason; 
        }
    }

    public static class CerebrasMessageDelta {
        
        @JsonProperty("role")
        private String role;

        @JsonProperty("content")
        private String content;

        public String getRole() { return role; }
        public void setRole(String role) { this.role = role; }

        public String getContent() { return content; }
        public void setContent(String content) { this.content = content; }
    }
}
```

---

## 4Ô∏è‚É£ REST Client

### CerebrasClient.java

```java
package tech.kayys.wayang.inference.providers.cerebras;

import io.smallrye.mutiny.Multi;
import io.smallrye.mutiny.Uni;
import jakarta.ws.rs.*;
import jakarta.ws.rs.core.MediaType;
import org.eclipse.microprofile.rest.client.annotation.RegisterProvider;
import org.eclipse.microprofile.rest.client.inject.RegisterRestClient;
import tech.kayys.wayang.inference.providers.cerebras.exception.CerebrasClientExceptionMapper;
import tech.kayys.wayang.inference.providers.cerebras.model.CerebrasRequest;
import tech.kayys.wayang.inference.providers.cerebras.model.CerebrasResponse;
import tech.kayys.wayang.inference.providers.cerebras.model.CerebrasStreamChunk;

/**
 * REST client for Cerebras API.
 * Uses OpenAI-compatible endpoints.
 */
@RegisterRestClient(configKey = "cerebras")
@RegisterProvider(CerebrasClientExceptionMapper.class)
@Path("/chat/completions")
public interface CerebrasClient {

    /**
     * Create chat completion (synchronous).
     * 
     * @param request Chat completion request
     * @param authorization Bearer token (API key)
     * @return Uni containing the response
     */
    @POST
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON)
    Uni<CerebrasResponse> createChatCompletion(
        CerebrasRequest request,
        @HeaderParam("Authorization") String authorization
    );

    /**
     * Create chat completion with default headers.
     */
    default Uni<CerebrasResponse> createChatCompletion(
        CerebrasRequest request,
        String apiKey
    ) {
        return createChatCompletion(request, "Bearer " + apiKey);
    }

    /**
     * Stream chat completion.
     * 
     * @param request Chat completion request (with stream=true)
     * @param authorization Bearer token (API key)
     * @return Multi stream of chunks
     */
    @POST
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.SERVER_SENT_EVENTS)
    Multi<CerebrasStreamChunk> streamChatCompletion(
        CerebrasRequest request,
        @HeaderParam("Authorization") String authorization
    );

    /**
     * Stream with default headers.
     */
    default Multi<CerebrasStreamChunk> streamChatCompletion(
        CerebrasRequest request,
        String apiKey
    ) {
        return streamChatCompletion(request, "Bearer " + apiKey);
    }
}
```

---

## 5Ô∏è‚É£ Main Provider Implementation

### CerebrasProvider.java

```java
package tech.kayys.wayang.inference.providers.cerebras;

import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import io.opentelemetry.api.trace.Span;
import io.opentelemetry.api.trace.Tracer;
import io.smallrye.mutiny.Multi;
import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import org.eclipse.microprofile.faulttolerance.CircuitBreaker;
import org.eclipse.microprofile.faulttolerance.Retry;
import org.eclipse.microprofile.faulttolerance.Timeout;
import org.jboss.logging.Logger;
import tech.kayys.wayang.inference.api.*;
import tech.kayys.wayang.inference.provider.ProviderCapabilities;
import tech.kayys.wayang.inference.provider.ProviderRequest;
import tech.kayys.wayang.inference.provider.StreamingLLMProvider;
import tech.kayys.wayang.inference.providers.cerebras.exception.CerebrasException;
import tech.kayys.wayang.inference.providers.cerebras.mapper.RequestMapper;
import tech.kayys.wayang.inference.providers.cerebras.mapper.ResponseMapper;
import tech.kayys.wayang.inference.providers.cerebras.model.CerebrasRequest;
import tech.kayys.wayang.inference.providers.cerebras.model.CerebrasResponse;
import tech.kayys.wayang.inference.providers.cerebras.model.CerebrasStreamChunk;

import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

/**
 * Cerebras Provider implementation.
 * 
 * Cerebras specializes in ultra-fast inference for Llama models
 * using their proprietary Wafer Scale Engine (WSE) technology.
 * 
 * Key features:
 * - Extremely low latency (sub-second for most queries)
 * - High throughput
 * - OpenAI-compatible API
 * - Supports Llama 3.1, 3.3 models
 * - Streaming support
 */
@ApplicationScoped
public class CerebrasProvider implements StreamingLLMProvider {

    private static final Logger LOG = Logger.getLogger(CerebrasProvider.class);
    private static final String PROVIDER_ID = "cerebras";

    @Inject
    CerebrasClient client;

    @Inject
    CerebrasConfig config;

    @Inject
    RequestMapper requestMapper;

    @Inject
    ResponseMapper responseMapper;

    @Inject
    MeterRegistry meterRegistry;

    @Inject
    Tracer tracer;

    private final ProviderCapabilities capabilities = ProviderCapabilities.builder()
        .streaming(true)
        .functionCalling(false) // Cerebras doesn't support function calling yet
        .multimodal(false) // Text-only for now
        .maxContextTokens(131072) // 128K context window
        .supportedModels(
            "llama-3.3-70b",
            "llama-3.1-70b",
            "llama-3.1-8b"
        )
        .metadata("provider_type", "ultra_fast")
        .metadata("hardware", "wafer_scale_engine")
        .build();

    @Override
    public String id() {
        return PROVIDER_ID;
    }

    @Override
    public ProviderCapabilities capabilities() {
        return capabilities;
    }

    @Override
    @Retry(
        maxRetries = 3,
        delay = 300,
        maxDuration = 20000,
        retryOn = CerebrasException.class
    )
    @CircuitBreaker(
        requestVolumeThreshold = 10,
        failureRatio = 0.5,
        delay = 5000
    )
    @Timeout(20000) // Cerebras is fast, so shorter timeout
    public InferenceResponse infer(ProviderRequest request) {
        Span span = tracer.spanBuilder("cerebras.infer").startSpan();
        Timer.Sample sample = Timer.start(meterRegistry);
        long startTime = System.nanoTime();

        try {
            span.setAttribute("model", request.getModel());
            span.setAttribute("streaming", request.isStreaming());
            span.setAttribute("tenant", request.getTenantContext().getTenantId());
            span.setAttribute("provider", PROVIDER_ID);

            LOG.debugf("Cerebras inference request: model=%s, messages=%d",
                request.getModel(), request.getMessages().size());

            // Map to Cerebras format
            CerebrasRequest cerebrasRequest = requestMapper.map(request);

            // Execute synchronous call
            CerebrasResponse cerebrasResponse = client.createChatCompletion(
                cerebrasRequest,
                resolveApiKey(request.getTenantContext())
            ).await().indefinitely();

            // Calculate actual latency
            long durationMs = (System.nanoTime() - startTime) / 1_000_000;

            // Map response
            InferenceResponse response = responseMapper.map(
                request.getRequestId(),
                cerebrasResponse,
                durationMs
            );

            // Record metrics (including tokens/sec for Cerebras)
            recordSuccess(request, response, sample, durationMs);

            // Audit log
            auditInference(request, response, null, durationMs);

            LOG.infof("Cerebras inference completed in %dms, tokens: %d, tokens/sec: %.2f",
                durationMs, 
                response.getTokensUsed(),
                calculateTokensPerSecond(response.getTokensUsed(), durationMs));

            return response;

        } catch (Exception e) {
            long durationMs = (System.nanoTime() - startTime) / 1_000_000;
            recordFailure(request, e, sample);
            auditInference(request, null, e, durationMs);
            throw handleError(e, request);
        } finally {
            span.end();
        }
    }

    @Override
    public Multi<StreamChunk> stream(ProviderRequest request) {
        Span span = tracer.spanBuilder("cerebras.stream").startSpan();
        Timer.Sample sample = Timer.start(meterRegistry);

        try {
            span.setAttribute("model", request.getModel());
            span.setAttribute("streaming", true);
            span.setAttribute("tenant", request.getTenantContext().getTenantId());
            span.setAttribute("provider", PROVIDER_ID);

            LOG.debugf("Cerebras streaming request: model=%s, messages=%d",
                request.getModel(), request.getMessages().size());

            // Map to Cerebras format with streaming enabled
            CerebrasRequest cerebrasRequest = requestMapper.map(request)
                .withStream(true);

            // Counters for streaming metrics
            AtomicInteger tokenCount = new AtomicInteger(0);
            AtomicLong startTime = new AtomicLong(System.currentTimeMillis());
            AtomicLong firstChunkTime = new AtomicLong(0);

            // Stream chunks
            return client.streamChatCompletion(
                    cerebrasRequest,
                    resolveApiKey(request.getTenantContext())
                )
                .onItem().invoke(chunk -> {
                    // Record time to first chunk (TTFT) - important for Cerebras
                    if (firstChunkTime.get() == 0) {
                        long ttft = System.currentTimeMillis() - startTime.get();
                        firstChunkTime.set(ttft);
                        recordTimeToFirstToken(request, ttft);
                    }
                })
                .map(chunk -> mapStreamChunk(request, chunk, tokenCount))
                .onTermination().invoke((failure, cancellation, subscription) -> {
                    long duration = System.currentTimeMillis() - startTime.get();

                    if (failure != null) {
                        recordFailure(request, (Exception) failure, sample);
                        auditStream(request, tokenCount.get(), duration, 
                            firstChunkTime.get(), failure);
                    } else {
                        recordStreamSuccess(request, tokenCount.get(), duration, 
                            firstChunkTime.get(), sample);
                        auditStream(request, tokenCount.get(), duration, 
                            firstChunkTime.get(), null);
                    }

                    span.end();
                });

        } catch (Exception e) {
            recordFailure(request, e, sample);
            span.recordException(e);
            span.end();
            throw handleError(e, request);
        }
    }

    /**
     * Map Cerebras stream chunk to generic StreamChunk
     */
    private StreamChunk mapStreamChunk(
        ProviderRequest request,
        CerebrasStreamChunk chunk,
        AtomicInteger tokenCount
    ) {
        String delta = chunk.getChoices().isEmpty()
            ? ""
            : chunk.getChoices().get(0).getDelta().getContent();

        // Token estimation
        if (delta != null && !delta.isEmpty()) {
            tokenCount.addAndGet(estimateTokens(delta));
        }

        boolean isFinal = chunk.getChoices().isEmpty()
            || "stop".equals(chunk.getChoices().get(0).getFinishReason());

        return StreamChunk.builder()
            .requestId(request.getRequestId())
            .delta(delta != null ? delta : "")
            .isFinal(isFinal)
            .metadata("chunk_id", chunk.getId())
            .metadata("model", chunk.getModel())
            .metadata("provider", PROVIDER_ID)
            .metadata("finish_reason", 
                chunk.getChoices().isEmpty() ? null : 
                chunk.getChoices().get(0).getFinishReason())
            .build();
    }

    /**
     * Resolve API key from tenant context or config
     */
    private String resolveApiKey(TenantContext tenantContext) {
        return tenantContext.getAttribute("cerebras.api.key")
            .or(() -> config.apiKey())
            .orElseThrow(() -> new CerebrasException(
                "Cerebras API key not configured for tenant: " + 
                tenantContext.getTenantId()
            ));
    }

    /**
     * Handle and map errors
     */
    private RuntimeException handleError(Exception e, ProviderRequest request) {
        if (e instanceof CerebrasException) {
            return (CerebrasException) e;
        }

        LOG.errorf(e, "Cerebras provider error for request %s", 
            request.getRequestId());

        return new CerebrasException(
            "Cerebras inference failed: " + e.getMessage(),
            e,
            ErrorPayload.builder()
                .type("ProviderError")
                .message(e.getMessage())
                .originNode("cerebras-provider")
                .originRunId(request.getRequestId())
                .retryable(isRetryable(e))
                .detail("provider", PROVIDER_ID)
                .detail("model", request.getModel())
                .detail("error_class", e.getClass().getName())
                .build()
        );
    }

    /**
     * Determine if error is retryable
     */
    private boolean isRetryable(Exception e) {
        String message = e.getMessage();
        if (message == null) return false;

        return message.contains("timeout") ||
               message.contains("503") ||
               message.contains("502") ||
               message.contains("429") || // Rate limit
               message.contains("overloaded");
    }

    /**
     * Estimate tokens from text (simple whitespace split)
     */
    private int estimateTokens(String text) {
        if (text == null || text.isEmpty()) return 0;
        return text.split("\\s+").length;
    }

    /**
     * Calculate tokens per second
     */
    private double calculateTokensPerSecond(int tokens, long durationMs) {
        if (durationMs == 0) return 0;
        return (tokens * 1000.0) / durationMs;
    }

    /**
     * Record time to first token (important for Cerebras)
     */
    private void recordTimeToFirstToken(ProviderRequest request, long ttftMs) {
        if (config.trackLatencyPercentiles()) {
            meterRegistry.timer("cerebras.time_to_first_token",
                "model", request.getModel(),
                "tenant", request.getTenantContext().getTenantId()
            ).record(java.time.Duration.ofMillis(ttftMs));
        }

        LOG.debugf("Cerebras TTFT: %dms for model %s", ttftMs, request.getModel());
    }

    /**
     * Record successful inference metrics
     */
    private void recordSuccess(
        ProviderRequest request,
        InferenceResponse response,
        Timer.Sample sample,
        long actualDurationMs
    ) {
        sample.stop(meterRegistry.timer("cerebras.inference.duration",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "success"
        ));

        meterRegistry.counter("cerebras.inference.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "success"
        ).increment();

        meterRegistry.counter("cerebras.tokens.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "type", "completion"
        ).increment(response.getTokensUsed());

        // Track tokens/sec (Cerebras specialty)
        if (config.trackTokensPerSecond()) {
            double tokensPerSec = calculateTokensPerSecond(
                response.getTokensUsed(), 
                actualDurationMs
            );
            
            meterRegistry.gauge("cerebras.tokens_per_second",
                "model", request.getModel(),
                "tenant", request.getTenantContext().getTenantId(),
                tokensPerSec
            );
        }
    }

    /**
     * Record streaming success metrics
     */
    private void recordStreamSuccess(
        ProviderRequest request,
        int tokens,
        long durationMs,
        long ttftMs,
        Timer.Sample sample
    ) {
        sample.stop(meterRegistry.timer("cerebras.stream.duration",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "success"
        ));

        meterRegistry.counter("cerebras.stream.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "success"
        ).increment();

        meterRegistry.counter("cerebras.tokens.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "type", "stream"
        ).increment(tokens);

        // Tokens per second for streaming
        if (config.trackTokensPerSecond() && durationMs > 0) {
            double tokensPerSec = calculateTokensPerSecond(tokens, durationMs);
            
            meterRegistry.gauge("cerebras.stream.tokens_per_second",
                "model", request.getModel(),
                "tenant", request.getTenantContext().getTenantId(),
                tokensPerSec
            );
        }
    }

    /**
     * Record failure metrics
     */
    private void recordFailure(
        ProviderRequest request,
        Exception error,
        Timer.Sample sample
    ) {
        sample.stop(meterRegistry.timer("cerebras.inference.duration",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "error"
        ));

        meterRegistry.counter("cerebras.inference.total",
            "model", request.getModel(),
            "tenant", request.getTenantContext().getTenantId(),
            "status", "error",
            "error_type", error.getClass().getSimpleName()
        ).increment();
    }

    /**
     * Audit inference execution
     */
    private void auditInference(
        ProviderRequest request,
        InferenceResponse response,
        Exception error,
        long durationMs
    ) {
        AuditPayload.Builder audit = AuditPayload.builder()
            .runId(request.getRequestId())
            .event(error == null ? "INFERENCE_SUCCESS" : "INFERENCE_FAILED")
            .level(error == null ? "INFO" : "ERROR")
            .actor(AuditPayload.Actor.system("cerebras-provider"))
            .tag("provider:cerebras")
            .tag("ultra_fast_inference")
            .tag("model:" + request.getModel())
            .metadata("tenant", request.getTenantContext().getTenantId())
            .metadata("model", request.getModel())
            .metadata("streaming", request.isStreaming())
            .metadata("message_count", request.getMessages().size())
            .metadata("duration_ms", durationMs);

        if (response != null) {
            audit.metadata("tokens_used", response.getTokensUsed())
                .metadata("tokens_per_second", 
                    calculateTokensPerSecond(response.getTokensUsed(), durationMs));
        }

        if (error != null) {
            audit.metadata("error", error.getMessage())
                .metadata("error_type", error.getClass().getSimpleName());
        }

        LOG.infof("Audit: %s", audit.build());
    }

    /**
     * Audit streaming execution
     */
    private void auditStream(
        ProviderRequest request,
        int tokens,
        long durationMs,
        long ttftMs,
        Throwable error
    ) {
        AuditPayload.Builder audit = AuditPayload.builder()
            .runId(request.getRequestId())
            .event(error == null ? "STREAM_COMPLETED" : "STREAM_FAILED")
            .level(error == null ? "INFO" : "ERROR")
            .actor(AuditPayload.Actor.system("cerebras-provider"))
            .tag("provider:cerebras")
            .tag("ultra_fast_inference")
            .tag("streaming")
            .tag("model:" + request.getModel())
            .metadata("tenant", request.getTenantContext().getTenantId())
            .metadata("model", request.getModel())
            .metadata("tokens_streamed", tokens)
            .metadata("duration_ms", durationMs)
            .metadata("time_to_first_token_ms", ttftMs)
            .metadata("tokens_per_second", 
                calculateTokensPerSecond(tokens, durationMs));

        if (error != null) {
            audit.metadata("error", error.getMessage())
                .metadata("error_type", error.getClass().getSimpleName());
        }

        LOG.infof("Audit: %s", audit.build());
    }
}
```

---

## 6Ô∏è‚É£ Mappers & Exceptions

### RequestMapper.java & ResponseMapper.java
*(Same pattern as OpenAI implementation)*

### CerebrasException.java

```java
package tech.kayys.wayang.inference.providers.cerebras.exception;

import tech.kayys.wayang.inference.api.ErrorPayload;

public class CerebrasException extends RuntimeException {

    private final ErrorPayload errorPayload;

    public CerebrasException(String message) {
        super(message);
        this.errorPayload = null;
    }

    public CerebrasException(String message, Throwable cause, ErrorPayload errorPayload) {
        super(message, cause);
        this.errorPayload = errorPayload;
    }

    public ErrorPayload getErrorPayload() {
        return errorPayload;
    }
}
```

---

## 7Ô∏è‚É£ Configuration

### application.yml

```yaml
wayang:
  inference:
    provider:
      cerebras:
        base-url: https://api.cerebras.ai/v1
        timeout: 20s  # Faster timeout due to ultra-fast inference
        max-retries: 3
        retry-backoff-multiplier: 2.0
        retry-initial-delay: 300ms
        circuit-breaker:
          failure-threshold: 5
          delay: 5s
        logging:
          enabled: true
          log-bodies: false
        metrics:
          enabled: true
          track-latency-percentiles: true
          track-tokens-per-second: true
        default-model: llama-3.3-70b
        max-tokens-limit: 131072
        streaming:
          enabled: true
          buffer-size: 8192

quarkus:
  rest-client:
    cerebras:
      url: ${wayang.inference.provider.cerebras.base-url}
      scope: jakarta.enterprise.context.ApplicationScoped
      connect-timeout: 5000
      read-timeout: 20000
```

---

## üìä Key Features

### Ultra-Fast Performance Tracking
- **Time to First Token (TTFT)** metrics
- **Tokens per second** gauge
- Sub-second inference monitoring

### Complete Error Handling
- Retry with exponential backoff
- Circuit breaker protection
- Standardized error payloads

### Comprehensive Audit
- Execution time tracking
- Token usage monitoring
- Performance metrics (tokens/sec)

### Multi-Tenancy
- Tenant-specific API keys
- Resource isolation
- Per-tenant metrics

This implementation follows the exact same patterns as OpenAI/Anthropic/Gemini while leveraging Cerebras' ultra-fast inference capabilities! üöÄ


# ü¶ô Ollama Provider + Refactored Modular Architecture

You're absolutely right! Let me create a **future-proof, modular architecture** that eliminates duplication and makes adding new providers trivial.

---

## üèóÔ∏è Refactored Architecture

```
inference-providers/
‚îú‚îÄ‚îÄ inference-providers-spi/              # Core abstractions
‚îú‚îÄ‚îÄ inference-providers-common/           # Shared implementations ‚≠ê NEW
‚îú‚îÄ‚îÄ inference-provider-openai/
‚îú‚îÄ‚îÄ inference-provider-anthropic/
‚îú‚îÄ‚îÄ inference-provider-gemini/
‚îú‚îÄ‚îÄ inference-provider-cerebras/
‚îú‚îÄ‚îÄ inference-provider-ollama/            # ‚≠ê NEW
‚îî‚îÄ‚îÄ inference-provider-openai-compatible/ # ‚≠ê NEW (for OpenRouter, Together, etc.)
```

---

## 1Ô∏è‚É£ Common Provider Base (Shared Module)

### pom.xml (inference-providers-common)

```xml
<?xml version="1.0" encoding="UTF-8"?>
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 
         http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>tech.kayys.wayang</groupId>
        <artifactId>wayang-inference-server</artifactId>
        <version>1.0.0-SNAPSHOT</version>
    </parent>

    <artifactId>inference-providers-common</artifactId>
    <name>Wayang Inference :: Providers :: Common</name>
    <description>Shared base classes, utilities, and patterns for all providers</description>

    <dependencies>
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-api</artifactId>
        </dependency>
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-kernel</artifactId>
        </dependency>
        <dependency>
            <groupId>tech.kayys.wayang</groupId>
            <artifactId>inference-providers-spi</artifactId>
        </dependency>

        <!-- Quarkus -->
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-rest-client-reactive-jackson</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-smallrye-fault-tolerance</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-micrometer-registry-prometheus</artifactId>
        </dependency>
        <dependency>
            <groupId>io.quarkus</groupId>
            <artifactId>quarkus-opentelemetry</artifactId>
        </dependency>

        <!-- Reactive -->
        <dependency>
            <groupId>io.smallrye.reactive</groupId>
            <artifactId>mutiny</artifactId>
        </dependency>

        <!-- Utilities -->
        <dependency>
            <groupId>org.apache.commons</groupId>
            <artifactId>commons-lang3</artifactId>
        </dependency>
    </dependencies>
</project>
```

---

## 2Ô∏è‚É£ Abstract Base Provider

### AbstractLLMProvider.java

```java
package tech.kayys.wayang.inference.providers.common;

import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Timer;
import io.opentelemetry.api.trace.Span;
import io.opentelemetry.api.trace.Tracer;
import io.smallrye.mutiny.Multi;
import io.smallrye.mutiny.Uni;
import jakarta.inject.Inject;
import org.eclipse.microprofile.faulttolerance.CircuitBreaker;
import org.eclipse.microprofile.faulttolerance.Retry;
import org.eclipse.microprofile.faulttolerance.Timeout;
import org.jboss.logging.Logger;
import tech.kayys.wayang.inference.api.*;
import tech.kayys.wayang.inference.provider.ProviderCapabilities;
import tech.kayys.wayang.inference.provider.ProviderRequest;
import tech.kayys.wayang.inference.provider.StreamingLLMProvider;
import tech.kayys.wayang.inference.providers.common.config.ProviderConfig;
import tech.kayys.wayang.inference.providers.common.metrics.ProviderMetrics;
import tech.kayys.wayang.inference.providers.common.audit.ProviderAuditor;

import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;

/**
 * Abstract base class for all LLM providers.
 * Implements common patterns for:
 * - Metrics collection
 * - Tracing
 * - Audit logging
 * - Error handling
 * - Retry/circuit breaker
 * - Multi-tenancy
 * 
 * Subclasses only need to implement:
 * - createProviderRequest()
 * - executeInference()
 * - executeStreaming()
 * - mapResponse()
 * - mapStreamChunk()
 */
public abstract class AbstractLLMProvider implements StreamingLLMProvider {

    protected final Logger log;

    @Inject
    protected MeterRegistry meterRegistry;

    @Inject
    protected Tracer tracer;

    @Inject
    protected ProviderMetrics metrics;

    @Inject
    protected ProviderAuditor auditor;

    protected AbstractLLMProvider() {
        this.log = Logger.getLogger(getClass());
    }

    /**
     * Provider-specific configuration
     */
    protected abstract ProviderConfig getConfig();

    /**
     * Provider capabilities
     */
    @Override
    public abstract ProviderCapabilities capabilities();

    /**
     * Create provider-specific request object
     */
    protected abstract <T> T createProviderRequest(ProviderRequest request);

    /**
     * Execute synchronous inference (provider-specific)
     */
    protected abstract <REQ, RES> Uni<RES> executeInference(
        REQ providerRequest, 
        ProviderRequest originalRequest
    );

    /**
     * Execute streaming inference (provider-specific)
     */
    protected abstract <REQ, CHUNK> Multi<CHUNK> executeStreaming(
        REQ providerRequest, 
        ProviderRequest originalRequest
    );

    /**
     * Map provider response to standard InferenceResponse
     */
    protected abstract <RES> InferenceResponse mapResponse(
        RES providerResponse, 
        ProviderRequest originalRequest,
        long durationMs
    );

    /**
     * Map provider stream chunk to standard StreamChunk
     */
    protected abstract <CHUNK> StreamChunk mapStreamChunk(
        CHUNK providerChunk,
        ProviderRequest originalRequest,
        AtomicInteger tokenCounter
    );

    /**
     * Resolve API credentials for tenant
     */
    protected abstract String resolveCredentials(TenantContext tenantContext);

    /**
     * Determine if error is retryable
     */
    protected boolean isRetryable(Throwable error) {
        String message = error.getMessage();
        if (message == null) return false;

        return message.contains("timeout") ||
               message.contains("503") ||
               message.contains("502") ||
               message.contains("429") ||
               message.contains("rate limit") ||
               message.contains("overloaded");
    }

    /**
     * Create standardized error payload
     */
    protected ErrorPayload createErrorPayload(
        Throwable error,
        ProviderRequest request
    ) {
        return ErrorPayload.builder()
            .type("ProviderError")
            .message(error.getMessage())
            .originNode(id() + "-provider")
            .originRunId(request.getRequestId())
            .retryable(isRetryable(error))
            .detail("provider", id())
            .detail("model", request.getModel())
            .detail("error_class", error.getClass().getName())
            .build();
    }

    /**
     * Template method for synchronous inference
     */
    @Override
    @Retry(maxRetries = 3, delay = 500, maxDuration = 30000)
    @CircuitBreaker(requestVolumeThreshold = 10, failureRatio = 0.5, delay = 5000)
    @Timeout(30000)
    public InferenceResponse infer(ProviderRequest request) {
        Span span = tracer.spanBuilder(id() + ".infer").startSpan();
        Timer.Sample sample = Timer.start(meterRegistry);
        long startTime = System.nanoTime();

        try {
            // Add trace attributes
            enrichSpan(span, request, false);

            log.debugf("%s inference: model=%s, messages=%d",
                id(), request.getModel(), request.getMessages().size());

            // Create provider-specific request
            Object providerRequest = createProviderRequest(request);

            // Execute
            Object providerResponse = executeInference(providerRequest, request)
                .await()
                .indefinitely();

            // Calculate duration
            long durationMs = (System.nanoTime() - startTime) / 1_000_000;

            // Map response
            InferenceResponse response = mapResponse(
                providerResponse, 
                request, 
                durationMs
            );

            // Record metrics
            metrics.recordSuccess(id(), request, response, durationMs);

            // Audit
            auditor.auditInference(id(), request, response, null, durationMs);

            return response;

        } catch (Exception e) {
            long durationMs = (System.nanoTime() - startTime) / 1_000_000;
            
            metrics.recordFailure(id(), request, e);
            auditor.auditInference(id(), request, null, e, durationMs);
            
            throw handleError(e, request);
        } finally {
            sample.stop(meterRegistry.timer(id() + ".inference.duration"));
            span.end();
        }
    }

    /**
     * Template method for streaming inference
     */
    @Override
    public Multi<StreamChunk> stream(ProviderRequest request) {
        Span span = tracer.spanBuilder(id() + ".stream").startSpan();
        Timer.Sample sample = Timer.start(meterRegistry);

        try {
            enrichSpan(span, request, true);

            log.debugf("%s streaming: model=%s, messages=%d",
                id(), request.getModel(), request.getMessages().size());

            // Create provider-specific streaming request
            Object providerRequest = createProviderRequest(request);

            // Counters
            AtomicInteger tokenCount = new AtomicInteger(0);
            AtomicLong startTime = new AtomicLong(System.currentTimeMillis());
            AtomicLong firstChunkTime = new AtomicLong(0);

            // Execute streaming
            return executeStreaming(providerRequest, request)
                .onItem().invoke(chunk -> {
                    // Record TTFT
                    if (firstChunkTime.compareAndSet(0, System.currentTimeMillis())) {
                        long ttft = firstChunkTime.get() - startTime.get();
                        metrics.recordTimeToFirstToken(id(), request, ttft);
                    }
                })
                .map(chunk -> mapStreamChunk(chunk, request, tokenCount))
                .onTermination().invoke((failure, cancellation, subscription) -> {
                    long duration = System.currentTimeMillis() - startTime.get();

                    if (failure != null) {
                        metrics.recordFailure(id(), request, (Exception) failure);
                        auditor.auditStream(id(), request, tokenCount.get(), 
                            duration, firstChunkTime.get(), failure);
                    } else {
                        metrics.recordStreamSuccess(id(), request, 
                            tokenCount.get(), duration, firstChunkTime.get());
                        auditor.auditStream(id(), request, tokenCount.get(), 
                            duration, firstChunkTime.get(), null);
                    }

                    sample.stop(meterRegistry.timer(id() + ".stream.duration"));
                    span.end();
                });

        } catch (Exception e) {
            metrics.recordFailure(id(), request, e);
            span.recordException(e);
            span.end();
            throw handleError(e, request);
        }
    }

    /**
     * Enrich trace span with attributes
     */
    protected void enrichSpan(Span span, ProviderRequest request, boolean streaming) {
        span.setAttribute("provider", id());
        span.setAttribute("model", request.getModel());
        span.setAttribute("streaming", streaming);
        span.setAttribute("tenant", request.getTenantContext().getTenantId());
        span.setAttribute("message_count", request.getMessages().size());
    }

    /**
     * Handle and wrap errors
     */
    protected RuntimeException handleError(Exception e, ProviderRequest request) {
        log.errorf(e, "%s provider error for request %s", id(), request.getRequestId());

        return new ProviderException(
            id() + " inference failed: " + e.getMessage(),
            e,
            createErrorPayload(e, request)
        );
    }

    /**
     * Estimate tokens from text
     */
    protected int estimateTokens(String text) {
        if (text == null || text.isEmpty()) return 0;
        // Simple estimation: ~1.3 tokens per word
        return (int) (text.split("\\s+").length * 1.3);
    }

    /**
     * Calculate tokens per second
     */
    protected double calculateTokensPerSecond(int tokens, long durationMs) {
        if (durationMs == 0) return 0;
        return (tokens * 1000.0) / durationMs;
    }
}
```

---

## 3Ô∏è‚É£ Shared Utilities

### ProviderMetrics.java

```java
package tech.kayys.wayang.inference.providers.common.metrics;

import io.micrometer.core.instrument.MeterRegistry;
import io.micrometer.core.instrument.Tag;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import tech.kayys.wayang.inference.api.InferenceResponse;
import tech.kayys.wayang.inference.provider.ProviderRequest;

import java.time.Duration;
import java.util.Arrays;

/**
 * Centralized metrics collection for all providers
 */
@ApplicationScoped
public class ProviderMetrics {

    @Inject
    MeterRegistry registry;

    public void recordSuccess(
        String providerId,
        ProviderRequest request,
        InferenceResponse response,
        long durationMs
    ) {
        Iterable<Tag> tags = tags(providerId, request, "success");

        registry.counter("provider.inference.total", tags).increment();
        registry.counter("provider.tokens.total", tags)
            .increment(response.getTokensUsed());
        registry.timer("provider.inference.duration", tags)
            .record(Duration.ofMillis(durationMs));
    }

    public void recordStreamSuccess(
        String providerId,
        ProviderRequest request,
        int tokens,
        long durationMs,
        long ttftMs
    ) {
        Iterable<Tag> tags = tags(providerId, request, "success");

        registry.counter("provider.stream.total", tags).increment();
        registry.counter("provider.tokens.total", tags).increment(tokens);
        registry.timer("provider.stream.duration", tags)
            .record(Duration.ofMillis(durationMs));
        registry.timer("provider.time_to_first_token", tags)
            .record(Duration.ofMillis(ttftMs));
    }

    public void recordFailure(
        String providerId,
        ProviderRequest request,
        Exception error
    ) {
        Iterable<Tag> tags = Arrays.asList(
            Tag.of("provider", providerId),
            Tag.of("model", request.getModel()),
            Tag.of("tenant", request.getTenantContext().getTenantId()),
            Tag.of("status", "error"),
            Tag.of("error_type", error.getClass().getSimpleName())
        );

        registry.counter("provider.inference.total", tags).increment();
    }

    public void recordTimeToFirstToken(
        String providerId,
        ProviderRequest request,
        long ttftMs
    ) {
        Iterable<Tag> tags = tags(providerId, request, "success");
        registry.timer("provider.time_to_first_token", tags)
            .record(Duration.ofMillis(ttftMs));
    }

    private Iterable<Tag> tags(
        String providerId,
        ProviderRequest request,
        String status
    ) {
        return Arrays.asList(
            Tag.of("provider", providerId),
            Tag.of("model", request.getModel()),
            Tag.of("tenant", request.getTenantContext().getTenantId()),
            Tag.of("status", status)
        );
    }
}
```

### ProviderAuditor.java

```java
package tech.kayys.wayang.inference.providers.common.audit;

import jakarta.enterprise.context.ApplicationScoped;
import org.jboss.logging.Logger;
import tech.kayys.wayang.inference.api.AuditPayload;
import tech.kayys.wayang.inference.api.InferenceResponse;
import tech.kayys.wayang.inference.provider.ProviderRequest;

/**
 * Centralized audit logging for all providers
 */
@ApplicationScoped
public class ProviderAuditor {

    private static final Logger LOG = Logger.getLogger(ProviderAuditor.class);

    public void auditInference(
        String providerId,
        ProviderRequest request,
        InferenceResponse response,
        Exception error,
        long durationMs
    ) {
        AuditPayload.Builder audit = AuditPayload.builder()
            .runId(request.getRequestId())
            .event(error == null ? "INFERENCE_SUCCESS" : "INFERENCE_FAILED")
            .level(error == null ? "INFO" : "ERROR")
            .actor(AuditPayload.Actor.system(providerId + "-provider"))
            .tag("provider:" + providerId)
            .tag("model:" + request.getModel())
            .metadata("tenant", request.getTenantContext().getTenantId())
            .metadata("model", request.getModel())
            .metadata("streaming", request.isStreaming())
            .metadata("message_count", request.getMessages().size())
            .metadata("duration_ms", durationMs);

        if (response != null) {
            audit.metadata("tokens_used", response.getTokensUsed());
        }

        if (error != null) {
            audit.metadata("error", error.getMessage())
                .metadata("error_type", error.getClass().getSimpleName());
        }

        LOG.infof("Audit: %s", audit.build());
    }

    public void auditStream(
        String providerId,
        ProviderRequest request,
        int tokens,
        long durationMs,
        long ttftMs,
        Throwable error
    ) {
        AuditPayload.Builder audit = AuditPayload.builder()
            .runId(request.getRequestId())
            .event(error == null ? "STREAM_COMPLETED" : "STREAM_FAILED")
            .level(error == null ? "INFO" : "ERROR")
            .actor(AuditPayload.Actor.system(providerId + "-provider"))
            .tag("provider:" + providerId)
            .tag("streaming")
            .tag("model:" + request.getModel())
            .metadata("tenant", request.getTenantContext().getTenantId())
            .metadata("model", request.getModel())
            .metadata("tokens_streamed", tokens)
            .metadata("duration_ms", durationMs)
            .metadata("time_to_first_token_ms", ttftMs);

        if (error != null) {
            audit.metadata("error", error.getMessage())
                .metadata("error_type", error.getClass().getSimpleName());
        }

        LOG.infof("Audit: %s", audit.build());
    }
}
```

### ProviderException.java

```java
package tech.kayys.wayang.inference.providers.common;

import tech.kayys.wayang.inference.api.ErrorPayload;

public class ProviderException extends RuntimeException {

    private final ErrorPayload errorPayload;

    public ProviderException(String message, Throwable cause, ErrorPayload errorPayload) {
        super(message, cause);
        this.errorPayload = errorPayload;
    }

    public ErrorPayload getErrorPayload() {
        return errorPayload;
    }
}
```

---

## 4Ô∏è‚É£ Ollama Provider (Using Base)

### pom.xml

```xml
<dependencies>
    <!-- Use common base -->
    <dependency>
        <groupId>tech.kayys.wayang</groupId>
        <artifactId>inference-providers-common</artifactId>
    </dependency>
    
    <!-- Minimal additional dependencies -->
</dependencies>
```

### OllamaProvider.java

```java
package tech.kayys.wayang.inference.providers.ollama;

import io.smallrye.mutiny.Multi;
import io.smallrye.mutiny.Uni;
import jakarta.enterprise.context.ApplicationScoped;
import jakarta.inject.Inject;
import tech.kayys.wayang.inference.api.Message;
import tech.kayys.wayang.inference.api.StreamChunk;
import tech.kayys.wayang.inference.api.InferenceResponse;
import tech.kayys.wayang.inference.api.TenantContext;
import tech.kayys.wayang.inference.provider.ProviderCapabilities;
import tech.kayys.wayang.inference.provider.ProviderRequest;
import tech.kayys.wayang.inference.providers.common.AbstractLLMProvider;
import tech.kayys.wayang.inference.providers.common.config.ProviderConfig;
import tech.kayys.wayang.inference.providers.ollama.model.*;

import java.util.concurrent.atomic.AtomicInteger;
import java.util.stream.Collectors;

/**
 * Ollama Provider - Local LLM inference
 * 
 * Features:
 * - Local deployment (no API keys needed)
 * - Multiple model support (Llama, Mistral, etc.)
 * - Streaming support
 * - No rate limits
 * - Full privacy (no data leaves local network)
 */
@ApplicationScoped
public class OllamaProvider extends AbstractLLMProvider {

    private static final String PROVIDER_ID = "ollama";

    @Inject
    OllamaClient client;

    @Inject
    OllamaConfig config;

    private final ProviderCapabilities capabilities = ProviderCapabilities.builder()
        .streaming(true)
        .functionCalling(false) // Ollama doesn't support native function calling
        .multimodal(true) // Some Ollama models support vision
        .maxContextTokens(128000) // Depends on model
        .supportedModels(
            "llama3.3:70b",
            "llama3.1:70b",
            "llama3.1:8b",
            "mistral:7b",
            "mixtral:8x7b",
            "phi3:medium",
            "gemma2:27b",
            "codellama:13b"
        )
        .metadata("deployment", "local")
        .metadata("requires_api_key", "false")
        .build();

    @Override
    public String id() {
        return PROVIDER_ID;
    }

    @Override
    public ProviderCapabilities capabilities() {
        return capabilities;
    }

    @Override
    protected ProviderConfig getConfig() {
        return new ProviderConfig() {
            @Override
            public String baseUrl() {
                return config.baseUrl();
            }

            @Override
            public int maxRetries() {
                return config.maxRetries();
            }
        };
    }

    @Override
    protected OllamaRequest createProviderRequest(ProviderRequest request) {
        return OllamaRequest.builder()
            .model(request.getModel())
            .messages(request.getMessages().stream()
                .map(this::mapMessage)
                .collect(Collectors.toList()))
            .stream(request.isStreaming())
            .options(OllamaOptions.builder()
                .temperature(getParam(request, "temperature", 0.7))
                .topP(getParam(request, "top_p", 1.0))
                .numPredict(getParam(request, "max_tokens", -1))
                .build())
            .build();
    }

    @Override
    protected Uni<OllamaResponse> executeInference(
        OllamaRequest providerRequest,
        ProviderRequest originalRequest
    ) {
        return client.chat(providerRequest);
    }

    @Override
    protected Multi<OllamaStreamChunk> executeStreaming(
        OllamaRequest providerRequest,
        ProviderRequest originalRequest
    ) {
        return client.chatStream(providerRequest.withStream(true));
    }

    @Override
    protected InferenceResponse mapResponse(
        OllamaResponse providerResponse,
        ProviderRequest originalRequest,
        long durationMs
    ) {
        String content = providerResponse.getMessage() != null
            ? providerResponse.getMessage().getContent()
            : "";

        int tokens = providerResponse.getEvalCount() != null
            ? providerResponse.getEvalCount()
            : estimateTokens(content);

        return InferenceResponse.builder()
            .requestId(originalRequest.getRequestId())
            .content(content)
            .model(providerResponse.getModel())
            .tokensUsed(tokens)
            .durationMs(durationMs)
            .metadata("eval_count", providerResponse.getEvalCount())
            .metadata("eval_duration_ms", providerResponse.getEvalDuration())
            .metadata("load_duration_ms", providerResponse.getLoadDuration())
            .metadata("total_duration_ms", providerResponse.getTotalDuration())
            .build();
    }

    @Override
    protected StreamChunk mapStreamChunk(
        OllamaStreamChunk providerChunk,
        ProviderRequest originalRequest,
        AtomicInteger tokenCounter
    ) {
        String delta = providerChunk.getMessage() != null
            ? providerChunk.getMessage().getContent()
            : "";

        if (delta != null && !delta.isEmpty()) {
            tokenCounter.addAndGet(estimateTokens(delta));
        }

        return StreamChunk.builder()
            .requestId(originalRequest.getRequestId())
            .delta(delta)
            .isFinal(providerChunk.getDone() != null && providerChunk.getDone())
            .metadata("model", providerChunk.getModel())
            .metadata("eval_count", providerChunk.getEvalCount())
            .build();
    }

    @Override
    protected String resolveCredentials(TenantContext tenantContext) {
        // Ollama doesn't require API keys
        return null;
    }

    private OllamaMessage mapMessage(Message message) {
        return OllamaMessage.builder()
            .role(message.getRole().name().toLowerCase())
            .content(message.getContent())
            .build();
    }

    @SuppressWarnings("unchecked")
    private <T> T getParam(ProviderRequest request, String key, T defaultValue) {
        Object value = request.getParameters().get(key);
        return value != null ? (T) value : defaultValue;
    }
}
```

### OllamaClient.java

```java
package tech.kayys.wayang.inference.providers.ollama;

import io.smallrye.mutiny.Multi;
import io.smallrye.mutiny.Uni;
import jakarta.ws.rs.*;
import jakarta.ws.rs.core.MediaType;
import org.eclipse.microprofile.rest.client.inject.RegisterRestClient;
import tech.kayys.wayang.inference.providers.ollama.model.*;

/**
 * Ollama REST API client
 */
@RegisterRestClient(configKey = "ollama")
@Path("/api")
public interface OllamaClient {

    /**
     * Chat completion (non-streaming)
     */
    @POST
    @Path("/chat")
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON)
    Uni<OllamaResponse> chat(OllamaRequest request);

    /**
     * Chat completion (streaming)
     */
    @POST
    @Path("/chat")
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON) // Ollama uses newline-delimited JSON
    Multi<OllamaStreamChunk> chatStream(OllamaRequest request);

    /**
     * List available models
     */
    @GET
    @Path("/tags")
    @Produces(MediaType.APPLICATION_JSON)
    Uni<OllamaModelsResponse> listModels();

    /**
     * Pull a model
     */
    @POST
    @Path("/pull")
    @Consumes(MediaType.APPLICATION_JSON)
    @Produces(MediaType.APPLICATION_JSON)
    Multi<OllamaPullResponse> pullModel(OllamaPullRequest request);
}
```

### Model Classes (Simplified)

```java
// OllamaRequest.java
@JsonInclude(JsonInclude.Include.NON_NULL)
public class OllamaRequest {
    @JsonProperty("model")
    private String model;

    @JsonProperty("messages")
    private List<OllamaMessage> messages;

    @JsonProperty("stream")
    private Boolean stream;

    @JsonProperty("options")
    private OllamaOptions options;

    // Builder, getters, setters
    public OllamaRequest withStream(boolean stream) {
        this.stream = stream;
        return this;
    }
}

// OllamaMessage.java
public class OllamaMessage {
    @JsonProperty("role")
    private String role;

    @JsonProperty("content")
    private String content;

    // Builder, getters, setters
}

// OllamaOptions.java
@JsonInclude(JsonInclude.Include.NON_NULL)
public class OllamaOptions {
    @JsonProperty("temperature")
    private Double temperature;

    @JsonProperty("top_p")
    private Double topP;

    @JsonProperty("num_predict")
    private Integer numPredict;

    // Builder, getters, setters
}

// OllamaResponse.java
public class OllamaResponse {
    @JsonProperty("model")
    private String model;

    @JsonProperty("message")
    private OllamaMessage message;

    @JsonProperty("done")
    private Boolean done;

    @JsonProperty("eval_count")
    private Integer evalCount;

    @JsonProperty("eval_duration")
    private Long evalDuration;

    @JsonProperty("load_duration")
    private Long loadDuration;

    @JsonProperty("total_duration")
    private Long totalDuration;

    // Getters, setters
}

// OllamaStreamChunk.java
public class OllamaStreamChunk {
    @JsonProperty("model")
    private String model;

    @JsonProperty("message")
    private OllamaMessage message;

    @JsonProperty("done")
    private Boolean done;

    @JsonProperty("eval