

// Rate limiting middleware
func rateLimitMiddleware() gin.HandlerFunc {
	// Simple in-memory rate limiter
	requests := make(map[string][]time.Time)
	var mutex sync.RWMutex
	
	return func(c *gin.Context) {
		clientIP := c.ClientIP()
		now := time.Now()
		
		mutex.Lock()
		// Clean old requests (older than 1 minute)
		if reqs, exists := requests[clientIP]; exists {
			var validReqs []time.Time
			for _, reqTime := range reqs {
				if now.Sub(reqTime) < time.Minute {
					validReqs = append(validReqs, reqTime)
				}
			}
			requests[clientIP] = validReqs
		}
		
		// Check rate limit (60 requests per minute)
		if len(requests[clientIP]) >= 60 {
			mutex.Unlock()
			c.JSON(http.StatusTooManyRequests, gin.H{
				"error": "Rate limit exceeded",
				"retry_after": 60,
			})
			c.Abort()
			return
		}
		
		// Add current request
		requests[clientIP] = append(requests[clientIP], now)
		mutex.Unlock()
		
		c.Next()
	}
}

// Complete main function with all integrations
func mainWithAdvancedFeatures() {
	// Initialize logging
	setupLogging()
	
	// Load and validate configuration
	config = loadConfig()
	if err := validateAndOptimizeConfig(config); err != nil {
		log.Fatalf("Configuration validation failed: %v", err)
	}
	
	// Initialize llama.cpp backend with NUMA if available
	llama.BackendInit()
	if config.Hardware.NumaStrategy != "" {
		// llama.NumaInit() - would need to implement this binding
	}
	defer llama.BackendFree()
	
	// Initialize model manager with advanced features
	modelManager = NewAdvancedModelManager(config)
	defer modelManager.Cleanup()
	
	// Initialize metrics
	metrics = &Metrics{LastRequest: time.Now()}
	
	// Pre-load default model
	if config.Models.AutoLoadDefault {
		if err := modelManager.LoadAdvancedModel(config.Models.DefaultModel, nil); err != nil {
			log.Fatalf("Failed to load default model: %v", err)
		}
	}
	
	// Setup advanced routes
	router := setupAdvancedRoutes(gin.New())
	router.Use(gin.Recovery())
	router.Use(recoverMiddleware())
	router.Use(corsMiddleware())
	router.Use(metricsMiddleware())
	router.Use(rateLimitMiddleware())
	
	// Start health monitoring
	go startHealthMonitoring()
	
	// Start periodic maintenance
	go startMaintenanceTasks()
	
	// Configure server with timeouts
	server := &http.Server{
		Addr:         fmt.Sprintf("%s:%d", config.Server.Host, config.Server.Port),
		Handler:      router,
		ReadTimeout:  time.Duration(config.Server.ReadTimeout) * time.Second,
		WriteTimeout: time.Duration(config.Server.WriteTimeout) * time.Second,
		IdleTimeout:  120 * time.Second,
	}
	
	log.Printf("ðŸš€ LLaMA Inference Server starting...")
	log.Printf("ðŸ“¡ Address: %s", server.Addr)
	log.Printf("ðŸ–¥ï¸  CPU Cores: %d, Threads: %d", runtime.NumCPU(), config.Hardware.Threads)
	log.Printf("ðŸŽ® GPU: %v, Layers: %d", config.Hardware.UseGPU, config.Hardware.GPULayers)
	log.Printf("ðŸ§  Context Size: %d, Batch Size: %d", config.Hardware.ContextSize, config.Hardware.BatchSize)
	log.Printf("âš¡ Max Concurrent: %d", config.Performance.MaxConcurrent)
	
	// Graceful shutdown
	setupGracefulShutdown(server)
	
	if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
		log.Fatalf("Server failed to start: %v", err)
	}
}

func setupLogging() {
	if config.Logging.File != "" {
		logFile, err := os.OpenFile(config.Logging.File, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0666)
		if err != nil {
			log.Printf("Failed to open log file: %v", err)
		} else {
			log.SetOutput(logFile)
		}
	}
}

func startHealthMonitoring() {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()
	
	for range ticker.C {
		// Monitor model health
		modelManager.mutex.RLock()
		for name, model := range modelManager.models {
			if model == nil {
				log.Printf("Warning: Model %s is nil", name)
				continue
			}
			
			// Basic health check - try to get model info
			info := getModelInfo(model)
			if info == nil {
				log.Printf("Warning: Model %s health check failed", name)
			}
		}
		modelManager.mutex.RUnlock()
		
		// Monitor memory usage
		var memStats runtime.MemStats
		runtime.ReadMemStats(&memStats)
		
		// Alert if memory usage is high
		if memStats.Alloc > 8*1024*1024*1024 { // 8GB threshold
			log.Printf("Warning: High memory usage: %d MB", memStats.Alloc/(1024*1024))
		}
		
		// Alert if error rate is high
		metrics.mutex.RLock()
		errorRate := float64(metrics.ErrorCount) / float64(metrics.RequestCount)
		metrics.mutex.RUnlock()
		
		if metrics.RequestCount > 100 && errorRate > 0.1 {
			log.Printf("Warning: High error rate: %.2f%%", errorRate*100)
		}
	}
}

func startMaintenanceTasks() {
	ticker := time.NewTicker(5 * time.Minute)
	defer ticker.Stop()
	
	for range ticker.C {
		// Auto garbage collection if enabled
		if config.Performance.AutoGC {
			var memStats runtime.MemStats
			runtime.ReadMemStats(&memStats)
			
			if memStats.Alloc > uint64(config.Performance.GCThreshold)*1024*1024 {
				log.Printf("Running automatic garbage collection")
				runtime.GC()
			}
		}
		
		// Clean up old WebSocket connections
		wsManager.mutex.Lock()
		for id, conn := range wsManager.connections {
			// Ping connection to check if alive
			if err := conn.WriteMessage(websocket.PingMessage, nil); err != nil {
				conn.Close()
				delete(wsManager.connections, id)
				log.Printf("Cleaned up dead WebSocket connection: %s", id)
			}
		}
		wsManager.mutex.Unlock()
		
		// Reset performance counters periodically
		if config.Performance.EnablePerformanceLogging {
			logPerformanceStats()
		}
	}
}

func logPerformanceStats() {
	metrics.mutex.RLock()
	defer metrics.mutex.RUnlock()
	
	log.Printf("Performance Stats - Requests: %d, Errors: %d, Avg Latency: %dms, Active: %d, Total Tokens: %d",
		metrics.RequestCount,
		metrics.ErrorCount,
		metrics.AvgLatency.Milliseconds(),
		metrics.ActiveRequests,
		metrics.TotalTokens,
	)
}

func setupGracefulShutdown(server *http.Server) {
	// Implementation would handle SIGTERM/SIGINT for graceful shutdown
	// This is a placeholder for the concept
	log.Printf("Graceful shutdown configured")
}

// Advanced model manager with caching and optimization
func NewAdvancedModelManager(cfg *Config) *ModelManager {
	mm := &ModelManager{
		models:       make(map[string]*llama.LLama),
		modelConfigs: make(map[string]*llama.ModelParams),
		config:       cfg,
		semaphore:    make(chan struct{}, cfg.Performance.MaxConcurrent),
	}
	
	// Initialize model cache
	if cfg.Models.ModelCacheSize > 0 {
		mm.modelCache = make(map[string]*modelCacheEntry, cfg.Models.ModelCacheSize)
	}
	
	return mm
}

type modelCacheEntry struct {
	model      *llama.LLama
	lastUsed   time.Time
	useCount   int64
	memorySize uint64
}

// Enhanced model manager with caching
func (mm *ModelManager) modelCache map[string]*modelCacheEntry

// Integration test suite
func runIntegrationTests() error {
	log.Printf("Running integration tests...")
	
	tests := []struct {
		name string
		test func() error
	}{
		{"Basic Health Check", testBasicHealth},
		{"Model Loading", testModelLoading},
		{"Simple Inference", testSimpleInference},
		{"Streaming Inference", testStreamingInference},
		{"Batch Processing", testBatchProcessing},
		{"Context Management", testContextManagement},
		{"MCP Protocol", testMCPProtocol},
		{"WebSocket Connection", testWebSocket},
		{"Performance Under Load", testPerformanceLoad},
	}
	
	for _, test := range tests {
		log.Printf("Running test: %s", test.name)
		if err := test.test(); err != nil {
			return fmt.Errorf("test %s failed: %v", test.name, err)
		}
		log.Printf("âœ“ Test passed: %s", test.name)
	}
	
	log.Printf("âœ… All integration tests passed")
	return nil
}

func testBasicHealth() error {
	resp, err := http.Get("http://localhost:8080/health")
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("health check failed with status: %d", resp.StatusCode)
	}
	
	return nil
}

func testModelLoading() error {
	// Test dynamic model loading
	payload := `{"gpu_layers": 20, "vocab_only": false}`
	resp, err := http.Post(
		"http://localhost:8080/v2/models/llama2-7b-chat/load",
		"application/json",
		strings.NewReader(payload),
	)
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	
	return nil
}

func testSimpleInference() error {
	payload := `{
		"model": "llama2-7b-chat",
		"messages": [{"role": "user", "content": "Hello"}],
		"max_tokens": 50,
		"temperature": 0.7
	}`
	
	resp, err := http.Post(
		"http://localhost:8080/v1/chat/completions",
		"application/json",
		strings.NewReader(payload),
	)
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("inference failed with status: %d", resp.StatusCode)
	}
	
	return nil
}

func testStreamingInference() error {
	payload := `{
		"model": "llama2-7b-chat",
		"messages": [{"role": "user", "content": "Count to 5"}],
		"max_tokens": 30,
		"stream": true
	}`
	
	resp, err := http.Post(
		"http://localhost:8080/v1/chat/completions",
		"application/json",
		strings.NewReader(payload),
	)
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	
	// Read streaming response
	scanner := bufio.NewScanner(resp.Body)
	tokenCount := 0
	for scanner.Scan() {
		line := scanner.Text()
		if strings.HasPrefix(line, "data: ") && !strings.Contains(line, "[DONE]") {
			tokenCount++
		}
	}
	
	if tokenCount == 0 {
		return fmt.Errorf("no streaming tokens received")
	}
	
	return nil
}

func testBatchProcessing() error {
	payload := `[
		{
			"model": "llama2-7b-chat",
			"messages": [{"role": "user", "content": "Hello"}],
			"max_tokens": 20
		},
		{
			"model": "llama2-7b-chat",
			"messages": [{"role": "user", "content": "Goodbye"}],
			"max_tokens": 20
		}
	]`
	
	resp, err := http.Post(
		"http://localhost:8080/v2/inference/batch",
		"application/json",
		strings.NewReader(payload),
	)
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("batch processing failed with status: %d", resp.StatusCode)
	}
	
	return nil
}

func testContextManagement() error {
	// Test context saving
	payload := `{
		"file_path": "/tmp/test_context.bin",
		"tokens": [1, 2, 3, 4, 5]
	}`
	
	resp, err := http.Post(
		"http://localhost:8080/v2/context/1/save",
		"application/json",
		strings.NewReader(payload),
	)
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	
	return nil
}

func testMCPProtocol() error {
	// Test MCP capabilities
	resp, err := http.Get("http://localhost:8080/mcp/capabilities")
	if err != nil {
		return err
	}
	defer resp.Body.Close()
	
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("MCP capabilities failed with status: %d", resp.StatusCode)
	}
	
	// Test MCP protocol info
	resp2, err := http.Get("http://localhost:8080/mcp/protocol")
	if err != nil {
		return err
	}
	defer resp2.Body.Close()
	
	return nil
}

func testWebSocket() error {
	// Basic WebSocket connection test
	// In a real implementation, this would use a WebSocket client
	log.Printf("WebSocket test placeholder - would connect to ws://localhost:8080/ws")
	return nil
}

func testPerformanceLoad() error {
	// Simple load test with concurrent requests
	concurrency := 3
	requests := 10
	
	var wg sync.WaitGroup
	errors := make(chan error, concurrency)
	
	for i := 0; i < concurrency; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			
			for j := 0; j < requests/concurrency; j++ {
				payload := `{
					"model": "llama2-7b-chat",
					"messages": [{"role": "user", "content": "Test"}],
					"max_tokens": 10
				}`
				
				resp, err := http.Post(
					"http://localhost:8080/v1/chat/completions",
					"application/json",
					strings.NewReader(payload),
				)
				if err != nil {
					errors <- err
					return
				}
				resp.Body.Close()
				
				if resp.StatusCode != http.StatusOK {
					errors <- fmt.Errorf("request failed with status: %d", resp.StatusCode)
					return
				}
			}
		}()
	}
	
	wg.Wait()
	close(errors)
	
	for err := range errors {
		if err != nil {
			return err
		}
	}
	
	return nil
}

// Production deployment helpers
func createProductionDeployment() {
	log.Printf("Setting up production deployment configuration...")
	
	// Create systemd service file
	serviceContent := `[Unit]
Description=LLaMA Inference Server
After=network.target
Wants=network.target

[Service]
Type=simple
User=llama
Group=llama
WorkingDirectory=/opt/llama-inference
ExecStart=/opt/llama-inference/llama-inference-server
ExecReload=/bin/kill -HUP $MAINPID
Restart=always
RestartSec=10
StandardOutput=journal
StandardError=journal
SyslogIdentifier=llama-inference

# Resource limits
LimitNOFILE=65536
LimitNPROC=32768
LimitMEMLOCK=infinity

# Environment
Environment=CONFIG_PATH=/opt/llama-inference/config.yaml
Environment=GIN_MODE=release
Environment=GOMAXPROCS=16

# Security
NoNewPrivileges=true
PrivateTmp=true
ProtectSystem=strict
ProtectHome=true
ReadWritePaths=/opt/llama-inference/logs /opt/llama-inference/models

[Install]
WantedBy=multi-user.target`
	
	os.WriteFile("llama-inference.service", []byte(serviceContent), 0644)
	
	// Create monitoring configuration
	monitoringConfig := `# Grafana dashboard configuration
{
  "dashboard": {
    "title": "LLaMA Inference Server",
    "panels": [
      {
        "title": "Request Rate",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(llama_requests_total[5m])",
            "legendFormat": "Requests/sec"
          }
        ]
      },
      {
        "title": "Response Time",
        "type": "graph", 
        "targets": [
          {
            "expr": "llama_request_duration_seconds",
            "legendFormat": "Response Time"
          }
        ]
      },
      {
        "title": "GPU Utilization",
        "type": "singlestat",
        "targets": [
          {
            "expr": "llama_gpu_utilization_percent",
            "legendFormat": "GPU %"
          }
        ]
      }
    ]
  }
}`
	
	os.WriteFile("grafana-dashboard.json", []byte(monitoringConfig), 0644)
	
	log.Printf("âœ… Production deployment files created")
}

// Final integration points for Quarkus MCP
type MCPIntegration struct {
	ProtocolVersion string                 `json:"protocol_version"`
	ServerInfo      map[string]interface{} `json:"server_info"`
	Capabilities    map[string]interface{} `json:"capabilities"`
	Tools           []MCPTool              `json:"tools"`
	Resources       []MCPResource          `json:"resources"`
}

type MCPResource struct {
	URI         string `json:"uri"`
	Name        string `json:"name"`
	Description string `json:"description"`
	MimeType    string `json:"mimeType"`
}

func getMCPIntegrationInfo() MCPIntegration {
	return MCPIntegration{
		ProtocolVersion: "1.0",
		ServerInfo: map[string]interface{}{
			"name":    "llama-inference-server",
			"version": "1.0.0",
			"vendor":  "Custom LLaMA.cpp Integration",
		},
		Capabilities: map[string]interface{}{
			"tools": map[string]bool{
				"listChanged": false,
			},
			"resources": map[string]bool{
				"subscribe":   true,
				"listChanged": false,
			},
			"prompts": map[string]bool{
				"listChanged": false,
			},
			"inference": map[string]bool{
				"streaming":       true,
				"batch":          true,
				"context_caching": true,
				"gpu_acceleration": config.Hardware.UseGPU,
			},
		},
		Tools: []MCPTool{
			{
				Name:        "llama_inference",
				Description: "Perform LLaMA model inference with advanced options",
				InputSchema: map[string]interface{}{
					"type": "object",
					"properties": map[string]interface{}{
						"messages": map[string]interface{}{
							"type": "array",
							"items": map[string]interface{}{
								"type": "object",
								"properties": map[string]interface{}{
									"role":    map[string]string{"type": "string"},
									"content": map[string]string{"type": "string"},
								},
							},
						},
						"model":       map[string]string{"type": "string"},
						"temperature": map[string]string{"type": "number"},
						"max_tokens":  map[string]string{"type": "integer"},
					},
					"required": []string{"messages"},
				},
			},
		},
		Resources: []MCPResource{
			{
				URI:         "llama://models",
				Name:        "Available Models",
				Description: "List of available LLaMA models",
				MimeType:    "application/json",
			},
			{
				URI:         "llama://metrics",
				Name:        "Performance Metrics", 
				Description: "Real-time performance metrics",
				MimeType:    "application/json",
			},
		},
	}
}

// Startup banner and information
func printStartupBanner() {
	banner := `
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              LLaMA Inference Server              â•‘
â•‘                                                  â•‘
â•‘  ðŸ¦™ Powered by llama.cpp                         â•‘
â•‘  ðŸš€ Built with Go                                â•‘
â•‘  ðŸ”— MCP Protocol Compatible                      â•‘
â•‘  âš¡ GPU Acceleration Ready                       â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
`
	fmt.Println(banner)
	
	fmt.Printf("Version: 1.0.0\n")
	fmt.Printf("Build Type: %s\n", getBuildType())
	fmt.Printf("System: %s/%s\n", runtime.GOOS, runtime.GOARCH)
	fmt.Printf("Go Version: %s\n", runtime.Version())
	fmt.Printf("CPU Cores: %d\n", runtime.NumCPU())
	
	if config.Hardware.UseGPU {
		fmt.Printf("ðŸŽ® GPU Acceleration: ENABLED (%d layers)\n", config.Hardware.GPULayers)
	} else {
		fmt.Printf("ðŸ–¥ï¸  CPU-Only Mode: ENABLED\n")
	}
	
	fmt.Println()
}

func getBuildType() string {
	if config.Hardware.UseGPU {
		return "GPU (CUDA)"
	}
	return "CPU"
}

// Final initialization with all features
func init() {
	// Set up signal handling for graceful shutdown
	// Set up logging configuration
	// Initialize performance monitoring
	// Set up health checks
	
	log.Printf("LLaMA Inference Server initializing...")
}
	}
	
	// Add grammar sampler if specified
	if req.Grammar != "" {
		grammarSampler := llama.NewSamplerGrammar(vocab, req.Grammar, "root")
		if grammarSampler != nil {
			chain.ChainAdd(grammarSampler)
		}
	}
	
	// Add repetition penalty
	penaltyLastN := req.PenaltyLastN
	if penaltyLastN <= 0 {
		penaltyLastN = 64
	}
	
	repeatPenalty := req.RepeatPenalty
	if repeatPenalty <= 0 {
		repeatPenalty = 1.1
	}
	
	penaltySampler := llama.NewSamplerPenalties(
		int32(penaltyLastN),
		repeatPenalty,
		req.FrequencyPenalty,
		req.PresencePenalty,
	)
	chain.ChainAdd(penaltySampler)
	
	// Add Top-K if specified
	if req.TopK > 0 {
		chain.ChainAdd(llama.NewSamplerTopK(int32(req.TopK)))
	}
	
	// Add Top-P
	topP := req.TopP
	if topP <= 0 {
		topP = 0.9
	}
	chain.ChainAdd(llama.NewSamplerTopP(topP, 1))
	
	// Add Min-P if specified
	if req.MinP > 0 {
		chain.ChainAdd(llama.NewSamplerMinP(req.MinP, 1))
	}
	
	// Add Typical P if specified
	if req.TypicalP > 0 && req.TypicalP < 1.0 {
		chain.ChainAdd(llama.NewSamplerTypical(req.TypicalP, 1))
	}
	
	// Add temperature
	temperature := req.Temperature
	if temperature <= 0 {
		temperature = 0.8
	}
	chain.ChainAdd(llama.NewSamplerTemp(temperature))
	
	// Add Mirostat if enabled
	if req.Mirostat == 1 {
		tau := req.MirostatTau
		if tau <= 0 {
			tau = 5.0
		}
		eta := req.MirostatEta
		if eta <= 0 {
			eta = 0.1
		}
		
		mirostatSampler := llama.NewSamplerMirostat(
			vocab.NTokens(),
			uint32(time.Now().UnixNano()),
			tau,
			eta,
			100,
		)
		chain.ChainAdd(mirostatSampler)
	} else if req.Mirostat == 2 {
		tau := req.MirostatTau
		if tau <= 0 {
			tau = 5.0
		}
		eta := req.MirostatEta
		if eta <= 0 {
			eta = 0.1
		}
		
		mirostatV2Sampler := llama.NewSamplerMirostatV2(
			uint32(time.Now().UnixNano()),
			tau,
			eta,
		)
		chain.ChainAdd(mirostatV2Sampler)
	} else {
		// Default distribution sampler
		seed := uint32(time.Now().UnixNano())
		if req.Metadata != nil {
			if seedVal, ok := req.Metadata["seed"].(float64); ok {
				seed = uint32(seedVal)
			}
		}
		chain.ChainAdd(llama.NewSamplerDist(seed))
	}
	
	return chain
}

func generateWithAdvancedOptions(model *llama.LLama, sampler *llama.Sampler, req AdvancedInferenceRequest, promptTokens, seqID int) ([]llama.Token, string) {
	maxTokens := req.MaxTokens
	if maxTokens <= 0 {
		maxTokens = 512
	}
	
	var generatedTokens []llama.Token
	var textBuilder strings.Builder
	vocab := model.Model.Vocab()
	
	// Stop sequences
	stopSequences := req.Stop
	if len(stopSequences) == 0 {
		// Add default stop sequences
		stopSequences = []string{"</s>", "<|endoftext|>", "\n\nUser:", "\n\nHuman:"}
	}
	
	for i := 0; i < maxTokens; i++ {
		token := sampler.Sample(model.Context, -1)
		
		// Check for end-of-generation
		if vocab.IsEog(token) {
			break
		}
		
		// Convert token to text
		tokenText := vocab.GetText(token)
		generatedTokens = append(generatedTokens, token)
		textBuilder.WriteString(tokenText)
		
		// Check stop sequences
		currentText := textBuilder.String()
		shouldStop := false
		for _, stopSeq := range stopSequences {
			if strings.Contains(currentText, stopSeq) {
				// Remove stop sequence from output
				if idx := strings.Index(currentText, stopSeq); idx >= 0 {
					textBuilder.Reset()
					textBuilder.WriteString(currentText[:idx])
				}
				shouldStop = true
				break
			}
		}
		
		if shouldStop {
			break
		}
		
		sampler.Accept(token)
		
		// Continue generation
		batch := llama.NewBatch(1, 0, 1)
		batch.Add(token, llama.Pos(promptTokens+i), []llama.SeqID{llama.SeqID(seqID)}, true)
		
		if err := model.Decode(batch); err != nil {
			batch.Free()
			log.Printf("Decode error at token %d: %v", i, err)
			break
		}
		batch.Free()
		
		// Context management
		if promptTokens+i+1 >= config.Hardware.ContextSize && req.ContextShift {
			// Implement context shifting
			shiftContext(model, seqID, config.Hardware.ContextSize/4)
		}
	}
	
	return generatedTokens, textBuilder.String()
}

func shiftContext(model *llama.LLama, seqID int, shiftAmount int) {
	memory := model.Memory()
	if !memory.CanShift() {
		log.Printf("Context shifting not supported by model")
		return
	}
	
	// Remove old tokens from the beginning
	memory.SeqRemove(llama.SeqID(seqID), 0, llama.Pos(shiftAmount))
	
	// Shift remaining positions
	memory.SeqAdd(llama.SeqID(seqID), llama.Pos(shiftAmount), -1, llama.Pos(-shiftAmount))
	
	log.Printf("Context shifted: removed %d tokens from sequence %d", shiftAmount, seqID)
}

func handleContextCaching(model *llama.LLama, req AdvancedInferenceRequest, seqID int) {
	// Check if context is already cached for this sequence
	memory := model.Memory()
	posMax := memory.SeqPosMax(llama.SeqID(seqID))
	
	if posMax >= 0 {
		log.Printf("Using cached context for sequence %d, position: %d", seqID, posMax)
		return
	}
	
	log.Printf("No cached context found for sequence %d", seqID)
}

func getAdvancedMemoryState(model *llama.LLama, seqID int) map[string]interface{} {
	memory := model.Memory()
	
	posMin := memory.SeqPosMin(llama.SeqID(seqID))
	posMax := memory.SeqPosMax(llama.SeqID(seqID))
	
	return map[string]interface{}{
		"sequence_id":     seqID,
		"position_min":    posMin,
		"position_max":    posMax,
		"sequence_length": posMax - posMin + 1,
		"can_shift":       memory.CanShift(),
		"context_usage":   float64(posMax-posMin+1) / float64(config.Hardware.ContextSize),
	}
}

func determineAdvancedFinishReason(tokens []llama.Token, stopSequences []string, vocab *llama.Vocab, maxTokens int) string {
	if len(tokens) == 0 {
		return "error"
	}
	
	lastToken := tokens[len(tokens)-1]
	if vocab.IsEog(lastToken) {
		return "stop"
	}
	
	if len(tokens) >= maxTokens {
		return "length"
	}
	
	// Check if stopped due to stop sequence
	for _, stopSeq := range stopSequences {
		if stopSeq != "" {
			return "stop"
		}
	}
	
	return "stop"
}

// Batch inference handler for processing multiple requests
func batchInferenceHandler(c *gin.Context) {
	var requests []AdvancedInferenceRequest
	if err := c.ShouldBindJSON(&requests); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid batch request format"})
		return
	}
	
	if len(requests) == 0 {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Empty batch request"})
		return
	}
	
	if len(requests) > config.Performance.MaxConcurrent {
		c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprintf("Batch size exceeds maximum concurrent requests: %d", config.Performance.MaxConcurrent)})
		return
	}
	
	// Process batch concurrently
	responses := make([]InferenceResponse, len(requests))
	var wg sync.WaitGroup
	semaphore := make(chan struct{}, config.Performance.MaxConcurrent)
	
	for i, req := range requests {
		wg.Add(1)
		go func(index int, request AdvancedInferenceRequest) {
			defer wg.Done()
			
			// Acquire semaphore
			semaphore <- struct{}{}
			defer func() { <-semaphore }()
			
			model, err := modelManager.GetModel(request.Model)
			if err != nil {
				responses[index] = InferenceResponse{Error: err.Error()}
				return
			}
			
			responses[index] = processAdvancedInference(model, request, index+1)
		}(i, req)
	}
	
	wg.Wait()
	
	c.JSON(http.StatusOK, gin.H{
		"responses": responses,
		"batch_size": len(requests),
		"timestamp": time.Now(),
	})
}

// Capabilities handler
func capabilitiesHandler(c *gin.Context) {
	capabilities := InferenceCapabilities{
		SupportedModels: func() []string {
			models := make([]string, 0, len(config.Models.ModelPaths))
			for name := range config.Models.ModelPaths {
				models = append(models, name)
			}
			return models
		}(),
		SupportedSamplers: []string{
			"greedy", "multinomial", "top_k", "top_p", "min_p",
			"typical_p", "temperature", "mirostat", "mirostat_v2",
			"repetition_penalty", "frequency_penalty", "presence_penalty",
		},
		MaxContextSize:      config.Hardware.ContextSize,
		MaxBatchSize:        config.Hardware.BatchSize,
		SupportsStreaming:   true,
		SupportsGPU:         config.Hardware.UseGPU,
		SupportsFlashAttn:   true,
		SupportsChatTemplate: true,
		Features: map[string]interface{}{
			"context_shifting":    true,
			"context_caching":     true,
			"sequence_management": true,
			"grammar_sampling":    true,
			"logit_bias":          true,
			"batch_processing":    true,
			"websocket_streaming": true,
			"performance_metrics": config.Performance.EnableMetrics,
			"memory_optimization": true,
		},
	}
	
	c.JSON(http.StatusOK, capabilities)
}

// Model warming handler
func warmModelHandler(c *gin.Context) {
	modelName := c.Param("model")
	
	model, err := modelManager.GetModel(modelName)
	if err != nil {
		if err := modelManager.LoadAdvancedModel(modelName, nil); err != nil {
			c.JSON(http.StatusNotFound, gin.H{"error": err.Error()})
			return
		}
		model, _ = modelManager.GetModel(modelName)
	}
	
	// Warm up the model with a small inference
	warmupPrompt := "Hello"
	tokens := model.Tokenize(warmupPrompt, true, true)
	
	if len(tokens) > 0 {
		batch := llama.NewBatch(len(tokens), 0, 1)
		for i, token := range tokens {
			batch.Add(token, llama.Pos(i), []llama.SeqID{0}, i == len(tokens)-1)
		}
		
		model.Decode(batch)
		batch.Free()
	}
	
	c.JSON(http.StatusOK, gin.H{
		"message": fmt.Sprintf("Model %s warmed up successfully", modelName),
		"model_info": getModelInfo(model),
	})
}

// Context management handlers
func saveContextHandler(c *gin.Context) {
	seqIDStr := c.Param("seq_id")
	seqID, err := strconv.Atoi(seqIDStr)
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid sequence ID"})
		return
	}
	
	var request struct {
		FilePath string `json:"file_path"`
		Tokens   []int  `json:"tokens,omitempty"`
	}
	
	if err := c.ShouldBindJSON(&request); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request"})
		return
	}
	
	model, err := modelManager.GetModel("")
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": err.Error()})
		return
	}
	
	// Convert tokens
	var tokens []llama.Token
	for _, t := range request.Tokens {
		tokens = append(tokens, llama.Token(t))
	}
	
	// Save sequence state
	size := model.Context.StateSeqSaveFile(
		request.FilePath,
		llama.SeqID(seqID),
		tokens,
	)
	
	if size == 0 {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to save context"})
		return
	}
	
	c.JSON(http.StatusOK, gin.H{
		"message":     "Context saved successfully",
		"sequence_id": seqID,
		"file_path":   request.FilePath,
		"size_bytes":  size,
	})
}

func loadContextHandler(c *gin.Context) {
	seqIDStr := c.Param("seq_id")
	seqID, err := strconv.Atoi(seqIDStr)
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid sequence ID"})
		return
	}
	
	var request struct {
		FilePath string `json:"file_path"`
	}
	
	if err := c.ShouldBindJSON(&request); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request"})
		return
	}
	
	model, err := modelManager.GetModel("")
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": err.Error()})
		return
	}
	
	// Load sequence state
	tokens := make([]llama.Token, 1024) // Buffer for loaded tokens
	size := model.Context.StateSeqLoadFile(
		request.FilePath,
		llama.SeqID(seqID),
		tokens,
	)
	
	if size == 0 {
		c.JSON(http.StatusInternalServerError, gin.H{"error": "Failed to load context"})
		return
	}
	
	c.JSON(http.StatusOK, gin.H{
		"message":     "Context loaded successfully",
		"sequence_id": seqID,
		"size_bytes":  size,
		"tokens_loaded": len(tokens),
	})
}

func clearContextHandler(c *gin.Context) {
	seqIDStr := c.Param("seq_id")
	seqID, err := strconv.Atoi(seqIDStr)
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid sequence ID"})
		return
	}
	
	model, err := modelManager.GetModel("")
	if err != nil {
		c.JSON(http.StatusNotFound, gin.H{"error": err.Error()})
		return
	}
	
	// Clear sequence
	memory := model.Memory()
	memory.SeqRemove(llama.SeqID(seqID), 0, -1)
	
	c.JSON(http.StatusOK, gin.H{
		"message":     "Context cleared successfully",
		"sequence_id": seqID,
	})
}

// MCP protocol handlers
func mcpProtocolHandler(c *gin.Context) {
	protocol := map[string]interface{}{
		"version": "1.0",
		"implementation": map[string]interface{}{
			"name":    "llama-inference-server",
			"version": "1.0.0",
		},
		"capabilities": map[string]interface{}{
			"tools": map[string]interface{}{
				"listChanged": false,
			},
			"resources": map[string]interface{}{
				"subscribe":   true,
				"listChanged": false,
			},
			"prompts": map[string]interface{}{
				"listChanged": false,
			},
			"logging": map[string]interface{}{},
		},
		"serverInfo": map[string]interface{}{
			"name":    "LLaMA Inference Server",
			"version": "1.0.0",
		},
	}
	
	c.JSON(http.StatusOK, protocol)
}

// Tool registration for MCP
type MCPTool struct {
	Name        string                 `json:"name"`
	Description string                 `json:"description"`
	InputSchema map[string]interface{} `json:"inputSchema"`
}

var registeredTools = make(map[string]MCPTool)
var toolsMutex sync.RWMutex

func registerToolHandler(c *gin.Context) {
	var tool MCPTool
	if err := c.ShouldBindJSON(&tool); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid tool definition"})
		return
	}
	
	toolsMutex.Lock()
	registeredTools[tool.Name] = tool
	toolsMutex.Unlock()
	
	c.JSON(http.StatusOK, gin.H{
		"message": fmt.Sprintf("Tool %s registered successfully", tool.Name),
		"tool":    tool,
	})
}

func executeToolHandler(c *gin.Context) {
	var request struct {
		Name      string                 `json:"name"`
		Arguments map[string]interface{} `json:"arguments"`
	}
	
	if err := c.ShouldBindJSON(&request); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid tool execution request"})
		return
	}
	
	toolsMutex.RLock()
	tool, exists := registeredTools[request.Name]
	toolsMutex.RUnlock()
	
	if !exists {
		c.JSON(http.StatusNotFound, gin.H{"error": fmt.Sprintf("Tool %s not found", request.Name)})
		return
	}
	
	// Execute tool (this would be implemented based on specific tool logic)
	result := executeRegisteredTool(tool, request.Arguments)
	
	c.JSON(http.StatusOK, gin.H{
		"tool":   tool.Name,
		"result": result,
	})
}

func executeRegisteredTool(tool MCPTool, args map[string]interface{}) map[string]interface{} {
	// Placeholder for tool execution logic
	// In a real implementation, this would dispatch to specific tool handlers
	return map[string]interface{}{
		"status":    "executed",
		"tool_name": tool.Name,
		"timestamp": time.Now(),
		"arguments": args,
	}
}

// Resource management for MCP
func listResourcesHandler(c *gin.Context) {
	resources := []map[string]interface{}{
		{
			"uri":         "model://loaded-models",
			"name":        "Loaded Models",
			"description": "List of currently loaded models",
			"mimeType":    "application/json",
		},
		{
			"uri":         "metrics://performance",
			"name":        "Performance Metrics",
			"description": "Real-time performance metrics",
			"mimeType":    "application/json",
		},
		{
			"uri":         "config://server",
			"name":        "Server Configuration",
			"description": "Current server configuration",
			"mimeType":    "application/json",
		},
	}
	
	c.JSON(http.StatusOK, gin.H{"resources": resources})
}

func readResourceHandler(c *gin.Context) {
	var request struct {
		URI string `json:"uri"`
	}
	
	if err := c.ShouldBindJSON(&request); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid resource request"})
		return
	}
	
	var content interface{}
	var mimeType string
	
	switch {
	case strings.HasPrefix(request.URI, "model://"):
		content = getLoadedModelsInfo()
		mimeType = "application/json"
		
	case strings.HasPrefix(request.URI, "metrics://"):
		content = getCurrentMetrics()
		mimeType = "application/json"
		
	case strings.HasPrefix(request.URI, "config://"):
		content = config
		mimeType = "application/json"
		
	default:
		c.JSON(http.StatusNotFound, gin.H{"error": "Resource not found"})
		return
	}
	
	c.JSON(http.StatusOK, gin.H{
		"contents": []map[string]interface{}{
			{
				"uri":      request.URI,
				"mimeType": mimeType,
				"text":     content,
			},
		},
	})
}

func getLoadedModelsInfo() map[string]interface{} {
	modelManager.mutex.RLock()
	defer modelManager.mutex.RUnlock()
	
	modelsInfo := make(map[string]interface{})
	for name, model := range modelManager.models {
		modelsInfo[name] = getModelInfo(model)
	}
	
	return map[string]interface{}{
		"loaded_models": modelsInfo,
		"total_models":  len(modelManager.models),
		"timestamp":     time.Now(),
	}
}

func getCurrentMetrics() map[string]interface{} {
	metrics.mutex.RLock()
	defer metrics.mutex.RUnlock()
	
	return map[string]interface{}{
		"request_count":    metrics.RequestCount,
		"error_count":      metrics.ErrorCount,
		"avg_latency_ms":   metrics.AvgLatency.Milliseconds(),
		"active_requests":  metrics.ActiveRequests,
		"total_tokens":     metrics.TotalTokens,
		"last_request":     metrics.LastRequest,
		"uptime_seconds":   time.Since(metrics.LastRequest).Seconds(),
		"memory_usage":     getMemoryUsage(),
		"timestamp":        time.Now(),
	}
}

// Administrative handlers
func adminStatusHandler(c *gin.Context) {
	var memStats runtime.MemStats
	runtime.ReadMemStats(&memStats)
	
	status := map[string]interface{}{
		"server": map[string]interface{}{
			"version":    "1.0.0",
			"uptime":     time.Since(startTime).String(),
			"goroutines": runtime.NumGoroutine(),
		},
		"models": getLoadedModelsInfo(),
		"metrics": getCurrentMetrics(),
		"hardware": map[string]interface{}{
			"cpu_cores":     runtime.NumCPU(),
			"memory_alloc":  memStats.Alloc,
			"memory_sys":    memStats.Sys,
			"gc_runs":       memStats.NumGC,
		},
		"configuration": config,
	}
	
	c.JSON(http.StatusOK, status)
}

func reloadConfigHandler(c *gin.Context) {
	// Reload configuration from file
	newConfig := loadConfig()
	if err := validateAndOptimizeConfig(newConfig); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprintf("Invalid configuration: %v", err)})
		return
	}
	
	// Update global config (thread-safe operations only)
	config = newConfig
	
	c.JSON(http.StatusOK, gin.H{
		"message": "Configuration reloaded successfully",
		"timestamp": time.Now(),
	})
}

func forceGCHandler(c *gin.Context) {
	var memBefore runtime.MemStats
	runtime.ReadMemStats(&memBefore)
	
	runtime.GC()
	runtime.GC() // Run twice for more thorough cleanup
	
	var memAfter runtime.MemStats
	runtime.ReadMemStats(&memAfter)
	
	freed := memBefore.Alloc - memAfter.Alloc
	
	c.JSON(http.StatusOK, gin.H{
		"message":       "Garbage collection completed",
		"memory_freed":  freed,
		"memory_before": memBefore.Alloc,
		"memory_after":  memAfter.Alloc,
		"gc_runs":       memAfter.NumGC,
	})
}

func debugVarsHandler(c *gin.Context) {
	vars := map[string]interface{}{
		"goroutines":     runtime.NumGoroutine(),
		"cgo_calls":      runtime.NumCgoCall(),
		"memory":         getMemoryUsage(),
		"models_loaded":  len(modelManager.models),
		"active_requests": metrics.ActiveRequests,
	}
	
	c.JSON(http.StatusOK, vars)
}

// Model quantization handler
func quantizeModelHandler(c *gin.Context) {
	var request struct {
		InputPath  string `json:"input_path"`
		OutputPath string `json:"output_path"`
		QuantType  string `json:"quant_type"`
		Threads    int    `json:"threads,omitempty"`
	}
	
	if err := c.ShouldBindJSON(&request); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid quantization request"})
		return
	}
	
	// This would require implementing the quantization bindings
	// For now, return a placeholder response
	c.JSON(http.StatusAccepted, gin.H{
		"message": "Quantization started",
		"input":   request.InputPath,
		"output":  request.OutputPath,
		"type":    request.QuantType,
	})
}

// WebSocket subscription handler
func handleWSSubscription(conn *websocket.Conn, data interface{}, connectionID string) {
	// Handle subscription to real-time metrics or inference streams
	subscriptionData, ok := data.(map[string]interface{})
	if !ok {
		conn.WriteJSON(WSMessage{Type: "error", Error: "Invalid subscription data"})
		return
	}
	
	subscriptionType, ok := subscriptionData["type"].(string)
	if !ok {
		conn.WriteJSON(WSMessage{Type: "error", Error: "Missing subscription type"})
		return
	}
	
	switch subscriptionType {
	case "metrics":
		// Send periodic metrics updates
		go func() {
			ticker := time.NewTicker(5 * time.Second)
			defer ticker.Stop()
			
			for {
				select {
				case <-ticker.C:
					wsManager.mutex.RLock()
					if _, exists := wsManager.connections[connectionID]; !exists {
						wsManager.mutex.RUnlock()
						return
					}
					wsManager.mutex.RUnlock()
					
					metrics := getCurrentMetrics()
					msg := WSMessage{
						Type: "metrics_update",
						Data: metrics,
					}
					
					if err := conn.WriteJSON(msg); err != nil {
						log.Printf("Failed to send metrics update: %v", err)
						return
					}
				}
			}
		}()
		
		conn.WriteJSON(WSMessage{Type: "subscribed", Data: map[string]string{"type": "metrics"}})
		
	default:
		conn.WriteJSON(WSMessage{Type: "error", Error: "Unknown subscription type"})
	}
}

// Utility functions for advanced features
var startTime = time.Now()

// Grammar sampler wrapper (would need to be implemented in bindings)
func NewSamplerGrammar(vocab *llama.Vocab, grammar, root string) *llama.Sampler {
	// This would require implementing grammar sampler bindings
	// For now, return nil to indicate unsupported
	log.Printf("Grammar sampling requested but not yet implemented")
	return nil
}

// Context state management extensions
func (ctx *Context) StateSeqSaveFile(filePath string, seqID llama.SeqID, tokens []llama.Token) int {
	// Implementation would use llama_state_seq_save_file
	log.Printf("Context state save requested for sequence %d", seqID)
	return 0 // Placeholder
}

func (ctx *Context) StateSeqLoadFile(filePath string, seqID llama.SeqID, tokens []llama.Token) int {
	// Implementation would use llama_state_seq_load_file
	log.Printf("Context state load requested for sequence %d", seqID)
	return 0 // Placeholder
}

// Enhanced error handling and recovery
func recoverMiddleware() gin.HandlerFunc {
	return gin.CustomRecovery(func(c *gin.Context, recovered interface{}) {
		if err, ok := recovered.(string); ok {
			log.Printf("Panic recovered: %s", err)
			c.JSON(http.StatusInternalServerError, gin.H{
				"error": "Internal server error",
				// advanced_features.go - Advanced inference features and MCP integration

package main

import (
	"bufio"
	"context"
	"encoding/json"
	"fmt"
	"log"
	"net/http"
	"os"
	"strings"
	"sync"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/gorilla/websocket"
)

// Advanced inference capabilities
type InferenceCapabilities struct {
	SupportedModels     []string               `json:"supported_models"`
	SupportedSamplers   []string               `json:"supported_samplers"`
	MaxContextSize      int                    `json:"max_context_size"`
	MaxBatchSize        int                    `json:"max_batch_size"`
	SupportsStreaming   bool                   `json:"supports_streaming"`
	SupportsGPU         bool                   `json:"supports_gpu"`
	SupportsFlashAttn   bool                   `json:"supports_flash_attention"`
	SupportsChatTemplate bool                  `json:"supports_chat_template"`
	Features            map[string]interface{} `json:"features"`
}

// Advanced request options
type AdvancedInferenceRequest struct {
	InferenceRequest
	
	// Advanced sampling parameters
	RepeatPenalty     float32  `json:"repeat_penalty,omitempty"`
	FrequencyPenalty  float32  `json:"frequency_penalty,omitempty"`
	PresencePenalty   float32  `json:"presence_penalty,omitempty"`
	PenaltyLastN      int      `json:"penalty_last_n,omitempty"`
	MinP              float32  `json:"min_p,omitempty"`
	TfsZ              float32  `json:"tfs_z,omitempty"`
	TypicalP          float32  `json:"typical_p,omitempty"`
	Mirostat          int      `json:"mirostat,omitempty"`
	MirostatTau       float32  `json:"mirostat_tau,omitempty"`
	MirostatEta       float32  `json:"mirostat_eta,omitempty"`
	
	// Context management
	ContextShift      bool     `json:"context_shift,omitempty"`
	CachePrompt       bool     `json:"cache_prompt,omitempty"`
	SequenceID        int      `json:"sequence_id,omitempty"`
	
	// Generation control
	Stop              []string `json:"stop,omitempty"`
	LogitBias         map[int]float32 `json:"logit_bias,omitempty"`
	Grammar           string   `json:"grammar,omitempty"`
	
	// Performance options
	Threads           int      `json:"threads,omitempty"`
	BatchSize         int      `json:"batch_size,omitempty"`
	KeepModelLoaded   bool     `json:"keep_model_loaded,omitempty"`
}

// WebSocket connection manager
type WSConnectionManager struct {
	connections map[string]*websocket.Conn
	mutex       sync.RWMutex
	upgrader    websocket.Upgrader
}

func NewWSConnectionManager() *WSConnectionManager {
	return &WSConnectionManager{
		connections: make(map[string]*websocket.Conn),
		upgrader: websocket.Upgrader{
			ReadBufferSize:  1024,
			WriteBufferSize: 1024,
			CheckOrigin: func(r *http.Request) bool {
				return true // Allow all origins in development
			},
		},
	}
}

var wsManager *WSConnectionManager

func init() {
	wsManager = NewWSConnectionManager()
}

// Advanced route setup
func setupAdvancedRoutes(router *gin.Engine) {
	// WebSocket endpoint for real-time inference
	router.GET("/ws", wsHandler)
	
	// Advanced inference endpoints
	v2 := router.Group("/v2")
	{
		v2.POST("/inference/advanced", advancedInferenceHandler)
		v2.POST("/inference/batch", batchInferenceHandler)
		v2.GET("/capabilities", capabilitiesHandler)
		v2.POST("/models/:model/warm", warmModelHandler)
		v2.POST("/context/:seq_id/save", saveContextHandler)
		v2.POST("/context/:seq_id/load", loadContextHandler)
		v2.DELETE("/context/:seq_id", clearContextHandler)
	}
	
	// MCP integration endpoints
	mcp := router.Group("/mcp")
	{
		mcp.GET("/protocol", mcpProtocolHandler)
		mcp.POST("/tools/register", registerToolHandler)
		mcp.POST("/tools/execute", executeToolHandler)
		mcp.GET("/resources", listResourcesHandler)
		mcp.POST("/resources/read", readResourceHandler)
	}
	
	// Administrative endpoints
	admin := router.Group("/admin")
	{
		admin.GET("/status", adminStatusHandler)
		admin.POST("/config/reload", reloadConfigHandler)
		admin.POST("/memory/gc", forceGCHandler)
		admin.GET("/debug/vars", debugVarsHandler)
		admin.POST("/models/quantize", quantizeModelHandler)
	}
}

// WebSocket handler for real-time inference
func wsHandler(c *gin.Context) {
	conn, err := wsManager.upgrader.Upgrade(c.Writer, c.Request, nil)
	if err != nil {
		log.Printf("WebSocket upgrade failed: %v", err)
		return
	}
	defer conn.Close()
	
	connectionID := generateID()
	wsManager.mutex.Lock()
	wsManager.connections[connectionID] = conn
	wsManager.mutex.Unlock()
	
	defer func() {
		wsManager.mutex.Lock()
		delete(wsManager.connections, connectionID)
		wsManager.mutex.Unlock()
	}()
	
	log.Printf("WebSocket connection established: %s", connectionID)
	
	// Handle WebSocket messages
	for {
		var wsMsg WSMessage
		if err := conn.ReadJSON(&wsMsg); err != nil {
			log.Printf("WebSocket read error: %v", err)
			break
		}
		
		switch wsMsg.Type {
		case "inference":
			handleWSInference(conn, wsMsg.Data, connectionID)
		case "ping":
			conn.WriteJSON(WSMessage{Type: "pong", Data: time.Now()})
		case "subscribe":
			// Handle subscription to inference streams
			handleWSSubscription(conn, wsMsg.Data, connectionID)
		default:
			conn.WriteJSON(WSMessage{Type: "error", Error: "Unknown message type"})
		}
	}
}

func handleWSInference(conn *websocket.Conn, data interface{}, connectionID string) {
	// Convert data to inference request
	jsonData, _ := json.Marshal(data)
	var req AdvancedInferenceRequest
	if err := json.Unmarshal(jsonData, &req); err != nil {
		conn.WriteJSON(WSMessage{Type: "error", Error: "Invalid request format"})
		return
	}
	
	// Process inference with WebSocket streaming
	model, err := modelManager.GetModel(req.Model)
	if err != nil {
		conn.WriteJSON(WSMessage{Type: "error", Error: err.Error()})
		return
	}
	
	// Stream results through WebSocket
	streamToWebSocket(conn, model, req, connectionID)
}

func streamToWebSocket(conn *websocket.Conn, model *llama.LLama, req AdvancedInferenceRequest, connID string) {
	// Implementation similar to HTTP streaming but via WebSocket
	prompt := formatMessagesWithTemplate(req.Messages, model, req.Model)
	tokens := model.Tokenize(prompt, true, true)
	
	if len(tokens) == 0 {
		conn.WriteJSON(WSMessage{Type: "error", Error: "Failed to tokenize input"})
		return
	}
	
	// Process prompt
	if err := processPromptBatch(model, tokens); err != nil {
		conn.WriteJSON(WSMessage{Type: "error", Error: err.Error()})
		return
	}
	
	// Create advanced sampler
	sampler := createAdvancedSamplerChain(InferenceRequest(req), model.Model.Vocab())
	defer sampler.Free()
	
	maxTokens := req.MaxTokens
	if maxTokens <= 0 {
		maxTokens = 512
	}
	
	// Stream generation
	for i := 0; i < maxTokens; i++ {
		token := sampler.Sample(model.Context, -1)
		
		if model.Model.Vocab().IsEog(token) {
			break
		}
		
		tokenText := model.Model.Vocab().GetText(token)
		
		response := WSMessage{
			Type: "token",
			Data: map[string]interface{}{
				"token":    token,
				"text":     tokenText,
				"position": i,
			},
		}
		
		if err := conn.WriteJSON(response); err != nil {
			log.Printf("WebSocket write error: %v", err)
			break
		}
		
		sampler.Accept(token)
		
		// Continue generation
		batch := llama.NewBatch(1, 0, 1)
		batch.Add(token, llama.Pos(len(tokens)+i), []llama.SeqID{0}, true)
		
		if err := model.Decode(batch); err != nil {
			batch.Free()
			break
		}
		batch.Free()
	}
	
	// Send completion
	conn.WriteJSON(WSMessage{Type: "complete", Data: map[string]interface{}{"connection_id": connID}})
}

// Advanced inference handler
func advancedInferenceHandler(c *gin.Context) {
	var req AdvancedInferenceRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "Invalid request format"})
		return
	}
	
	// Acquire semaphore
	select {
	case modelManager.semaphore <- struct{}{}:
		defer func() { <-modelManager.semaphore }()
	case <-time.After(config.Performance.Timeout):
		c.JSON(http.StatusTooManyRequests, gin.H{"error": "Server busy"})
		return
	}
	
	model, err := modelManager.GetModel(req.Model)
	if err != nil {
		// Try to load model dynamically
		if err := modelManager.LoadAdvancedModel(req.Model, nil); err != nil {
			c.JSON(http.StatusNotFound, gin.H{"error": fmt.Sprintf("Model not found: %s", req.Model)})
			return
		}
		model, _ = modelManager.GetModel(req.Model)
	}
	
	// Use sequence ID for context management
	seqID := req.SequenceID
	if seqID == 0 {
		seqID = 1 // Default sequence
	}
	
	// Handle context caching
	if req.CachePrompt {
		handleContextCaching(model, req, seqID)
	}
	
	// Process with advanced features
	response := processAdvancedInference(model, req, seqID)
	c.JSON(http.StatusOK, response)
}

func processAdvancedInference(model *llama.LLama, req AdvancedInferenceRequest, seqID int) InferenceResponse {
	start := time.Now()
	
	// Format prompt with chat template
	prompt := formatMessagesWithTemplate(req.Messages, model, req.Model)
	tokens := model.Tokenize(prompt, true, true)
	
	if len(tokens) == 0 {
		return InferenceResponse{Error: "Failed to tokenize input"}
	}
	
	// Clear sequence if not caching
	if !req.CachePrompt {
		model.Memory().SeqRemove(llama.SeqID(seqID), 0, -1)
	}
	
	// Process prompt batch
	if err := processPromptBatch(model, tokens); err != nil {
		return InferenceResponse{Error: fmt.Sprintf("Prompt processing failed: %v", err)}
	}
	
	// Create advanced sampler with all parameters
	sampler := createFullSamplerChain(req, model.Model.Vocab())
	defer sampler.Free()
	
	// Generate with advanced options
	generatedTokens, generatedText := generateWithAdvancedOptions(model, sampler, req, len(tokens), seqID)
	
	// Calculate metrics
	completionTokens := len(generatedTokens)
	generationTime := time.Since(start)
	
	return InferenceResponse{
		ID:    generateID(),
		Model: req.Model,
		Choices: []Choice{{
			Index: 0,
			Message: ChatMessage{
				Role:    "assistant",
				Content: generatedText,
			},
			FinishReason: determineAdvancedFinishReason(generatedTokens, req.Stop, model.Model.Vocab(), req.MaxTokens),
		}},
		Usage: Usage{
			PromptTokens:     len(tokens),
			CompletionTokens: completionTokens,
			TotalTokens:      len(tokens) + completionTokens,
		},
		Metadata: map[string]interface{}{
			"generation_time_ms":     generationTime.Milliseconds(),
			"tokens_per_second":      float64(completionTokens) / generationTime.Seconds(),
			"sequence_id":            seqID,
			"context_cached":         req.CachePrompt,
			"sampler_seed":           sampler.GetSeed(),
			"memory_state":           getAdvancedMemoryState(model, seqID),
			"performance_stats":      model.Context.PerfContext(),
		},
	}
}

func createFullSamplerChain(req AdvancedInferenceRequest, vocab *llama.Vocab) *llama.Sampler {
	chainParams := llama.DefaultSamplerChainParams()
	chainParams.NoPerf = !config.Performance.EnableMetrics
	chain := llama.NewSamplerChain(chainParams)
	
	// Add logit bias if specified
	if len(req.LogitBias) > 0 {
		// Implementation would require custom logit bias sampler
				"error": "Internal server error",
				"timestamp": time.Now(),
			})
		}
		c.Abort()
	})
}